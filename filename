 1/1:
import pandas as pd
import numpy as np
 1/2: print('Hello Capstone Project Course!')
 3/1:
import pandas as pd
import requests
from bs4 import BeasutifulSoup
 3/2:
import pandas as pd
import requests
from bs4 import BeautifulSoup
 3/3:
import pandas as pd
import requests
from bs4 import BeautifulSoup
 3/4:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(table)
df
 3/5:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table)
df
 3/6:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df
 3/7: df= pd.DataFrame(df)
 3/8: df
 3/9:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df
3/10: df[0]
3/11:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/12: df.shape()
3/13: df.shape
3/14: df.shape[0]
3/15: df.iloc[0]
3/16: df.iloc[0]['Borough']
3/17: df.iloc[0]
3/18: df.iloc[0]['Borough']
3/19:
for i in range(df.shape[0]):
    if df.iloc[i]['Borough'] == 'Not assigned':
        df.drop(df.index[i])
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
3/20: df.head(10)
3/21: df
3/22:
for i in range(df.shape[0]):
    if df.iloc[i]['Borough'] == 'Not assigned':
        df.drop(df.index[i], inplace = True)
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
3/23:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/24:
for i in range(df.shape[0]):
    if df.iloc[i]['Borough'] == 'Not assigned':
        df.drop(df.index[i], inplace = True)
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
3/25:
for row in df:
    print(df[row])
3/26:
for row in df:
    print(df[row][1])
3/27:
for row in df:
    print(df[row][1])
3/28:
for row in df['Borough']:
    print(df[row])
3/29:
for row in df['Borough']:
    print(row)
3/30: df.drop('Borough' == 'Not assigned', axis = 0)
3/31: df.drop('Borough' = 'Not assigned', axis = 0)
3/32: df.drop('Borough' == 'Not assigned', axis = 0)
3/33: df.drop('Borough' == 'Not assigned', axis = 1)
3/34:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/35: df.drop('Borough' == 'Not assigned', axis = 0)
3/36: df
3/37: df.drop(df.Borough == 'Not assigned', axis = 0)
3/38: df[df.Borough != 'Not assigned']
3/39:
df[df.Borough != 'Not assigned'].reset_index
df
3/40:
df[df.Borough != 'Not assigned'].reset_index
df
3/41:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/42:
df[df.Borough != 'Not assigned'].reset_index
df
3/43:
df[df.Borough != 'Not assigned']
df
3/44:
df[df.Borough != 'Not assigned']
df
3/45: df[df.Borough != 'Not assigned']
3/46:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/47:
df = df[df.Borough != 'Not assigned']
df
3/48:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/49:
df = df[df.Borough != 'Not assigned'].reset_index
df
3/50:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/51:
df = df[df.Borough != 'Not assigned'].reset_index()
df
3/52:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/53:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
#df.drop('index', axis = 1, inplace = True)
df
3/54:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/55:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
3/56: df[10]
3/57: df.head(10)
3/58: df.head(20)
3/59:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df[20]
3/60:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(20)
3/61:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
3/62: df.groupby('Postcode')
3/63: test = df.groupby('Postcode')
3/64:
test = df.groupby('Postcode')
test
3/65: test = df.groupby('Postcode').count()
3/66: df.groupby('Postcode').count()
3/67: df['Postcode'].distinct
3/68: df['Postcode'].distinct()
3/69: df['Postcode'].unique()
3/70: df['Postcode'].unique().count()
3/71: df['Postcode'].unique().count
3/72: df['Postcode'].unique().value_count
3/73: df['Postcode'].unique().value_counts
3/74: df['Postcode'].unique().value_counts()
3/75: df['Postcode'].unique().value_count()
3/76: df['Postcode'].unique().values_count()
3/77: df['Postcode'].unique().values
3/78: df['Postcode'].unique().value
3/79: df['Postcode'].unique()
3/80: df['Postcode'].unique()[0]
3/81: df['Postcode'].unique()[1]
3/82: df['Postcode'].unique()
3/83: test = pd.DataFrame
3/84: test
3/85: test = pd.DataFrame('1', '2')
3/86: test = pd.DataFrame(columns = {'1', '2'})
3/87: test
3/88: df[0]
3/89: df.iloc[0]
3/90: test = pd.DataFrame(columns = {'1', '2', '3'})
3/91: test
3/92: test.append(df.iloc[0])
3/93: test = pd.DataFrame
3/94: test.append(df.iloc[0])
3/95: test.append(df.iloc[0])
3/96: test = pd.DataFrame
3/97: test
3/98: test.append(df.iloc[0])
3/99: test.append(otherdf.iloc[0])
3/100: test.append(other = df.iloc[0])
3/101: test = pd.DataFrame(columns = {'1', '2', '3'})
3/102: test
3/103: test.append(df.iloc[0])
3/104: test = pd.DataFrame(columns = {})
3/105: test
3/106: test.append(df.iloc[0])
3/107: df.iloc[0]
3/108: 'Mohab'.append(', ', 'Hassan')
3/109: 'Mohab'.concat(', ', 'Hassan')
3/110: 'Mohab' + ', ' + 'Hassan'
3/111:
new_df = pd.DataFrame(columns = {})
new_df.append(df.iloc[0])

for row in df.iloc[1:]:
    for i in range(new_df.shape[0]):
        if row['Postcode'] == new_df['Postcode']:
            new_df['Neighbourhood'] = new_df['Neighbourhood'] + ', ' + row['Neighbourhood'] 
        else:
            new_df.append(row)
3/112: new_df
3/113:
new_df = pd.DataFrame(columns = {})
new_df.append(df.iloc[0])

for row in df.iloc[1:]:
    for i in range(new_df.shape[0]):
        if row['Postcode'] == new_df['Postcode']:
            new_df['Neighbourhood'] = new_df['Neighbourhood'] + ', ' + row['Neighbourhood'] 
        else:
            new_df.append(row)
3/114:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
3/115:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
3/116:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
3/117:
new_df = pd.DataFrame(columns = {})
new_df.append(df.iloc[0])

for row in df.iloc[1:]:
    for i in range(new_df.shape[0]):
        if row['Postcode'] == new_df['Postcode']:
            new_df['Neighbourhood'] = new_df['Neighbourhood'] + ', ' + row['Neighbourhood'] 
        else:
            new_df.append(row)
3/118: new_df
3/119: df
3/120:
new_df = pd.DataFrame(columns = {})
new_df.append(df.iloc[0])

new_df
3/121:
new_df = pd.DataFrame(columns = {})
new_df.append(df.iloc[0], inplace = True)

new_df
3/122:
new_df = pd.DataFrame(columns = {})
new_df.append(df.iloc[0])
3/123: new_df
3/124:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])
3/125: new_df
3/126:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for row in df.iloc[1:]:
    for i in range(new_df.shape[0]):
        if row['Postcode'] == new_df['Postcode']:
            new_df['Neighbourhood'] = new_df['Neighbourhood'] + ', ' + row['Neighbourhood'] 
        else:
            new_df = new_df.append(row)
3/127:
for row in df.iloc[1:]:
    print row
3/128:
for row in df.iloc[1:]:
    print(row)
3/129:
for row in df:
    print(row)
3/130: df
3/131: df
3/132:
for row in df:
    print(row)
3/133:
for row in df:
    row
3/134:
for row in df.rows:
    row
3/135:
for row in df.columns:
    row
3/136:
for row in df.columns:
    print(row)
3/137:
for row in df:
    print(row)
3/138:
for row in df.values:
    print(row)
3/139:
for row in df.values:
    print(row[0])
3/140:
for row in df.values:
    print(row)
3/141:
for row in df.values[1:]:
    print(row)
3/142: new_df
3/143: new_df['Postcode']
3/144: new_df['Postcode'][0]
3/145: new_df = new_df.append(df.iloc[0])
3/146: new_df
3/147: new_df['Postcode']
3/148: new_df['Postcode'][0]
3/149: new_df['Postcode'][1]
3/150: new_df['Postcode']
3/151: new_df['Postcode'].to_list()
3/152: new_df['Postcode'].to_list()[0]
3/153: new_df['Postcode']
3/154: new_df[0]['Postcode']
3/155: new_df.loc['Postcode'][0]
3/156: new_df.['Postcode']
3/157: new_df.Postcode
3/158: new_df.Postcode[0]
3/159: new_df.Postcode.iloc[0]
3/160: new_df.Postcode.iloc[1]
3/161: new_df.Postcode.iloc[2]
3/162:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for row in df.values[1:]:
    for i in range(new_df.shape[0]):
        if row[0] == new_df.Postcode.iloc[i]:
            new_df.Neighbourhood.iloc[i] = new_df.Neighbourhood.iloc[i] + ', ' + row[2] 
        else:
            new_df = new_df.append(row)
3/163: df.iloc[0]
3/164:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i, row enumerate df.values[1:]:
    for j in range(new_df.shape[0]):
        if row[0] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + row[2] 
        else:
            new_df = new_df.append(df.iloc[i])
3/165:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i, row enumerate(df.values[1:]):
    for j in range(new_df.shape[0]):
        if row[0] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + row[2] 
        else:
            new_df = new_df.append(df.iloc[i])
3/166:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i, row in enumerate(df.values[1:]):
    for j in range(new_df.shape[0]):
        if row[0] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + row[2] 
        else:
            new_df = new_df.append(df.iloc[i])
 4/1:
import pandas as pd
import requests
from bs4 import BeautifulSoup
 4/2:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
 4/3:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
 4/4:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
 4/5:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i, row in enumerate(df.values[1:]):
    for j in range(new_df.shape[0]):
        if row[0] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + row[2] 
        else:
            new_df = new_df.append(df.iloc[i])
 5/1:
import pandas as pd
import requests
from bs4 import BeautifulSoup
 5/2:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
 5/3:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
 5/4:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
 5/5:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i, row in enumerate(df.values[1:]):
    for j in range(new_df.shape[0]):
        if row[0] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + row[2] 
        else:
            new_df = new_df.append(df.iloc[i])
 6/1:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
        else:
            new_df = new_df.append(df.iloc[i])
 6/2:
import pandas as pd
import requests
from bs4 import BeautifulSoup
 6/3:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
 6/4:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
 6/5:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
 6/6:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
        else:
            new_df = new_df.append(df.iloc[i])
 7/1:
import pandas as pd
import requests
from bs4 import BeautifulSoup
 7/2:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
 7/3:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
 7/4:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
 7/5:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
        #else:
        #    new_df = new_df.append(df.iloc[i])
 7/6:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])
found = 1
for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
        found = 1
 7/7:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
 7/8:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
 7/9:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
7/10:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])
found = 1
for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
        found = 1
7/11: new_df
7/12:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
7/13:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
7/14:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
7/15:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
7/16:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
7/17: new_df
7/18: df['Postcode'].unique()
7/19: df['Postcode'].unique().values
7/20: df['Postcode'].unique().values()
7/21: df['Postcode'].unique().count
7/22: df['Postcode'].unique().count()
7/23: df['Postcode'].unique().to_list()
7/24: df['Postcode'].unique().to_list
7/25: df['Postcode'].unique()
7/26: df['Postcode'].unique()[0]
7/27: df['Postcode'].unique()
7/28: new_df = new_df['Postcode', 'Borough', 'Neighborhood']
7/29: new_df = new_df[['Postcode', 'Borough', 'Neighborhood']]
7/30: new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
7/31: new_df
7/32:
import pandas as pd
import requests
from bs4 import BeautifulSoup
7/33:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
7/34:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
7/35:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
7/36:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.head(10)
 8/1: new_df.shape
 8/2:
import pandas as pd
import requests
from bs4 import BeautifulSoup
 8/3:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
 8/4:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
 8/5:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
 8/6:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.head(10)
 8/7: new_df.shape
 8/8: %store new_df
 9/1: %store -r new_df
 9/2: new_df
 9/3: new_df.head()
 9/4: new_df.head(10)
 9/5:
%store -r new_df
new_df.head(10)
 9/6:
import geocoder
import pandas as pd
 9/7:
import pandas as pd
import requests
import numpy as np

!conda install -c conda-forge geopy --yes 
import geocoder
 9/8: import geocoders
 9/9: import geocoder
9/10: geocoder.google('Mountain View, CA')
9/11: import geocoder
9/12: import geocoder
9/13: $ pip install geocoder
9/14:
import pandas as pd
import requests
import numpy as np

!conda install -c conda-forge geocoder --yes
9/15: import geocoder
9/16: geocoder.google('Mountain View, CA')
9/17: geocoder.google('Mountain View, CA')
9/18:
g = geocoder.google('Mountain View, CA')
g.latlng
9/19:
g = geocoder.google('Mountain View, CA')
g.latlng
9/20:
g = geocoder.google('Mountain View, CA')
g.latlng
9/21: geocoder.google('Mountain View, CA')
9/22: geocoder.google('Mountain View, CA')
9/23: geocoder.google('Mountain View, CA')
9/24: geocoder.google('Mountain View, CA')
9/25: geocoder.google('Mountain View, CA')
9/26: geocoder.google('Mountain View, CA')
9/27: geocoder.google('Mountain View, CA')
9/28: geocoder.google('Mountain View, CA')
9/29: geocoder.google('Mountain View, CA')
9/30: geocoder.google('Mountain View, CA')
9/31: geocoder.google('Mountain View, CA')
9/32: geocoder.google('Mountain View, CA')
9/33: geocoder.google('Mountain View, CA')
9/34: geocoder.google('Mountain View, CA')
9/35: geocoder.google('Mountain View, CA')
9/36: geocoder.google('Mountain View, CA')
9/37: geocoder.google('Mountain View, CA')
9/38: geocoder.google('Mountain View, CA')
9/39: geocoder.google('Mountain View, CA')
9/40: geocoder.google('Mountain View, CA')
9/41: geocoder.google('Mountain View, CA')
9/42: geocoder.google('Mountain View, CA')
9/43: geocoder.google('Mountain View, CA')
9/44: geocoder.google('Mountain View, CA')
9/45: geocoder.google('Mountain View, CA')
9/46: geocoder.google('Mountain View, CA')
9/47: geocoder.google('Mountain View, CA')
9/48: geocoder.google('Mountain View, CA')
9/49: geocoder.google('Mountain View, CA')
9/50: geocoder.google('Mountain View, CA')
9/51: geocoder.google('Mountain View, CA')
9/52: geocoder.google('Mountain View, CA')
9/53: geocoder.google('Mountain View, CA')
9/54: geocoder.google('Mountain View, CA')
9/55: geocoder.google('Mountain View, CA')
9/56: geocoder.google('Mountain View, CA')
9/57: geocoder.google('Mountain View, CA')
9/58: geocoder.google('Mountain View, CA')
9/59: geocoder.google('Mountain View, CA')
9/60: geocoder.google('Mountain View, CA')
9/61: geocoder.google('Mountain View, CA')
9/62: geocoder.google('Mountain View, CA')
9/63: geocoder.google('Mountain View, CA')
9/64: geocoder.google('Mountain View, CA')
9/65: geocoder.google('Mountain View, CA')
9/66: geocoder.google('Mountain View, CA')
9/67: geocoder.google('Toronto, Ontario')
9/68: geocoder.google('M5G, Toronto, Ontario')
9/69: geocoder.google('M5G, Toronto, Ontario')
9/70: geocoder.google('M5G, Toronto, Ontario')
9/71: geocoder.google('M5G, Toronto, Ontario')
9/72: geocoder.google('M5G, Toronto, Ontario')
9/73: geocoder.google('M5G, Toronto, Ontario')
9/74: geocoder.google('M5G, Toronto, Ontario')
9/75: geocoder.google('M5G, Toronto, Ontario')
9/76: geocoder.google('M5G, Toronto, Ontario')
9/77: geocoder.google('M5G, Toronto, Ontario')
9/78: geocoder.google('M5G, Toronto, Ontario')
9/79:
lat_lng_coords = None
while(lat_lng_coords is None):
    g = geocoder.google('M5G, Toronto, Ontario')
    lat_lng_coords = g.latlng
10/1: data = pd.read_cvs('https://cocl.us/Geospatial_data')
10/2:
import pandas as pd
import requests
import numpy as np

#!conda install -c conda-forge geocoder --yes 
#import geocoder
10/3: data = pd.read_cvs('https://cocl.us/Geospatial_data')
10/4: data = pd.read_csv('https://cocl.us/Geospatial_data')
10/5: data
10/6:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data.head(10)
 8/9:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename({'Postcode' : 'PostalCode'})
new_df.head(10)
8/10:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename({'Postcode' : 'PostalCode'}, inplace = True)
new_df.head(10)
8/11:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'PostalCode'}, inplace = True)
new_df.head(10)
8/12:
import pandas as pd
import requests
from bs4 import BeautifulSoup
8/13:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
8/14:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
8/15:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
8/16:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'PostalCode'}, inplace = True)
new_df.head(10)
8/17: new_df.shape
8/18: %store new_df
10/7:
%store -r new_df
new_df.head(10)
10/8:
import pandas as pd
import requests
import numpy as np

#!conda install -c conda-forge geocoder --yes 
#import geocoder
10/9:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data.head(10)
8/19:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df.head(10)
8/20:
import pandas as pd
import requests
from bs4 import BeautifulSoup
8/21:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
8/22:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
8/23:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
8/24:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df.head(10)
8/25: new_df.shape
8/26: %store new_df
10/10:
%store -r new_df
new_df.head(10)
10/11:
import pandas as pd
import requests
import numpy as np

#!conda install -c conda-forge geocoder --yes 
#import geocoder
10/12:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data.head(10)
10/13:
df_merged = new_df.join(data, on='Postal Code')
df_merged
10/14: data.types
10/15: data.types()
10/16: data.dtypes()
10/17: data.dtypes
10/18: df_new.dtypes
10/19: new_df.dtypes
10/20:
df_merged = new_df.join(data.set_index('Postal Code'), on='Postal Code')
df_merged
10/21: data
10/22: df_merged.reset_index(drop = True)
10/23:
%store -r new_df
new_df.head(10)
10/24:
import pandas as pd
import requests
import numpy as np

#!conda install -c conda-forge geocoder --yes 
#import geocoder
10/25:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data.head(10)
10/26:
df_merged = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
df_merged
10/27:
df_merged = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
df_merged(10)
10/28:
df_merged = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
df_merged.head(10)
10/29:
%store -r new_df
new_df.head(10)
10/30:
import pandas as pd
import requests
import numpy as np

#!conda install -c conda-forge geocoder --yes 
#import geocoder
10/31:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data.head(10)
10/32:
df_merged = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
df_merged.head(10)
10/33: df_merged.shape
10/34: %store df_merged
11/1:
%store -r new_df
%store -r df_merged
df_merged.head(10)
11/2:
import numpy as np 
import pandas as pd 
import folium
11/3:
import numpy as np 
import pandas as pd

from sklearn.cluster import KMeans

import matplotlib.cm as cm
import matplotlib.colors as colors

!conda install -c conda-forge folium=0.5.0 --yes
import folium
11/4:
!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab
from geopy.geocoders import Nominatim # convert an address into latitude and longitude values
11/5:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
11/6: latitude
11/7:
map_newyork = folium.Map(location=[latitude, longitude], zoom_start=10)

map_newyork
11/8:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto, Ontario are {}, {}.'.format(latitude, longitude))
11/9:
map_newyork = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_newyork)
11/10: map_newyork
11/11:
map_newyork = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_newyork)  

map_newyork
11/12:
map_newyork = folium.Map(location=[latitude, longitude], zoom_start=11)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_newyork)  

map_newyork
11/13:
map_newyork = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_newyork)  

map_newyork
11/14:
map_Toronto = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_newyork)  

map_Toronto
11/15:
map_Toronto = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto)  

map_Toronto
11/16: Toronto_df = pd.DataFrame(columns = {})
11/17: Toronto_df = Toronto.append(df_merged.iloc[0])
11/18: Toronto_df = pd.DataFrame(columns = {})
11/19: Toronto_df = Toronto_df.append(df_merged.iloc[0])
11/20: Toronto_df
11/21: 'Mo' in 'Mohab'
11/22:
Toronto_df = pd.DataFrame(columns = {})
for i in range(df_merged.shape[0]):
    if 'Toronto' in df_merged.Borough.iloc[i]:
        Toronto_df = Toronto_df.append(df_merged.iloc[i])
Toronto_df
11/23:
Toronto_df = pd.DataFrame(columns = {})
for i in range(df_merged.shape[0]):
    if 'Toronto' in df_merged.Borough.iloc[i]:
        Toronto_df = Toronto_df.append(df_merged.iloc[i])
Toronto_df.reset_ind(drop=True) 
Toronto_df = Toronto_df[['Postal Code', 'Borough', 'Neighbourhood', 'Latitude', 'Longitude']]
11/24:
Toronto_df = pd.DataFrame(columns = {})
for i in range(df_merged.shape[0]):
    if 'Toronto' in df_merged.Borough.iloc[i]:
        Toronto_df = Toronto_df.append(df_merged.iloc[i])
Toronto_df.reset_index(drop=True) 
Toronto_df = Toronto_df[['Postal Code', 'Borough', 'Neighbourhood', 'Latitude', 'Longitude']]
11/25:
Toronto_df = pd.DataFrame(columns = {})
for i in range(df_merged.shape[0]):
    if 'Toronto' in df_merged.Borough.iloc[i]:
        Toronto_df = Toronto_df.append(df_merged.iloc[i])
Toronto_df.reset_index(drop=True) 
Toronto_df = Toronto_df[['Postal Code', 'Borough', 'Neighbourhood', 'Latitude', 'Longitude']]
Toronto_df
11/26:
Toronto_df = pd.DataFrame(columns = {})
for i in range(df_merged.shape[0]):
    if 'Toronto' in df_merged.Borough.iloc[i]:
        Toronto_df = Toronto_df.append(df_merged.iloc[i])
Toronto_df = Toronto_df.reset_index(drop=True) 
Toronto_df = Toronto_df[['Postal Code', 'Borough', 'Neighbourhood', 'Latitude', 'Longitude']]
Toronto_df
11/27:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario)  

map_Toronto_Ontario
11/28: 'Mo' in 'Mohab'
11/29:
map_Toronto = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(Toronto_df['Latitude'], Toronto_df['Longitude'], Toronto_df['Borough'], Toronto_df['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto)  

map_Toronto
11/30:
map_Toronto = folium.Map(location=[latitude, longitude], zoom_start=11)

for lat, lng, borough, neighborhood in zip(Toronto_df['Latitude'], Toronto_df['Longitude'], Toronto_df['Borough'], Toronto_df['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto)  

map_Toronto
11/31:
CLIENT_ID = 'DCFSCS0V02LYBBYV3K5ILCWJHPJNGIAZQHKECTF0PPKQVBUP' # your Foursquare ID
CLIENT_SECRET = 'KBJM2SSNVT1I2VUC3F4OJDDSLVXLOHQAVCNC0YRXDKZ4EVYF' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)
11/32:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
11/33:

Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
11/34:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
11/35:

Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
11/36:
import numpy as np 
import pandas as pd
import requests
import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans

import matplotlib.cm as cm
import matplotlib.colors as colors

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
11/37:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
11/38:

Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
11/39:
print(Toronto_venues.shape)
Toronto_venues.head()
11/40: Toronto_venues.groupby('Neighborhood').count()
11/41: print('There are {} uniques categories.'.format(len(Toronto_venues['Venue Category'].unique())))
11/42:
# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/43:
# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/44:
# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot
11/45: Toronto_venues['Neighborhood']
11/46: Toronto_onehot['Neighborhood']
11/47:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/48:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/49:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/50:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
#Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
#fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/51:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
#Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
#fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
#Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/52:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
#fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
#Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/53: Toronto_venues['Neighborhood']
11/54: Toronto_onehot['Neighborhood']
11/55: Toronto_onehot.set_index = Toronto_venues['Neighborhood']
11/56: Toronto_onehot
11/57: Toronto_venues['Neighborhood'].to_list()
11/58:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot['Neighborhood'] = Toronto_venues['Neighborhood'] 
Toronto_onehot.insert(0,'Neighborhood', Toronto_venues['Neighborhood'].to_list(),True)

# move neighborhood column to the first column
#fixed_columns = [Toronto_onehot.columns[-1]] + list(Toronto_onehot.columns[:-1])
#Toronto_onehot = Toronto_onehot[fixed_columns]

Toronto_onehot.head()
11/59:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot.insert(0,'Neighborhood', Toronto_venues['Neighborhood'].to_list(),True)


Toronto_onehot.head()
11/60:
print(Toronto_venues.shape)
Toronto_venues.head()
11/61: Toronto_onehot= pd.DataFrame({})
11/62: Toronto_onehot
11/63:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot.insert(0,'Neighborhood', Toronto_venues['Neighborhood'].to_list(),True)


Toronto_onehot.head()
11/64: Toronto_venues['Venue Category'].unique()
11/65:

Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
11/66: Toronto_venues['Venue Category'].unique()
11/67:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot.insert(0,'Neighbourhood', Toronto_venues['Neighborhood'].to_list(),True)

Toronto_onehot.head(10)
11/68: Toronto_onehot.shape
11/69:
Toronto_grouped = Toronto_onehot.groupby('Neighbourhood').mean().reset_index()
Toronto_grouped
11/70: Toronto_grouped.shape
11/71:
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]
11/72:
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighbourhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighbourhood'] = Toronto_grouped['Neighbourhood']

for ind in np.arange(manhattan_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(Toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()
11/73:
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighbourhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighbourhood'] = Toronto_grouped['Neighbourhood']

for ind in np.arange(Toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(Toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()
11/74:
# set number of clusters
kclusters = 5

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:10]
11/75:
# set number of clusters
kclusters = 5

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:30]
11/76:
# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

Toronto_merged = df_merged

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
Toronto_merged = Toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighbourhood'), on='Neighbourhood')

manhattan_merged.head() # check the last columns!
11/77: Toronto_merged.head() # check the last columns!
11/78:
# set number of clusters
kclusters = 5

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/79:
# set number of clusters
kclusters = 4

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/80:
# set number of clusters
kclusters = 5

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/81:
# set number of clusters
kclusters = 6

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/82:
# set number of clusters
kclusters = 5

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/83:
# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

Toronto_merged = Toronto_df

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
Toronto_merged = Toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighbourhood'), on='Neighbourhood')

Toronto_merged.head() # check the last columns!
11/84:
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighbourhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighbourhood'] = Toronto_grouped['Neighbourhood']

for ind in np.arange(Toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(Toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()
11/85:
# set number of clusters
kclusters = 5

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/86:
# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

Toronto_merged = Toronto_df

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
Toronto_merged = Toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighbourhood'), on='Neighbourhood')

Toronto_merged.head() # check the last columns!
11/87: Toronto_merged # check the last columns!
11/88:
# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, bo, cluster in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Neighbourhood'], Toronto_merged['Borough'], Toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi)+ ', ' + str(bo) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters
11/89:
# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, bo, cluster in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Neighbourhood'], Toronto_merged['Borough'], Toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi)+ ' (' + str(bo) + ')' + ' - Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters
11/90:
# set number of clusters
kclusters = 7

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/91:
# set number of clusters
kclusters = 5

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/92: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 0, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/93: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 1, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/94: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 2, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/95: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 3, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/96: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 4, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/97: __Exploring the top 10 venues in Cluster 1__
8/27:
import pandas as pd
import requests
from bs4 import BeautifulSoup
8/28:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]
df
8/29:
df = df[df.Borough != 'Not assigned'].reset_index(drop = True)
df
8/30:
for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
df.head(10)
8/31:
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df.head(10)
8/32: new_df.shape
8/33: %store new_df
10/35:
%store -r new_df
new_df.head(10)
10/36:
import pandas as pd
import requests
import numpy as np

#!conda install -c conda-forge geocoder --yes 
#import geocoder
10/37:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data.head(10)
10/38:
df_merged = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
df_merged.head(10)
10/39: df_merged.shape
10/40: %store df_merged
11/98:
%store -r new_df
%store -r df_merged
df_merged.head(10)
11/99:
import numpy as np 
import pandas as pd
import requests
import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans

import matplotlib.cm as cm
import matplotlib.colors as colors

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
11/100:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto, Ontario are {}, {}.'.format(latitude, longitude))
11/101:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario)  

map_Toronto_Ontario
11/102:
Toronto_df = pd.DataFrame(columns = {})
for i in range(df_merged.shape[0]):
    if 'Toronto' in df_merged.Borough.iloc[i]:
        Toronto_df = Toronto_df.append(df_merged.iloc[i])
Toronto_df = Toronto_df.reset_index(drop=True) 
Toronto_df = Toronto_df[['Postal Code', 'Borough', 'Neighbourhood', 'Latitude', 'Longitude']]
Toronto_df
11/103:
map_Toronto = folium.Map(location=[latitude, longitude], zoom_start=11)

for lat, lng, borough, neighborhood in zip(Toronto_df['Latitude'], Toronto_df['Longitude'], Toronto_df['Borough'], Toronto_df['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto)  

map_Toronto
11/104:
CLIENT_ID = 'DCFSCS0V02LYBBYV3K5ILCWJHPJNGIAZQHKECTF0PPKQVBUP' # your Foursquare ID
CLIENT_SECRET = 'KBJM2SSNVT1I2VUC3F4OJDDSLVXLOHQAVCNC0YRXDKZ4EVYF' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)
11/105:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
11/106:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
11/107:
print(Toronto_venues.shape)
Toronto_venues.head()
11/108: Toronto_venues.groupby('Neighborhood').count()
11/109: print('There are {} uniques categories.'.format(len(Toronto_venues['Venue Category'].unique())))
11/110:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# one hot encoding
Toronto_onehot = pd.get_dummies(Toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
Toronto_onehot.insert(0,'Neighbourhood', Toronto_venues['Neighborhood'].to_list(),True)

Toronto_onehot.head(10)
11/111: Toronto_onehot.shape
11/112:
Toronto_grouped = Toronto_onehot.groupby('Neighbourhood').mean().reset_index()
Toronto_grouped
11/113: Toronto_grouped.shape
11/114:
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]
11/115:
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighbourhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighbourhood'] = Toronto_grouped['Neighbourhood']

for ind in np.arange(Toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(Toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()
11/116:
# set number of clusters
kclusters = 5

Toronto_grouped_clustering = Toronto_grouped.drop('Neighbourhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(Toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_
11/117:
# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

Toronto_merged = Toronto_df

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
Toronto_merged = Toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighbourhood'), on='Neighbourhood')

Toronto_merged.head() # check the last columns!
11/118:
# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, bo, cluster in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Neighbourhood'], Toronto_merged['Borough'], Toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi)+ ' (' + str(bo) + ')' + ' - Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters
11/119: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 0, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/120: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 1, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/121: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 2, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/122: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 3, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
11/123: Toronto_merged.loc[Toronto_merged['Cluster Labels'] == 4, Toronto_merged.columns[[1] + list(range(5, Toronto_merged.shape[1]))]]
15/1:
import pandas as pd
import requests
from bs4 import BeautifulSoup

import pandas as pd
import requests
import numpy as np

import numpy as np 
import pandas as pd
import requests
import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans

import matplotlib.cm as cm
import matplotlib.colors as colors

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/2:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
15/3:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
df_merged = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
15/4: df_merged.head(10)
15/5:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto, Ontario are {}, {}.'.format(latitude, longitude))
15/6:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario)  

map_Toronto_Ontario
15/7:
CLIENT_ID = 'DCFSCS0V02LYBBYV3K5ILCWJHPJNGIAZQHKECTF0PPKQVBUP' # your Foursquare ID
CLIENT_SECRET = 'KBJM2SSNVT1I2VUC3F4OJDDSLVXLOHQAVCNC0YRXDKZ4EVYF' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)
15/8:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/9:
Toronto = df_merged
Toronto.head(10)
15/10:
Toronto_df = df_merged
Toronto_df.head(10)
15/11:
Toronto_df = df_merged
Toronto_df.head(10)
Toronto_df.shape
15/12:
Toronto_df = df_merged
Toronto_df.head(10)
15/13:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/14:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/15: Toronto_venues.head(10)
15/16: Toronto_venues.groupby(by='Venue Category').sum()
15/17: Toronto_venues.groupby(by='Venue Category').count()
15/18: Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
15/19:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/20: Toronto_restaurants.groupby(by='Venue Category').count()
15/21: Toronto_restaurants.groupby(by='Venue Category').count() > 10
15/22: Toronto_restaurants(Toronto_restaurants.groupby(by='Venue Category').count() > 10)
15/23: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count() > 10]
15/24: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count().iloc(:,1) > 10]
15/25: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count().iloc(,1) > 10]
15/26: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count().iloc[,1] > 10]
15/27: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,1] > 10]
15/28: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,:] > 10]
15/29: Toronto_restaurants['Neighborhood' = Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,1] > 10]
15/30: Toronto_restaurants['Neighborhood' == Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,1] > 10]
15/31: Toronto_restaurants[['Neighborhood' == Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,1] > 10]]
15/32: Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,1] > 10
15/33: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10)
15/34: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10)
15/35: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10).iloc[:,1]
15/36: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10).iloc[1]
15/37: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10).iloc[2]
15/38: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10).iloc[10]
15/39: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10).iloc[,]
15/40: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10).iloc[:]
15/41: Toronto_restaurants[Toronto_restaurants['Neighborhood'] == Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,1] > 10]
15/42: Toronto_restaurants['Neighborhood']
15/43: Toronto_restaurants[['Neighborhood']]
15/44: Toronto_restaurants[Toronto_restaurants[['Neighborhood']] == Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,1] > 10]
15/45: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,1] > 10]
15/46: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count() > 10]
15/47: (Toronto_restaurants.groupby(by='Venue Category').count().iloc[:,0] > 10).iloc[:]
15/48: (Toronto_restaurants.groupby(by='Venue Category').count() > 10).iloc[:]
15/49: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:]
15/50: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[1]
15/51: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[,0]
15/52: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0]
15/53: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count()reset_index(drop=True) > 10]
15/54: Toronto_restaurants[Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10]
15/55:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/56: Toronto_restaurants[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0]]
15/57: Toronto_restaurants['Neighborhood' == (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0]]
15/58: Toronto_restaurants[Toronto_restaurants['Neighborhood'] == (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0]]
15/59: Toronto_restaurants[True]
15/60: Toronto_restaurants[]
15/61: Toronto_restauranToronto_restaurants.groupby(by='Venue Category').count()
15/62: Toronto_restaurants.groupby(by='Venue Category').count()
15/63: Toronto_restaurants[['Neighborhood', 'Venue Category']]
15/64: Toronto_restaurants[['Venue Category']]
15/65:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True)
Toronto_restaurants_count.head(10)
15/66:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count.head(10)
15/67:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/68: Toronto_restaurants_count[['Venue']].plot(kind='barh')
15/69:
Toronto_restaurants_count[['Venue']].plot(kind='barh')
plt.show()
15/70:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/71:
Toronto_restaurants_count[['Venue']].plot(kind='barh')
plt.show()
15/72:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.show()
15/73:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total number of restaurants')
plt.show()
15/74:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.show()
15/75:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 70),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='blue', lw=2)
            )
plt.show()
15/76:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 70),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='--', connectionstyle='arc3', color='blue', lw=2)
            )
plt.show()
15/77:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 70),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='blue', lw=2)
            )
plt.show()
15/78:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 70),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/79:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 100),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 10),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/80:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 1),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/81:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 2),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/82:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 10),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/83:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 20),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/84:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 50),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/85:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 40),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/86:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 45),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/87:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 48),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/88:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 49),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/89:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      # s: str. will leave it blank for no text
             xy=(10, 48),             # place head of the arrow at point (year 2012 , pop 70)
             xytext=(10, 0),         # place base of the arrow at point (year 2008 , pop 20)
             xycoords='data',         # will use the coordinate system of the object being annotated 
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/90: Toronto_restaurants[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0]]
15/91: Toronto_restaurants[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,:]]
15/92: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10)
15/93: Toronto_restaurants[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10)]
15/94: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc(:,0)
15/95: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0]
15/96: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0].to_list()
15/97: Toronto_restaurants[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0].to_list()]
15/98: (Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0]
15/99: Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) > 10).iloc[:,0].to_list()]
15/100: Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()]
15/101: Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index(drop=True)
15/102: Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index()
15/103: Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0]
15/104: Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
15/105:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category' in Target_restaurants]
15/106: Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
15/107:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category' == Target_restaurants]
15/108:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants[Toronto_restaurants['Venue Category'] == Target_restaurants]
15/109:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants[Toronto_restaurants['Venue Category'] in Target_restaurants]
15/110:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants[Toronto_restaurants[['Venue Category']] in Target_restaurants]
15/111:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants[Toronto_restaurants['Venue Category'].to_list() in Target_restaurants]
15/112: 1 in [2,3,4]
15/113: Toronto_restaurants[['Venue Category']].iloc(0)
15/114: Toronto_restaurants[['Venue Category']].iloc(1)
15/115: Toronto_restaurants['Venue Category'].iloc(1)
15/116: Toronto_restaurants[['Venue Category']]
15/117: Toronto_restaurants['Venue Category']
15/118: Toronto_restaurants['Venue Category'].iloc[1]
15/119: Toronto_restaurants['Venue Category']
15/120: Toronto_restaurants.shape
15/121:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants[Toronto_restaurants['Venue Category'].iloc[i] in Target_restaurants for i in range(Toronto_restaurants.shape[0])]
15/122:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants[Toronto_restaurants['Venue Category'].iloc[i] for i in range(Toronto_restaurants.shape[0])]
15/123:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category'] in Target_restaurants
15/124:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category'] == Target_restaurants
15/125:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category'].to_list() in Target_restaurants
15/126:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category'].to_list().iloc[0] in Target_restaurants
15/127:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category'].to_list()[0] in Target_restaurants
15/128:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category'].to_list()[10] in Target_restaurants
15/129:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category'].to_list()[i] in Target_restaurants for i in range(Toronto_restaurants.shape[0])
15/130:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants['Venue Category'].isin(Target_restaurants)
15/131:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_restaurants[Toronto_restaurants['Venue Category'].isin(Target_restaurants)]
15/132:
Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
Toronto_target_restaurants = Toronto_restaurants[Toronto_restaurants['Venue Category'].isin(Target_restaurants)]
Toronto_target_restaurants.head(10)
15/133: Toronto_target_restaurants.shape()
15/134: Toronto_target_restaurants.shape
15/135:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Neighborhood').count()
Toronto_restaurants_count
15/136:
#Target_restaurants = Toronto_restaurants_count[(Toronto_restaurants.groupby(by='Venue Category').count().reset_index(drop=True) >= 10).iloc[:,0].to_list()].reset_index().iloc[:,0].to_list()
#Toronto_target_restaurants = Toronto_restaurants[Toronto_restaurants['Venue Category'].isin(Target_restaurants)]
#Toronto_target_restaurants.head(10)
15/137:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/138:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
plt.annotate('',                      
             xy=(10, 48),           
             xytext=(10, 0),    
             xycoords='data',         
             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
            )
plt.show()
15/139:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighborhood').count()
Toronto_neigh_restaurants_count
15/140:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/141:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (14,14))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/142:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (18,18))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/143:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/144: !wget --quiet https://github.com/jasonicarter/toronto-geojson/blob/master/toronto_topo.json
15/145: Toronto_geo = r'Toronto_topo.json'
15/146: Toronto_geo = r'toronto_topo.json'
15/147: Toronto_neigh_restaurants_count['Neighborhood']
15/148: Toronto_neigh_restaurants_count['Neighborhood']
15/149: json_normalize(Toronto_geo)
15/150: Toronto_geo
15/151: Toronto_geo = r'toronto_topo.json'
15/152: Toronto_geo
15/153: Toronto_geo
15/154: Toronto_geo.to_dataframe()
15/155: Toronto_geo[1]
15/156: Toronto_geo[5]
15/157: Toronto_geo
15/158: Toronto_geo.json()
15/159: Toronto_geo = pd.json_read('toronto_topo.json')
15/160: Toronto_geo = pd.read_json('toronto_topo.json')
15/161: Toronto_geo = pd.read_json('toronto_topo.json', orient = 'columns')
15/162: Toronto_geo = json.load('toronto_topo.json', orient = 'columns')
15/163: Toronto_geo = json.load('toronto_topo.json')
15/164: Toronto_geo = pd.read_json('neighbourhoods_toronto.geojson.json', orient = 'columns')
15/165:
Toronto_geo = pd.read_json('neighbourhoods_toronto.geojson.json', orient = 'columns')
Toronto_geo
15/166: Toronto_geo = pd.read_json('neighbourhoods_toronto.geojson.json', orient = 'columns')
15/167: json_normalize(Toronto_geo)
15/168: json_normalize('neighbourhoods_toronto.geojson.json')
15/169: Toronto_geo = r'neighbourhoods_toronto.geojson.json'
15/170: json_normalize(Toronto_geo)
15/171: Toronto_geo = pd.read_json('neighbourhoods_toronto.geojson.json')
15/172:
Toronto_geo = pd.read_json('neighbourhoods_toronto.geojson.json')
Toronto_geo
15/173:
Toronto_geo = pd.read('neighbourhoods_toronto.geojson.json')
Toronto_geo
15/174:
Toronto_geo = read('neighbourhoods_toronto.geojson.json')
Toronto_geo
15/175:
Toronto_geo = open('neighbourhoods_toronto.geojson.json', 'r')
Toronto_geo
15/176: json_normalize(Toronto_geo)
15/177: Toronto_neigh_restaurants_count.unique()
15/178: Toronto_neigh_restaurants_count['Venue'].unique()
15/179:
Toronto_df = df_merged
Toronto_df.head(10)
Toronto_df['Postal Code'].to_list()
15/180:
Toronto_df = df_merged
Toronto_df.head(10)
15/181: pd.read_csv('T1201EN')
15/182: pd.read_csv('T1201EN.CSV')
15/183: pd.read_csv('Population.csv')
15/184:
df_pop = pd.read_csv('Population.csv')
# we can immediately drop the columns we're not interested in 
df_pop=df_pop[['Geographic code','Province or territory', 'Population, 2016']]
df_pop=df_pop[(df_pop['Province or territory']== "Ontario")]
df_pop.head()
15/185:
df_pop = pd.read_csv('Population.csv')
# we can immediately drop the columns we're not interested in 
df_pop=df_pop[['Geographic code','Province or territory', 'Population, 2016']]
df_pop=df_pop[(df_pop['Province or territory']== "Ontario")]
df_pop
15/186:
Toronto_pop = pd.read_csv('Data/Population.csv')
Toronto_pop.head()
15/187:
Toronto_pop = pd.read_csv('./Data/Population.csv')/Users/mohab1/Desktop/Github_projects/Capstone-Project/Data
Toronto_pop.head()
15/188:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')/Users/mohab1/Desktop/Github_projects/Capstone-Project/Data
Toronto_pop.head()
15/189:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/190:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'})
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory']== "Ontario")]
Toronto_pop.head()
15/191:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")]
Toronto_pop.head()
15/192:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")]
Toronto_pop.head()
15/193:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/194:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")]
Toronto_pop.head()
15/195:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/196:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index()
Toronto_pop.head()
15/197:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/198:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop.head()
15/199:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.head()
15/200:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged
15/201:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged['Population, 2016']
15/202:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged['Population, 2016'].to_list()
15/203:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged
15/204:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.head(100)
15/205:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.iloc[0:10]
15/206:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.iloc[0:100]
15/207:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.iloc[20:30]
15/208:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.iloc[30:40]
15/209:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.iloc[40:50]
15/210:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.iloc[50:60]
15/211:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.iloc[60:70]
15/212:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.iloc[70:80]
15/213:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.dropna()
Toronto_merged
15/214:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.dropna()
Toronto_merged.iloc[70:80]
15/215:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.dropna(inplace=True)
Toronto_merged.iloc[70:80]
15/216:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.dropna(inplace=True)
Toronto_merged
15/217:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.drop('Province or territory', axis = 1, inplace = True)
Toronto_merged.dropna(inplace=True)
Toronto_merged
15/218:
Toronto_geo = 'Data/Toronto_geo.geojson'
map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Immigration to Canada'
)

# display map
world_map
15/219:
Toronto_geo = 'Data/Toronto_geo.geojson'
map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Immigration to Canada'
)

# display map
Toronto_geo
15/220:
Toronto_geo = 'Data/Toronto_geo.geojson'
Toronto_map.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Immigration to Canada'
)

# display map
Toronto_map
15/221:
Toronto_geo = 'Data/Toronto_geo.geojson'
map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Immigration to Canada'
)

# display map
map_Toronto_Ontario
15/222:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

Toronto_geo = 'Data/Toronto_geo.geojson'
map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

# display map
map_Toronto_Ontario
15/223:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linespace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = pop_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

# display map
map_Toronto_Ontario
15/224:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = pop_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

# display map
map_Toronto_Ontario
15/225:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

# display map
map_Toronto_Ontario
15/226:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

# display map
map_Toronto_Ontario
15/227:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Borough'], Toronto_merged['Neighbourhood'], Toronto_merged['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/228:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Borough'], Toronto_merged['Neighbourhood'], Toronto_merged['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/229:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Borough'], Toronto_merged['Neighbourhood'], Toronto_merged['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/230:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Borough'], Toronto_merged['Neighbourhood'], Toronto_merged['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/231: Toronto_venues.head(10)
15/232:
Toronto_venues.rename(columns = {'Neighborhood' : 'Neighbourhood'}, inplace = True)
Toronto_venues.head(10)
15/233:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/234:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/235:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighborhood').count()
Toronto_neigh_restaurants_count
15/236:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighbourhood').count()
Toronto_neigh_restaurants_count
15/237:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (18,18))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/238: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood')
15/239: Toronto_restaurants.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood')
15/240: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood')
15/241: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop('Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'enue Category')
15/242: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop('Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category')
15/243: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop('Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category', axis = 1, inplace = True)
15/244: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index(['Neighbourhood'), on='Neighbourhood').drop('Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1, inplace = True)
15/245: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1, inplace = True)
15/246: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1, inplace = True)
15/247: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1)
15/248: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'})
15/249: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').reset_index().drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'})
15/250: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'})
15/251: Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).reset_index(drop=True)
15/252:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).reset_index(drop=True)
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

map_Toronto_Ontario
15/253:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).reset_index(drop=True)
Restaurants_per_neigh.head()
15/254:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).reset_index(drop=True)
Restaurants_per_neigh.head(10)
15/255:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/256:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/257:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)



map_Toronto_Ontario
15/258: Restaurants_per_neigh.isnan()
15/259: Restaurants_per_neigh.isnan
15/260: Restaurants_per_neigh.isnull()
15/261: Restaurants_per_neigh.isnull().sum()
15/262:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh.head
15/263:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/264:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/265: Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']
15/266:
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform(Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants'])
15/267:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn import preprocessing

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/268:
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform(Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants'])
15/269: Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']
15/270:
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).to_list())
15/271:
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).to_list())
15/272:
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']))
15/273:
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values)
15/274:
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.astype(float))
15/275: Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']
15/276: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).to_dataframe()
15/277: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).to_dataframe
15/278: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).dataframe
15/279: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).dataframe()
15/280: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants'])
15/281: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values
15/282: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants'])
15/283: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values
15/284:
min_max_scaler = preprocessing.MinMaxScaler()
temp = (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values
min_max_scaler.fit_transform(temp)
15/285:
min_max_scaler = preprocessing.MinMaxScaler()
temp = (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.astype(float)
min_max_scaler.fit_transform(temp)
15/286:
min_max_scaler = preprocessing.MinMaxScaler()
temp = (Restaurants_per_neigh['Population, 2016']./Restaurants_per_neigh['Restaurants']).values.astype(float)
min_max_scaler.fit_transform(temp)
15/287:
min_max_scaler = preprocessing.MinMaxScaler()
temp = (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.astype(float)
min_max_scaler.fit_transform(temp)
15/288:
min_max_scaler = preprocessing.MinMaxScaler()
temp = (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.astype(float)
k= min_max_scaler.fit_transform(temp)
15/289:
d = {
       'Score':[62,-47,-55,74,31,77,85,63,42,67,89,81,56]}
 
df = pd.DataFrame(d,columns=['Score'])
print df
float_array = df['Score'].values.astype(float)
15/290:
d = {
       'Score':[62,-47,-55,74,31,77,85,63,42,67,89,81,56]}
 
dff = pd.DataFrame(d,columns=['Score'])
print(dff)
float_array = dff['Score'].values.astype(float)
15/291:
d = {
       'Score':[62,-47,-55,74,31,77,85,63,42,67,89,81,56]}
 
dff = pd.DataFrame(d,columns=['Score'])
print(dff)
float_array = dff['Score'].values.astype(float)
print(float_array)
15/292: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1)
15/293: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(1,-1)
15/294: (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1)
15/295:
min_max_scaler = preprocessing.MinMaxScaler()
temp = (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values..reshape(-1,1).astype(float)
k= min_max_scaler.fit_transform(temp)
15/296:
min_max_scaler = preprocessing.MinMaxScaler()
temp = (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float)
k= min_max_scaler.fit_transform(temp)
15/297:
min_max_scaler = preprocessing.MinMaxScaler()
temp = (Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float)
min_max_scaler.fit_transform(temp)
15/298:
min_max_scaler = preprocessing.MinMaxScaler()
min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float))
15/299:
min_max_scaler = preprocessing.MinMaxScaler()
Restaurants_per_neigh['Ratio_pop_neigh'] = min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float))
Restaurants_per_neigh.head(10)
15/300:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/301:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/302:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/303:
min_max_scaler = preprocessing.MinMaxScaler()
Restaurants_per_neigh['Ratio_pop_neigh'] = min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float))
Restaurants_per_neigh
15/304:
min_max_scaler = preprocessing.MinMaxScaler()
Restaurants_per_neigh['Ratio_pop_neigh'] = min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float)) * 100
Restaurants_per_neigh
15/305:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/306:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {}\%'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/307:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {.2}\%'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/308:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {}\%'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/309:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {%.2}\%'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/310:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {%.2f}\%'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/311:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, ratio in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {}\%'.format(neighborhood, borough, round(ratio,2))
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/312:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/313:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/314:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/315:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/316:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/317:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/318:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'],  
            v['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/319:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/320:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'],  
            v['categories']['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/321:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/322:
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'],  
            v['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/323:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/324:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'],  
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/325:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/326:
Toronto_venues.rename(columns = {'Neighborhood' : 'Neighbourhood'}, inplace = True)
Toronto_venues.head(10)
15/327:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/328: Toronto_venues.isnull()
15/329: Toronto_venues.isnull().any
15/330: Toronto_venues.isnull().sum()
15/331: Toronto_venues.dropna(inplace=True)
15/332:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/333:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/334:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/335:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighbourhood').count()
Toronto_neigh_restaurants_count
15/336:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (18,18))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/337:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/338:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop.head()
15/339:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Borough'], Toronto_merged['Neighbourhood'], Toronto_merged['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/340:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/341:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/342:
min_max_scaler = preprocessing.MinMaxScaler()
Restaurants_per_neigh['Ratio_pop_neigh'] = min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float)) * 100
Restaurants_per_neigh
15/343:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, ratio in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {}\%'.format(neighborhood, borough, round(ratio,2))
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/344: Restaurants_per_neigh['Neighbourhood']
15/345: Restaurants_per_neigh['Neighbourhood']['Willowdale West']
15/346: Restaurants_per_neigh['Neighbourhood'=='Willowdale West']
15/347: Restaurants_per_neigh['Neighbourhood'] =='Willowdale West'
15/348: Restaurants_per_neigh[Restaurants_per_neigh['Neighbourhood'] =='Willowdale West']
15/349: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Willowdale West']
15/350: Toronto_restaurants[Toronto_restaurants['Venue'] == 'McDonald\'s']
15/351: Toronto_restaurants[Toronto_restaurants['Venue Category'] == 'Fast Food Restaurant]
15/352: Toronto_restaurants[Toronto_restaurants['Venue Category'] == 'Fast Food Restaurant']
15/353: Toronto_restaurants[Toronto_restaurants['Venue Category'] == 'Fast Food Restaurant'].head(10)
15/354:
url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            latitude, 
            longitude, 
            500, 
            10)

results = requests.get(url).json()["response"]['venues']
15/355: results
15/356:
url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            latitude, 
            longitude, 
            30000, 
            1000)

results = requests.get(url).json()["response"]['venues']
15/357: results
15/358: results['name']
15/359: v['name'] for v in results
15/360: print(v['name']) for v in results
15/361: results.json_normalize()
15/362: json_normalize(results)
15/363:
url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            latitude, 
            longitude, 
            30000, 
            2000)

results = requests.get(url).json()["response"]['venues']
15/364: json_normalize(results)
15/365:
print(Toronto_venues.shape)
print(Toronto_venues['Venue Latitude'].unique().shape)
15/366:
print(Toronto_venues.shape[0])
print(Toronto_venues['Venue Latitude'].unique().shape)
15/367:
print(Toronto_venues.shape[0])
print(Toronto_venues['Venue Latitude'].unique().shape[0])
15/368:
print(Toronto_venues.shape[0])
print(Toronto_venues['Venue Latitude', 'Venue Longitude'].unique().shape[0])
15/369: Toronto_venues['Venue Latitude']
15/370: Toronto_venues['Venue Latitude', 'Venue Longitude']
15/371: Toronto_venues[['Venue Latitude', 'Venue Longitude']]
15/372:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue Latitude', 'Venue Longitude']].unique().shape[0])
15/373: Toronto_venues[['Venue Latitude', 'Venue Longitude']].to_series
15/374: Toronto_venues[['Venue Latitude', 'Venue Longitude']].iloc[:]
15/375: Toronto_venues[['Venue Latitude', 'Venue Longitude']].iloc[,0:2]
15/376: Toronto_venues[['Venue Latitude', 'Venue Longitude']].iloc[:,0:2]
15/377: Toronto_venues[['Venue Latitude', 'Venue Longitude']].iloc[:]
15/378:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue Latitude', 'Venue Longitude']].iloc[:].unique().shape[0])
15/379: Toronto_venues['Venue Latitude'].append(Toronto_venues['Venue Longitude'])
15/380: Toronto_venues['Venue Latitude'].append(Toronto_venues['Venue Longitude']).unique
15/381: Toronto_venues['Venue Latitude'].append(Toronto_venues['Venue Longitude']).unique()
15/382: Toronto_venues['Venue Latitude'].append(Toronto_venues['Venue Longitude']).unique().shape[0]
15/383: Toronto_venues['Venue Latitude'].append(Toronto_venues['Venue Longitude'])
15/384: Toronto_venues['Venue Latitude'].append(Toronto_venues['Venue Longitude'], axis=1)
15/385: Toronto_venues['Venue Latitude'].append(Toronto_venues['Venue Longitude'])
15/386: Toronto_venues['Venue Latitude'].astype(str).append(Toronto_venues['Venue Longitude'].astype(str))
15/387:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue Latitude', 'Venue Longitude']].unique().shape[0])
15/388: Toronto_venues['Venue Latitude']
15/389:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue Latitude', 'Venue Longitude']].unique().shape[0])
15/390: Toronto_venues.head(10)
15/391: Toronto_venues['Neighbourhood']
15/392: (Toronto_venues['Venue Latitude'].astype(str)).append(Toronto_venues['Venue Longitude'].astype(str))
15/393: ((Toronto_venues['Venue Latitude'].astype(str)).append(Toronto_venues['Venue Longitude'].astype(str))).shape[0]
15/394: Toronto_venues['Venue Latitude'].shape[0]
15/395: ((Toronto_venues['Venue Latitude'].astype(str)).append(Toronto_venues['Venue Longitude'].astype(str), ignore_index=True)).shape[0]
15/396: ((Toronto_venues[['Venue Latitude']].astype(str)).append(Toronto_venues[['Venue Longitude']].astype(str), ignore_index=True)).shape[0]
15/397: (-1.5).astype(str)
15/398: (-1.5).astype(int)
15/399: ((Toronto_venues[['Venue Latitude']]).append(Toronto_venues[['Venue Longitude']], ignore_index=True)).shape[0]
15/400: Toronto_venues[['Venue Latitude']]
15/401: Toronto_venues[['Venue Longitude']
15/402: Toronto_venues[['Venue Longitude']]
15/403: np.unique(Toronto_venues[['Venue Latitude', 'Venue Longitude']].values)
15/404: np.unique(Toronto_venues[['Venue Latitude', 'Venue Longitude']].values).shape
15/405: ((Toronto_venues[['Venue']]).append(Toronto_venues[['Venue Longitude']], ignore_index=True)).shape[0]
15/406: ((Toronto_venues[['Venue']]).append(Toronto_venues[['Venue Longitude']].astype(str), ignore_index=True)).shape[0]
15/407: Toronto_venues[['Venue Latitude', 'Venue Longitude']].drop_duplicates()
15/408: Toronto_venues[['Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0]
15/409:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/410: Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicates()
15/411: Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()
15/412: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
15/413: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
15/414: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()].head()
15/415: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()].head(10)
15/416: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]['Venue' = 'Bluenotes']
15/417: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]['Venue' == 'Bluenotes']
15/418: Toronto_venues[Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]['Venue'] == 'Bluenotes']
15/419: Toronto_venues[(Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()])['Venue'] == 'Bluenotes']
15/420: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
15/421: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]['Venue']
15/422: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]['Venue'] == 'Bluenotes'
15/423: Toronto_venues[Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]['Venue'] == 'Bluenotes']
15/424: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]['Venue'] == 'Bluenotes'
15/425: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]['Venue']
15/426: tempp = Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
15/427:
tempp = Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
tempp[tempp['Venue']=='Bluenotes']
15/428: Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
15/429:
Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
Toronto_venues[Toronto_venues['Venue'] == 'Bluenotes']
15/430:
Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
#Toronto_venues[Toronto_venues['Venue'] == 'Bluenotes']
15/431:
Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
Toronto_venues[Toronto_venues['Venue'] == 'Bluenotes']
15/432:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/433:
Toronto_venues[Toronto_venues[['Venue Latitude', 'Venue Longitude']].duplicated()]
#Toronto_venues[Toronto_venues['Venue'] == 'Bluenotes']
15/434: Toronto_venues[Toronto_venues['Venue'] == 'Sushi Shop']
15/435: Toronto_venues[Toronto_venues['Venue'] == 'Jugo Juice']
15/436: Toronto_venues[Toronto_venues['Venue'] == 'Thai Express']
15/437: Toronto_venues[Toronto_venues['Venue Latitude'] == 43.718846]
15/438: Toronto_venues[Toronto_venues['Venue Latitude'] == 43.718846 & Toronto_venues['Venue Longitude'] == -79.465906]
15/439: Toronto_venues[(Toronto_venues['Venue Latitude'] == 43.718846) & (Toronto_venues['Venue Longitude'] == -79.465906)]
15/440: Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/441: Toronto_venues[(Toronto_venues['Venue'] == 'Union Station')]
15/442:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/443:
for row in tempp:
    if row['Venue Category'].str.contains('Restaurant'):
        temppp = Toronto_venues[(Toronto_venues['Venue'] == row.Venue)]
15/444: tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/445:
for row in tempp:
    if row['Venue Category'].str.contains('Restaurant'):
        temppp = Toronto_venues[(Toronto_venues['Venue'] == row.Venue)]
15/446:
for row in tempp:
    if row['Venue Category'].str.contains('Restaurant'):
        temppp = Toronto_venues[(Toronto_venues['Venue'] == row.Venue)]
15/447:
for row in tempp:
    print(row)
    if row['Venue Category'].str.contains('Restaurant'):
        temppp = Toronto_venues[(Toronto_venues['Venue'] == row.Venue)]
15/448:
tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
tempp
15/449:
tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
tempp[0]
15/450:
tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
tempp.iloc[0]
15/451:
tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
tempp
15/452:
for i in range(tempp.shape[0]:
    if tempp['Venue'].iloc[i].str.contains('Restaurant'):
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
15/453:
for i in range(tempp.shape[0]):
    if tempp['Venue'].iloc[i].str.contains('Restaurant'):
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
15/454:
tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
tempp['Venue'].iloc[0]
15/455:
tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
tempp
15/456:
for i in range(tempp.shape[0]):
    if tempp['Venue'].iloc[i].contains('Restaurant'):
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
15/457:
for i in range(tempp.shape[0]):
    if 'Restaurant' is in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
15/458:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/459:
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
15/460:
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
temppp
15/461:
tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
tempp.iloc[0]
15/462:
tempp = Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
tempp
15/463:
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
        for j in range(temppp.shape[0]):
            dis[j] = sqrt((temppp['Venue Latitude'].iloc[j] - temppp['Neighborhood Latitude'].iloc[j])**2 + (temppp['Venue Longitude'].iloc[j] - temppp['Neighborhood Longitude'].iloc[j])**2)
        temppp['dis'] = dis    
temppp
15/464:
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
        for j in range(temppp.shape[0]):
            dis[j] = math.sqrt((temppp['Venue Latitude'].iloc[j] - temppp['Neighborhood Latitude'].iloc[j])**2 + (temppp['Venue Longitude'].iloc[j] - temppp['Neighborhood Longitude'].iloc[j])**2)
        temppp['dis'] = dis    
temppp
15/465:
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
        for j in range(temppp.shape[0]):
            dis[j] = np.sqrt((temppp['Venue Latitude'].iloc[j] - temppp['Neighborhood Latitude'].iloc[j])**2 + (temppp['Venue Longitude'].iloc[j] - temppp['Neighborhood Longitude'].iloc[j])**2)
        temppp['dis'] = dis    
temppp
15/466:
dis = []
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
        for j in range(temppp.shape[0]):
            dis[j] = np.sqrt((temppp['Venue Latitude'].iloc[j] - temppp['Neighborhood Latitude'].iloc[j])**2 + (temppp['Venue Longitude'].iloc[j] - temppp['Neighborhood Longitude'].iloc[j])**2)
        temppp['dis'] = dis    
temppp
15/467:
dis = 0
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
        for j in range(temppp.shape[0]):
            dis[j] = np.sqrt((temppp['Venue Latitude'].iloc[j] - temppp['Neighborhood Latitude'].iloc[j])**2 + (temppp['Venue Longitude'].iloc[j] - temppp['Neighborhood Longitude'].iloc[j])**2)
        temppp['dis'] = dis    
temppp
15/468:
dis = []
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
        for j in range(temppp.shape[0]):
            dis[j] = np.sqrt((temppp['Venue Latitude'].iloc[j] - temppp['Neighborhood Latitude'].iloc[j])**2 + (temppp['Venue Longitude'].iloc[j] - temppp['Neighborhood Longitude'].iloc[j])**2)
        temppp['dis'] = dis    
temppp
15/469:
dis = [0]
for i in range(tempp.shape[0]):
    if 'Restaurant' in tempp['Venue'].iloc[i]:
        temppp = Toronto_venues[(Toronto_venues['Venue'] == tempp['Venue'].iloc[i])]
        for j in range(temppp.shape[0]):
            dis[j] = np.sqrt((temppp['Venue Latitude'].iloc[j] - temppp['Neighborhood Latitude'].iloc[j])**2 + (temppp['Venue Longitude'].iloc[j] - temppp['Neighborhood Longitude'].iloc[j])**2)
        temppp['dis'] = dis    
temppp
15/470:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'],  
            v['location']['PostalCode'],
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/471:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/472: Toronto_venues
15/473:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'],  
            v['location']['postalCode'],
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/474:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/475:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_list = row['postalCode']
    except:
        postal_list = row['venue.postalCode']
        
    if len(postal_list) == 0:
        return None
    else:
        return postal_list[0]
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code'
                  'Venue Category']
    
    return(nearby_venues)
15/476:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/477:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_list = row['postalCode']
    except:
        postal_list = 0
        
    if len(postal_list) == 0:
        return None
    else:
        return postal_list[0]
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code'
                  'Venue Category']
    
    return(nearby_venues)
15/478:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/479:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_list = row['postalCode']
    except:
        postal_list = ''
        
    if len(postal_list) == 0:
        return None
    else:
        return postal_list[0]
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code'
                  'Venue Category']
    
    return(nearby_venues)
15/480:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/481:
Toronto_venues.rename(columns = {'Neighborhood' : 'Neighbourhood'}, inplace = True)
Toronto_venues.head(10)
15/482:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_list = row['postalCode']
    except:
        postal_list = ''
        
    if len(postal_list) == 0:
        return None
    else:
        return postal_list[0]
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/483:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/484:
Toronto_venues.rename(columns = {'Neighborhood' : 'Neighbourhood'}, inplace = True)
Toronto_venues.head(10)
15/485:
Toronto_venues.rename(columns = {'Neighborhood' : 'Neighbourhood'}, inplace = True)
Toronto_venues.head(20)
15/486:
getNearbyVenues(names=Toronto_df['Neighbourhood'].iloc[0],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/487:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_list = row['postalCode']
    except:
        postal_list = ''
        
    if len(postal_list) == 0:
        return None
    else:
        return postal_list[0]
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(results)
15/488:
getNearbyVenues(names=Toronto_df['Neighbourhood'].iloc[0],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/489:
h = getNearbyVenues(names=Toronto_df['Neighbourhood'].iloc[0],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/490: h['location']
15/491: h
15/492: h[0]['location']
15/493: h[0]['location']['postalCode']
15/494:
h = getNearbyVenues(names=Toronto_df['Neighbourhood'].iloc[0],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/495:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_list = row['postalCode']
    except:
        postal_list = ''
        
    if len(postal_list) == 0:
        return None
    else:
        return postal_list[0]
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            v['location']['postalCode'],
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/496:
h = getNearbyVenues(names=Toronto_df['Neighbourhood'].iloc[0],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/497:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_list = row['postalCode']
    except:
        postal_list = ''
        
    if len(postal_list) == 0:
        return None
    else:
        return postal_list[0]
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            v['location']['postalCode'],
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(results)
15/498:
h = getNearbyVenues(names=Toronto_df['Neighbourhood'].iloc[0],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/499: h
15/500: (x['location']['postalCode']) for x in h
15/501: (print(x['location']['postalCode'])) for x in h
15/502: [(print(x['location']['postalCode'])) for x in h]
15/503:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/504:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/505:
Toronto_venues.rename(columns = {'Neighborhood' : 'Neighbourhood'}, inplace = True)
Toronto_venues.head(20)
15/506: Toronto_venues.isnull().sum()
15/507:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/508: Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/509: Toronto_venues[Toronto_venues['Venue Category'].str.contains('Restaurant')].isnull().sum()
15/510: Toronto_venues[Toronto_venues['Venue Category'].str.contains('Restaurant')]
15/511:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/512: Toronto_venues.dropna(inplace=True)
15/513:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/514: Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/515:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/516:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/517:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/518:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/519:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude']
                                  )
15/520: Toronto_venues.head(20)
15/521: Toronto_venues.isnull().sum()
15/522: Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/523: Toronto_venues.isnull().sum()
15/524: Toronto_venues.dropna(inplace=True)
15/525:
print(Toronto_venues.shape[0])
print(Toronto_venues[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/526: Toronto_venues[Toronto_venues[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/527:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/528: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/529:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/530:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/531:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/532: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/533: Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates(inplace=True)
15/534:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/535: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/536: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/537:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/538: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/539: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/540:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/541:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/542: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/543: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/544: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/545:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/546:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/547:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighbourhood').count()
Toronto_neigh_restaurants_count
15/548:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/549:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop.head()
15/550:
Toronto_merged = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_merged.drop('Province or territory', axis = 1, inplace = True)
Toronto_merged.dropna(inplace=True)
Toronto_merged
15/551:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_merged['Population, 2016'].min(), Toronto_merged['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_merged,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_merged['Latitude'], Toronto_merged['Longitude'], Toronto_merged['Borough'], Toronto_merged['Neighbourhood'], Toronto_merged['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/552:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/553:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/554:
min_max_scaler = preprocessing.MinMaxScaler()
Restaurants_per_neigh['Ratio_pop_neigh'] = min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float)) * 100
Restaurants_per_neigh
15/555:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, ratio in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {}\%'.format(neighborhood, borough, round(ratio,2))
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/556: Restaurants_per_neigh[Restaurants_per_neigh['Neighbourhood'] =='Willowdale West']
15/557: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Willowdale West']
15/558:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=1000):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'],  
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/559:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'],  
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)
15/560:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 100
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/561:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude'],
                                    1000
                                  )
15/562:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude'],
                                    radius = 1000
                                  )
15/563: Toronto_venues.head(20)
15/564: Toronto_venues.isnull().sum()
15/565: Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/566: Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/567: Toronto_venues.isnull().sum()
15/568: Toronto_venues.dropna(inplace=True)
15/569:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/570:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/571:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 1000
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/572:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude'],
                                    radius = 1000
                                  )
15/573: Toronto_venues.head(20)
15/574: Toronto_venues.isnull().sum()
15/575: Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/576: Toronto_venues.shape
15/577: Toronto_venues.isnull().sum()
15/578: Toronto_venues.dropna(inplace=True)
15/579: Toronto_venues.shape
15/580:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/581:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/582: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/583: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/584: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/585: print(Toronto_restaurants.shape[0])
15/586:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/587:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/588:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/589:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/590:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighbourhood').count()
Toronto_neigh_restaurants_count
15/591:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (18,18))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/592:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/593:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/594:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = StandardScaler().fit_transform(Clus_dataSet)

# Compute DBSCAN
db = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

Toronto_restaurants.head(10)
15/595:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

# Compute DBSCAN
db = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

Toronto_restaurants.head(10)
15/596:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

# Compute DBSCAN
db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

Toronto_restaurants.head(10)
22/1:
import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
22/2:
!conda install -c conda-forge folium=0.5.0 --yes
import folium

print('Folium installed and imported!')
22/3:
# define the world map
world_map = folium.Map()

# display world map
world_map
22/4:
# define the world map centered around Canada with a low zoom level
world_map = folium.Map(location=[56.130, -106.35], zoom_start=4)

# display world map
world_map
22/5:
# define the world map centersfed around Canada with a higher zoom level
world_map = folium.Map(location=[56.130, -106.35], zoom_start=8)

# display world map
world_map
22/6:
### type your answer here
mexico_latitude = 23.6345 
mexico_longitude = -102.5528
mexico_map = folium.Map(location=[mexico_latitude, mexico_longitude], zoom_start=4)

mexico_map
22/7:
# create a Stamen Toner map of the world centered around Canada
world_map = folium.Map(location=[56.130, -106.35], zoom_start=4, tiles='Stamen Toner')

# display map
world_map
22/8:
# create a Stamen Toner map of the world centered around Canada
world_map = folium.Map(location=[56.130, -106.35], zoom_start=4, tiles='Stamen Terrain')

# display map
world_map
22/9:
# create a world map with a Mapbox Bright style.
world_map = folium.Map(tiles='Mapbox Bright')

# display the map
world_map
22/10:
### type your answer here

mexico_map = folium.Map(location=[mexico_latitude, mexico_longitude], zoom_start=6, tiles='Stamen Terrain')
mexico_map
22/11:
df_incidents = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DV0101EN/labs/Data_Files/Police_Department_Incidents_-_Previous_Year__2016_.csv')

print('Dataset downloaded and read into a pandas dataframe!')
22/12: df_incidents.head()
22/13: df_incidents.shape
22/14:
# get the first 100 crimes in the df_incidents dataframe
limit = 100
df_incidents = df_incidents.iloc[0:limit, :]
22/15: df_incidents.shape
22/16:
# San Francisco latitude and longitude values
latitude = 37.77
longitude = -122.42
22/17:
# create map and display it
sanfran_map = folium.Map(location=[latitude, longitude], zoom_start=12)

# display the map of San Francisco
sanfran_map
22/18:
# instantiate a feature group for the incidents in the dataframe
incidents = folium.map.FeatureGroup()

# loop through the 100 crimes and add each to the incidents feature group
for lat, lng, in zip(df_incidents.Y, df_incidents.X):
    incidents.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='yellow',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )

# add incidents to map
sanfran_map.add_child(incidents)
22/19:
# instantiate a feature group for the incidents in the dataframe
incidents = folium.map.FeatureGroup()

# loop through the 100 crimes and add each to the incidents feature group
for lat, lng, in zip(df_incidents.Y, df_incidents.X):
    incidents.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='yellow',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )

# add pop-up text to each marker on the map
latitudes = list(df_incidents.Y)
longitudes = list(df_incidents.X)
labels = list(df_incidents.Category)

for lat, lng, label in zip(latitudes, longitudes, labels):
    folium.Marker([lat, lng], popup=label).add_to(sanfran_map)    
    
# add incidents to map
sanfran_map.add_child(incidents)
22/20:
# create map and display it
sanfran_map = folium.Map(location=[latitude, longitude], zoom_start=12)

# loop through the 100 crimes and add each to the map
for lat, lng, label in zip(df_incidents.Y, df_incidents.X, df_incidents.Category):
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, # define how big you want the circle markers to be
        color='yellow',
        fill=True,
        popup=label,
        fill_color='blue',
        fill_opacity=0.6
    ).add_to(sanfran_map)

# show map
sanfran_map
22/21:
from folium import plugins

# let's start again with a clean copy of the map of San Francisco
sanfran_map = folium.Map(location = [latitude, longitude], zoom_start = 12)

# instantiate a mark cluster object for the incidents in the dataframe
incidents = plugins.MarkerCluster().add_to(sanfran_map)

# loop through the dataframe and add each data point to the mark cluster
for lat, lng, label, in zip(df_incidents.Y, df_incidents.X, df_incidents.Category):
    folium.Marker(
        location=[lat, lng],
        icon=None,
        popup=label,
    ).add_to(incidents)

# display map
sanfran_map
22/22:
!conda install -c anaconda xlrd --yes
df_can = pd.read_excel('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DV0101EN/labs/Data_Files/Canada.xlsx',
                     sheet_name='Canada by Citizenship',
                     skiprows=range(20),
                     skipfooter=2)

print('Data downloaded and read into a dataframe!')
22/23: df_can.head()
22/24:
# print the dimensions of the dataframe
print(df_can.shape)
22/25:
# clean up the dataset to remove unnecessary columns (eg. REG) 
df_can.drop(['AREA','REG','DEV','Type','Coverage'], axis=1, inplace=True)

# let's rename the columns so that they make sense
df_can.rename(columns={'OdName':'Country', 'AreaName':'Continent','RegName':'Region'}, inplace=True)

# for sake of consistency, let's also make all column labels of type string
df_can.columns = list(map(str, df_can.columns))

# add total column
df_can['Total'] = df_can.sum(axis=1)

# years that we will be using in this lesson - useful for plotting later on
years = list(map(str, range(1980, 2014)))
print ('data dimensions:', df_can.shape)
22/26: df_can.head()
22/27:
# download countries geojson file
!wget --quiet https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DV0101EN/labs/Data_Files/world_countries.json -O world_countries.json
    
print('GeoJSON file downloaded!')
22/28:
world_geo = r'world_countries.json' # geojson file

# create a plain world map
world_map = folium.Map(location=[0, 0], zoom_start=2, tiles='Mapbox Bright')
22/29:
# generate choropleth map using the total immigration of each country to Canada from 1980 to 2013
world_map.choropleth(
    geo_data=world_geo,
    data=df_can,
    columns=['Country', 'Total'],
    key_on='feature.properties.name',
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Immigration to Canada'
)

# display map
world_map
22/30:
world_geo = r'world_countries.json'

# create a numpy array of length 6 and has linear spacing from the minium total immigration to the maximum total immigration
threshold_scale = np.linspace(df_can['Total'].min(),
                              df_can['Total'].max(),
                              6, dtype=int)
threshold_scale = threshold_scale.tolist() # change the numpy array to a list
threshold_scale[-1] = threshold_scale[-1] + 1 # make sure that the last value of the list is greater than the maximum immigration

# let Folium determine the scale.
world_map = folium.Map(location=[0, 0], zoom_start=2, tiles='Mapbox Bright')
world_map.choropleth(
    geo_data=world_geo,
    data=df_can,
    columns=['Country', 'Total'],
    key_on='feature.properties.name',
    threshold_scale=threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Immigration to Canada',
    reset=True
)
world_map
15/597:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        popup=label,
        fill_color='blue',
        fill_opacity=0.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/598:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if np.int(label) == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        popup=label,
        fill_color='blue',
        fill_opacity=0.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/599:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color='blue',
        fill=True,
        popup=label,
        fill_color='blue',
        fill_opacity=0.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/600:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color='blue',
        fill=True,
        popup=label,
        fill_color='blue',
        fill_opacity=0.1
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/601:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color='yellow',
        fill=True,
        popup=label,
        fill_color='blue',
        fill_opacity=0.1
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/602:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
incidents = folium.map.FeatureGroup()

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    incidents.add_child(
        #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, 
            color='yellow',
            fill=True,
            popup=label,
            fill_color='blue',
            fill_opacity=0.1
        )
    )

# show map
map_Toronto_Ontario.add_child(incidents)
15/603:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color='yellow',
        fill=True,
        fill_color='blue',
        fill_opacity=0.1
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/604:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color='yellow',
        fill=True,
        fill_color='blue',
        fill_opacity=0.1
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/605:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) )
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color='blue',
        fill_opacity=0.1
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/606:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) )
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color='blue',
        fill_opacity=0.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/607:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) )
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/608:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/609:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors
15/610:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colors[label])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/611:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colors.iloc[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/612:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/613:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/614:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label.dtype)
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/615:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label.dtypes)
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/616:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/617:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    print(colors[label])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/618:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/619:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    print(label == -1)
    print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/620: Toronto_restaurants['Clus_Db'].unique()
15/621:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

# Compute DBSCAN
db = DBSCAN(eps=0.1, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

Toronto_restaurants.head(10)
15/622: Toronto_restaurants['Clus_Db'].unique()
15/623:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

# Compute DBSCAN
db = DBSCAN(eps=0.2, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

Toronto_restaurants.head(10)
15/624: Toronto_restaurants['Clus_Db'].unique()
15/625:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

# Compute DBSCAN
db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

Toronto_restaurants.head(10)
15/626: Toronto_restaurants['Clus_Db'].unique()
15/627:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors
15/628:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    print(label == -1)
    print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/629:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    print(label == -1)
    print(colors[np.int(label)])
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=[0.4,0.4,0.4],
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/630:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    print(label == -1)
    print(colors[np.int(label)])
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=[0.4, 0.4,0.4,0.4],
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/631:
colors = plt.get_cmap('jet')
colors
15/632:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    print(label == -1)
    print(colors[np.int(label)])
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=[0.4,0.4,0.4],
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/633:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    print(label)
    print(label == -1)
    print(colors[np.int(label)])
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=[0.4,0.4,0.4],
        fill=True,
        fill_color=[0.4,0.4,0.4],
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/634:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors
15/635:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors[0]
15/636:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors[0][0:3]
15/637:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)][0:3])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/638:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)][0:3])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=[0,0,0.4],
        fill=True,
        fill_color=[0,0,0.4],
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/639:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)][0:3])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=[0,0.6,0.4],
        fill=True,
        fill_color=[0,0.6,0.4],
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/640:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)][0:3])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=[0.8,0.6,0.4],
        fill=True,
        fill_color=[0.8,0.6,0.4],
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/641:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors[0][0:3]
colors = []

for i in range(10):
    colors.append('#%06X' % randint(0, 0xFFFFFF))
15/642:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors[0][0:3]
from random import randint
colors = []

for i in range(10):
    colors.append('#%06X' % randint(0, 0xFFFFFF))
15/643:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/644:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    print(c)
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/645:

from random import randint
colors = []

for i in range(10):
    colors.append('#%06X' % randint(0, 0xFFFFFF))
15/646:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors[0][0:3]
from random import randint
colors = []

for i in range(10):
    colors.append('#%06X' % randint(0, 0xFFFFFF))
15/647:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
#colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    print(c)
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/648:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
#colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    #print(c)
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/649:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors[0][0:3]
from random import randint
colors = []

for i in range(clusterNum):
    colors.append('#%06X' % randint(0, 0xFFFFFF))
15/650:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
#colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #print(label)
    #print(label == -1)
    #print(colors[np.int(label)])
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    #print(c)
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/651:
from random import randint
np.random.seed(0)
colors = []

for i in range(clusterNum):
    colors.append('#%06X' % randint(0, 0xFFFFFF))
15/652:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
#colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/653:
from random import randint
np.random.seed(0)
colors = []

for i in range(clusterNum):
    colors.append('#%06X' % randint(0, 0xFFFFFF))
15/654:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
#colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/655: plt.get_cmap('jet')
15/656: plt.get_cmap('jet')[0]
15/657: plt.get_cmap('jet')(0)
15/658: plt.get_cmap('jet')(1)
15/659: plt.get_cmap('jet')(2)
15/660: plt.get_cmap('jet')(3)
15/661: colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
15/662:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors
15/663:
colors = plt.get_cmap('jet')
colors(0)
15/664:
colors = plt.get_cmap('jet')
colors(0:3)
15/665:
colors = plt.get_cmap('jet')
colors(0)(0:3)
15/666:
colors = plt.get_cmap('jet')
colors(0)[0:3]
15/667:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))[0:3]
colors
15/668:
colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors
15/669:
colors = plt.get_cmap('jet')*(np.linspace(0.0, 1.0, clusterNum))
colors
15/670:
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3]
colors
15/671:
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
colors
15/672:
colors = []
for i in range(clusterNum):
    colors.append('#%02x%02x%02x' % plt.get_cmap('jet')(i)[0:3]) 
colors
15/673:
colors = []
for i in range(clusterNum):
    colors.append('#%02x%02x%02x' % (plt.get_cmap('jet')(i)[0:3]) 
colors
15/674:
colors = []
for i in range(clusterNum):
    colors.append('#%02x%02x%02x' % (plt.get_cmap('jet')(i)[0:3]))
colors
15/675:
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
colors
15/676:
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
colors(0)
15/677:
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
colors[0]
15/678:
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
rgb_to_hsv(colors[0])
15/679:
from random import randint

import matplotlib.colors
colors = []
for i in range(clusterNum):
    colors.append('#%06X' % randint(0, 0xFFFFFF))
15/680:
import matplotlib.colors
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
rgb_to_hsv(colors[0])
15/681:
import matplotlib.colors
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
rgb_to_hsv(colors[0])
15/682:
import matplotlib.colors
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
matplotlib.colors.rgb_to_hsv(colors[0])
15/683:
import matplotlib.colors
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
matplotlib.colors.to_hex(colors[0])
15/684:
import matplotlib.colors
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet'))
matplotlib.colors.to_hex(colors[0])
15/685:
import matplotlib.colors
colors = []
for i in range(clusterNum):
    colors.append(plt.get_cmap('jet')(i)[0:3])
matplotlib.colors.to_hex(colors[0])
15/686:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
#colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color='#000080',
        fill=True,
        fill_color='#000080',
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/687:
import matplotlib.colors
colors = []
for i in range(clusterNum):
    colors.append(matplotlib.colors.to_hex(plt.get_cmap('jet')(i)[0:3]))
colors[0]
15/688:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colors = []
for i in range(clusterNum):
    colors.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    #c=(([0.4,0.4,0.4]) if label == -1 else colors[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color='#000080',
        fill=True,
        fill_color='#000080',
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/689:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/690:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colo = []
for i in range(clusterNum):
    colo.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colo[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/691:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colo = []
for i in range(np.linspace(0.0, 1.0, clusterNum)):
    colo.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colo[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/692:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colo = []
for i in range(clusterNum):
    colo.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))
    
colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colo[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/693:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colo = []
for i in range(0,100*clusterNum,round(100/clusterNum)):
    colo.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colo[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=0.5
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/694:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colo = []
for i in range(0,1000*clusterNum,round(1000/clusterNum)):
    colo.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(([0.4,0.4,0.4]) if label == -1 else colo[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/695: Toronto_restaurants['Clus_Db'].unique()
15/696:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colo = []
for i in range(0,1000*clusterNum,round(1000/clusterNum)):
    colo.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/697:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colo = []
for i in range(0,10000*clusterNum,round(10000/clusterNum)):
    colo.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/698:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
colo = []
for i in range(0,100*clusterNum,round(100/clusterNum)):
    colo.append(colors.to_hex(plt.get_cmap('jet')(i)[0:3]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/699:

colors.to_hex([0.1, 0.1, 0.1, 1.0])
15/700:
colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colors.to_hex(colo)
15/701:

colors.to_hex([0.1, 0.1, 0.1, 1.0])
colo[0]
15/702:
colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
colo_hex
15/703:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/704:
Clus_dataSet = Toronto_restaurants[['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

Toronto_restaurants.head(10)
15/705: Toronto_restaurants['Clus_Db'].unique()
15/706:

map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/707:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 1000
    intent = 'browse'
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&intent={}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            intent,
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/708:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude'],
                                    radius = 1000
                                  )
15/709: Toronto_venues.head(20)
15/710: Toronto_venues.shape
15/711: Toronto_venues.isnull().sum()
15/712: Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/713: Toronto_venues.dropna(inplace=True)
15/714: Toronto_venues.shape
15/715:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/716:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/717: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/718: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/719: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/720: print(Toronto_restaurants.shape[0])
15/721:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 1000
    intent = 'browse'
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&intent={}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            intent,
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['id'],
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue_ID',
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/722:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 1000
    intent = 'browse'
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&intent={}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            intent,
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['id'],
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue_ID',
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/723:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude'],
                                    radius = 1000
                                  )
15/724: Toronto_venues.head(20)
15/725:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 1000
    intent = 'browse'
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&intent={}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            intent,
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['id'],
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            v['location']['distance'],
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue_ID',
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Distance',
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/726:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude'],
                                    radius = 1000
                                  )
15/727: Toronto_venues.head(20)
15/728: Toronto_venues.shape
15/729: Toronto_venues.isnull().sum()
15/730: Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/731: Toronto_venues.dropna(inplace=True)
15/732: Toronto_venues.shape
15/733:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/734:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/735: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/736: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/737: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/738: print(Toronto_restaurants.shape[0])
15/739: Toronto_restaurants['Venue_ID']
15/740: Toronto_restaurants['Venue_ID'].iloc[0]
15/741: Toronto_restaurants['Venue_ID'].iloc[0:10]
15/742:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

Toronto_restaurants.head(10)
15/743:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 1000
    intent = 'browse'
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&intent={}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            intent,
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['id'],
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            v['location']['distance'],
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue_ID',
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Distance',
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/744:
Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
                                   latitudes=Toronto_df['Latitude'],
                                   longitudes=Toronto_df['Longitude'],
                                    radius = 1000
                                  )
15/745: Toronto_venues.head(20)
15/746: Toronto_venues.shape
15/747: Toronto_venues.isnull().sum()
15/748: Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/749: Toronto_venues.dropna(inplace=True)
15/750: Toronto_venues.shape
15/751:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/752:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/753: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/754: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/755: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/756: print(Toronto_restaurants.shape[0])
15/757:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return likes

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(v), 
            get_likes(v),
            get_checkins(v),
            get_price(v)) for v in results])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venue_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price']
                            
    
    return(venue_details)
15/758: Toronto_restaurants_details = get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:450])
15/759:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(v), 
            get_likes(v),
            get_checkins(v),
            get_price(v)) for v in results])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venue_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price']
                            
    
    return(venue_details)
15/760: Toronto_restaurants_details = get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:450])
15/761: venues_details
15/762:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(v), 
            get_likes(v),
            get_checkins(v),
            get_price(v)) for v in results])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price']
                            
    
    return(venues_details)
15/763: Toronto_restaurants_details = get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:10])
15/764: Toronto_restaurants_details
15/765: Toronto_restaurants['Venue_ID'].iloc[0:10]
15/766: Toronto_restaurants_details.head(10)
15/767: Toronto_restaurants_details.head(20)
15/768:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(v), 
            get_likes(v),
            get_checkins(v),
            get_price(v)) for v in results])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price']
                            
    
    return(venues_details)
15/769:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(v), 
            get_likes(v),
            get_checkins(v),
            get_price(v)) for v in results])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price']
                            
    
    return(venues_details)
15/770: Toronto_restaurants_details = get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:5])
15/771: Toronto_restaurants_details
15/772:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(v), 
            get_likes(v),
            get_checkins(v),
            get_price(v)) for v in results])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price']
                            
    
    return(results)
15/773: Toronto_restaurants_details = get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])
15/774: Toronto_restaurants_details
15/775: Toronto_restaurants_details['rating']
15/776:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price']
                            
    
    return(venues_details)
15/777: Toronto_restaurants_details = get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])
15/778: Toronto_restaurants_details
15/779: Toronto_restaurants_details = pd.DataFrame({})
15/780: Toronto_restaurants_details
15/781: Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1]))
15/782: Toronto_restaurants_details
15/783: Toronto_restaurants_details = pd.DataFrame({})
15/784: Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1]), inplace = True)
15/785: Toronto_restaurants_details = pd.DataFrame({})
15/786: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1]))
15/787: Toronto_restaurants_details
15/788: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[1:3]))
15/789: Toronto_restaurants_details
15/790: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[3:10]))
15/791: Toronto_restaurants_details
15/792:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(venues_details)
15/793:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(venues_details)
15/794: Toronto_restaurants_details = pd.DataFrame({})
15/795: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1]))
15/796: Toronto_restaurants_details
15/797: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[1:15]))
15/798: Toronto_restaurants_details
15/799: Toronto_restaurants_details = pd.DataFrame({})
15/800: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1]))
15/801: Toronto_restaurants_details
15/802: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[1:3]))
15/803: Toronto_restaurants_details
15/804: Toronto_restaurants_details = pd.DataFrame({})
15/805: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])).reset_index(drop=True)
15/806: Toronto_restaurants_details
15/807: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[1:3])).reset_index(drop=True)
15/808: Toronto_restaurants_details
15/809: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])).reset_index(drop=True)
15/810: Toronto_restaurants_details
15/811:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(results)
15/812: get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])
15/813:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(results)
15/814: get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])
15/815:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(venues_details)
15/816:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(venues_details)
15/817: Toronto_restaurants_details = pd.DataFrame({})
15/818: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])).reset_index(drop=True)
15/819: Toronto_restaurants_details
15/820:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_merged.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/821:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/822:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
print(Toronto_restaurants.head(10))


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/823: set(Toronto_restaurants['Clus_Db'])
15/824:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
    

for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    #label = 'Cluster = {}'.format(neighborhood, borough, round(ratio,2))
    #label = folium.Popup(label, parse_html=True)
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    map_Toronto_Ontario.scatter(clust_set['Venue Latitude'], clust_set['Venue Longitude'], color =c,  marker='o', s= 20, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 
        plt.text(cenx,ceny,str(clust_number), fontsize=25, color='red',)
   

# show map
map_Toronto_Ontario
15/825:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
    

for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    #label = 'Cluster = {}'.format(neighborhood, borough, round(ratio,2))
    #label = folium.Popup(label, parse_html=True)
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    #map_Toronto_Ontario.scatter(clust_set['Venue Latitude'], clust_set['Venue Longitude'], color =c,  marker='o', s= 20, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 
        plt.text(cenx,ceny,str(clust_number), fontsize=25, color='red',)
    
    
    
# show map
map_Toronto_Ontario
15/826:
from folium.features import DivIcon
m = folium.Map([34.0302, -118.2352], zoom_start=13)
folium.map.Marker(
    [34.0302, -118.2352],
    icon=DivIcon(
        icon_size=(150,36),
        icon_anchor=(0,0),
        html='<div style="font-size: 24pt">Test</div>',
        )
    ).add_to(m)
15/827:
from folium.features import DivIcon
m = folium.Map([34.0302, -118.2352], zoom_start=13)
folium.map.Marker(
    [34.0302, -118.2352],
    icon=DivIcon(
        icon_size=(150,36),
        icon_anchor=(0,0),
        html='<div style="font-size: 24pt">Test</div>',
        )
    ).add_to(m)
m
15/828:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
    

for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    #label = 'Cluster = {}'.format(neighborhood, borough, round(ratio,2))
    #label = folium.Popup(label, parse_html=True)
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    #map_Toronto_Ontario.scatter(clust_set['Venue Latitude'], clust_set['Venue Longitude'], color =c,  marker='o', s= 20, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 
        #plt.text(cenx,ceny,str(clust_number), fontsize=25, color='red',)

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">Test</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/829:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
    

for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    #label = 'Cluster = {}'.format(neighborhood, borough, round(ratio,2))
    #label = folium.Popup(label, parse_html=True)
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    #map_Toronto_Ontario.scatter(clust_set['Venue Latitude'], clust_set['Venue Longitude'], color =c,  marker='o', s= 20, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 
        #plt.text(cenx,ceny,str(clust_number), fontsize=25, color='red',)

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/830:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/831:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/832:
Clus_dataSet = Toronto_restaurants[['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
    

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)

# show map
map_Toronto_Ontario
15/833: Toronto_restaurants_details = pd.DataFrame({})
15/834: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])).reset_index(drop=True)
15/835: Toronto_restaurants_details
15/836: Toronto_venues.to_csv('Toronto_venues.csv')
15/837: pd.read_csv('Toronto_venues')
15/838: pd.read_csv('Toronto_venues.csv')
15/839: Toronto_venues
15/840: Toronto_venues.to_csv('Toronto_venues.csv', index = False)
15/841: pd.read_csv('Toronto_venues.csv')
15/842: Toronto_venues.shape
15/843: Toronto_venues.isnull().sum()
15/844: Toronto_venues.isnull().sum()
15/845:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(venues_details)
15/846: Toronto_restaurants_details = pd.DataFrame({})
15/847: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:1])).reset_index(drop=True)
15/848: Toronto_restaurants_details
15/849: print(Toronto_restaurants.shape[0])
15/850:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(venues_details)
15/851: Toronto_restaurants_details = pd.DataFrame({})
15/852: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[0:450])).reset_index(drop=True)
15/853: Toronto_restaurants_details.to_csv('Toronto_restaurants_details.csv', index = False)
15/854: Toronto_restaurants_details
15/855: Toronto_restaurants_details.isnull.sum()
15/856: Toronto_restaurants_details.isnull().sum()
15/857: Toronto_restaurants_details['Rating'].mean()
15/858: hist(Toronto_restaurants_details['Rating'], bins = 10)
15/859: plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
15/860: plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
15/861: plt.hist(Toronto_restaurants_details['Likes'], bins = 50)
15/862: plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
15/863: plt.hist(Toronto_restaurants_details['Price'], bins = 10)
15/864: plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
15/865: Toronto_restaurants_details.isnull().sum()
15/866:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
15/867:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/868:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/869:
plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
plt.xlabel('Likes')
plt.ylabel('Counts')
15/870: Toronto_restaurants_details['Price'].fillna(value = 2.0, inplace = True)
15/871:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/872: Toronto_restaurants_details.isnull().sum()
15/873: Toronto_restaurants_details['Rating'].mean()
15/874: Toronto_restaurants_details['Rating'] = None
15/875: Toronto_restaurants_details.isnull().sum()
15/876: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/877: Toronto_restaurants_details
15/878: Toronto_restaurants_details.isnull().sum()
15/879: Toronto_restaurants_details.drop(['Checkins'], axis = 1, inplace = True)
15/880: Toronto_restaurants_details.drop(['CheckIns'], axis = 1, inplace = True)
15/881:
plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
plt.xlabel('Likes')
plt.ylabel('Counts')
15/882:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/883: Toronto_restaurants_details['Price'].fillna(value = 2.0, inplace = True)
15/884: Toronto_restaurants_details.isnull().sum()
15/885: Toronto_restaurants_details['Rating'] == None
15/886: Toronto_restaurants_details['Rating'].isnull()
15/887: Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]
15/888: len(Toronto_restaurants_details['Rating'].isnull())
15/889: Toronto_restaurants_details.isnull().sum()
15/890: len(Toronto_restaurants_details['Rating'].isnull())
15/891: Toronto_restaurants_details.isnull().sum()
15/892: Toronto_restaurants_details['Rating'].isnull().sum()
15/893: Toronto_restaurants_details['Rating'].fillna(value = Toronto_restaurants_details['Rating'].mean(), inplace = True)
15/894: Toronto_restaurants_details
15/895: Toronto_restaurants_details.isnull().sum()
15/896: Toronto_restaurants.join(Toronto_restaurants_details.set_index('Venue_ID'), on= 'Venue_ID').reset_index(drop=True)
15/897: Toronto_venues = pd.read_csv('Toronto_venues.csv')
15/898: Toronto_venues
15/899: Toronto_venues.shape
15/900: Toronto_venues.isnull().sum()
15/901: Toronto_venues.isnull().sum()
15/902: Toronto_venues.shape
15/903:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/904:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/905: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/906: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/907: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/908: print(Toronto_restaurants.shape[0])
15/909: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/910: Toronto_restaurants_details
15/911: Toronto_restaurants_details.isnull().sum()
15/912: Toronto_restaurants_details.drop(['CheckIns'], axis = 1, inplace = True)
15/913:
plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
plt.xlabel('Likes')
plt.ylabel('Counts')
15/914:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/915: Toronto_restaurants_details['Price'].fillna(value = 2.0, inplace = True)
15/916:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/917: Toronto_restaurants_details['Rating'].fillna(value = Toronto_restaurants_details['Rating'].mean(), inplace = True)
15/918: Toronto_restaurants_details.isnull().sum()
15/919: Toronto_restaurants = Toronto_restaurants.join(Toronto_restaurants_details.set_index('Venue_ID'), on= 'Venue_ID').reset_index(drop=True)
15/920: Toronto_restaurants
15/921:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price, Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/922:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/923:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']].dropna()
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/924:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']].dropna()
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"].dropna()=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/925: Toronto_restaurants.dropna(inplace=True)
15/926: Toronto_restaurants
15/927:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/928:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' str(np.mean(clust_set['Rating']))
                                             + ', Avg Likes: ' str(np.mean(clust_set['Likes'])) 
                                             + ', Avg Tips: ' str(np.mean(clust_set['Tips']))
                                             + ', Avg Price: ' str(np.mean(clust_set['Price']))
                                             + ', Avg Distance: ' str(np.mean(clust_set['Distance']))
             )
    


# show map
map_Toronto_Ontario
15/929: str(12)
15/930:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' str(np.mean(clust_set['Rating'])))
       


# show map
map_Toronto_Ontario
15/931:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(np.mean(clust_set['Rating']))
                                             + ', Avg Likes: ' + str(np.mean(clust_set['Likes'])) 
                                             + ', Avg Tips: ' + str(np.mean(clust_set['Tips']))
                                             + ', Avg Price: ' + str(np.mean(clust_set['Price']))
                                             + ', Avg Distance: ' + str(np.mean(clust_set['Distance']))
             )
    


# show map
map_Toronto_Ontario
15/932:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating'])),2)
                                             + ', Avg Likes: ' + str(np.mean(clust_set['Likes'])) 
                                             + ', Avg Tips: ' + str(np.mean(clust_set['Tips']))
                                             + ', Avg Price: ' + str(np.mean(clust_set['Price']))
                                             + ', Avg Distance: ' + str(np.mean(clust_set['Distance']))
             )
    


# show map
map_Toronto_Ontario
15/933:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(np.mean(clust_set['Likes'])) 
                                             + ', Avg Tips: ' + str(np.mean(clust_set['Tips']))
                                             + ', Avg Price: ' + str(np.mean(clust_set['Price']))
                                             + ', Avg Distance: ' + str(np.mean(clust_set['Distance']))
             )
    


# show map
map_Toronto_Ontario
15/934:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                             + ', Avg Distance: ' + str(round(np.mean(clust_set['Distance']),2))
             )
    


# show map
map_Toronto_Ontario
15/935:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                             + ', Avg Distance: ' + str(round(np.mean(clust_set['Distance']),2))
             )
    


# show map
map_Toronto_Ontario
15/936:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.2, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                             + ', Avg Distance: ' + str(round(np.mean(clust_set['Distance']),2))
             )
    


# show map
map_Toronto_Ontario
15/937:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.7, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                             + ', Avg Distance: ' + str(round(np.mean(clust_set['Distance']),2))
             )
    


# show map
map_Toronto_Ontario
15/938:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                             + ', Avg Distance: ' + str(round(np.mean(clust_set['Distance']),2))
             )
    


# show map
map_Toronto_Ontario
15/939:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price', 'Distance']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                             + ', Avg Distance: ' + str(round(np.mean(clust_set['Distance']),2))
             )
    


# show map
map_Toronto_Ontario
15/940: scatter([1,2,3], [1,2,3])
15/941: plt.scatter([1,2,3], [1,2,3])
15/942:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/943:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop.head()
15/944:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/945: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True)
15/946: Toronto_restaurants
15/947: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True).isnull().sum()
15/948: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True)
15/949: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True).duplicated()
15/950: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True).duplicated().sum()
15/951: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True)
15/952: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True).shape
15/953: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True)
15/954: Toronto_df.join(Toronto_restaurants.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True)
15/955: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True)
15/956: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True)
15/957: Toronto_restaurants.iloc[0:20]
15/958: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True).iloc[0:20]
15/959: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True).groupby(by='Neighbourhood').sum()
15/960: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').reset_index(drop=True).groupby(by='Neighbourhood').count()
15/961: Toronto_restaurants..groupby(by='Neighbourhood').count()
15/962: Toronto_restaurants.groupby(by='Neighbourhood').count()
15/963: Toronto_df['Neighbourhood' == 'Queen\'s Park']
15/964: 'Queen\'s Park'
15/965: Toronto_df[['Neighbourhood' == 'Queen\'s Park']]
15/966: Toronto_df[Toronto_df['Neighbourhood' == 'Queen\'s Park']]
15/967: Toronto_df[Toronto_df['Neighbourhood' == 'Parkwoods']]
15/968: Toronto_df[Toronto_df['Neighbourhood'] == 'Queen\'s Park']
15/969: Toronto_df.duplicates()
15/970: Toronto_df.duplicated()
15/971: Toronto_df.duplicated().sum()
15/972: Toronto_df['Neighbourhood'].duplicated().sum()
15/973: Toronto_df['Neighbourhood'].duplicated()
15/974: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/975: Toronto_df[Toronto_df['Borough'] == 'Etobicoke']
15/976:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
15/977: new_df
15/978: new_df.isnull().sum()
15/979: new_df
15/980: new_df[new_df['Borough'] == 'Etobicoke']
15/981: new_df
15/982: new_df[new_df['Borough'] == 'Etobicoke']
15/983: new_df[new_df['Neighbourhood'] == 'Queen\'s Park']
15/984: new_df[new_df['Borough'] == 'Etobicoke']
15/985: new_df[new_df['Neighbourhood'] == 'Queen\'s Park']
15/986:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/987:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data.isnull().sum()
15/988:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/989: df_merged = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
15/990:
Toronto_df = df_merged
Toronto_df
15/991:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/992: Toronto_df[Toronto_df['Neighbourhood'] == 'Queen\'s Park']
15/993: Toronto_df[Toronto_df['Borough'] == 'Etobicoke']
15/994:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/995:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop.head()
15/996: Toronto_pop[Toronto_pop['Postal Code'] == 'M7A']
15/997: Toronto_pop[Toronto_pop['Postal Code'] == 'M9A']
15/998:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/999: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1000: Toronto_df[Toronto_df['Neighbourhood'] == 'Queen\'s Park']
15/1001: Toronto_df[Toronto_df['Postal Code'] == 'M9A']
15/1002: Toronto_df[Toronto_df['Neighbourhood'] == 'Queen\'s Park']
15/1003:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1004:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop
15/1005: Toronto_pop['Postal Code'].str.contains('M')
15/1006: Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')]
15/1007: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1008: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
15/1009:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1)
15/1010: Toronto_restaurants
15/1011:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1, inplace = True)
15/1012:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1, inplace = True)
15/1013:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
15/1014: new_df[new_df['Neighbourhood'] == 'Queen\'s Park']
15/1015:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/1016:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/1017:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head(10)
15/1018:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants.head
15/1019:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants
15/1020:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/1021: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/1022: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/1023: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/1024: print(Toronto_restaurants.shape[0])
15/1025: Toronto_restaurants_details
15/1026: Toronto_restaurants_details.isnull().sum()
15/1027: Toronto_restaurants = Toronto_restaurants.join(Toronto_restaurants_details.set_index('Venue_ID'), on= 'Venue_ID').reset_index(drop=True)
15/1028: Toronto_restaurants
15/1029: Toronto_restaurants.dropna(inplace=True)
15/1030: Toronto_restaurants
15/1031:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1032:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop
15/1033: Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')]
15/1034:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/1035: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1036:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1, inplace = True)
15/1037: Toronto_restaurants
15/1038: __Since some neighbourhoods are close to each other (or to be specific, they are closer than 1000 m which is the radius indicated in the search query for venues using the Foursquare API), there are duplicates in the resturants that we got. They are not many though.__
15/1039:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1040:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')]
Toronto_pop
15/1041:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1042:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')]
Toronto_pop
15/1043:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')]
Toronto_pop
15/1044:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1045:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')]
Toronto_pop
15/1046:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1047:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1048:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/1049:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1050:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1051:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/1052:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df
15/1053:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/1054:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/1055:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1056:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1057: Toronto_df
15/1058: Toronto_pop
15/1059:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/1060: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1061: __A simple way to explore whether there are sufficient resturants in each neighbourhood to serve all the people living there is by dividing the population number by the number of restaurants in each neighbourhood. This will give us a percentage that shows the need for more resutrants in each neighbourhood.__
15/1062:
__McDonald's is the only restaurant there. <br />
Alhough there could be more restaurants in that area but they are not in the Foursquare database, this could be a very good location to start a new restaurant.__
15/1063: __Using DBSCAN to cluster the restaurants based on their location. That can give us an idea about the high density resturant locations.__
15/1064:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1065:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    folium.features.CircleMarker(
        [lat, lng],
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1066:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {}'.format(neigh, boro)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/1067:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/1068:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1069: Toronto_restaurants
15/1070: plt.scatter(Toronto_restaurants['Distance'], Toronto_restaurants['Rating'])
15/1071: Toronto_restaurants['Rating'].(kind='box')
15/1072: Toronto_restaurants['Rating'].plot(kind='box')
15/1073: Toronto_restaurants['Rating', 'Distance'].plot(kind='box')
15/1074: Toronto_restaurants[['Rating', 'Distance']].plot(kind='box')
15/1075: Toronto_restaurants['Distance'].plot(kind='box')
15/1076: Toronto_restaurants['Rating'].plot(kind='box')
15/1077: Toronto_restaurants['Likes'].plot(kind='box')
15/1078:

plt.scatter(Restaurants_per_neigh['Population, 2016'], Restaurants_per_neigh['Restaurants'])
15/1079: plt.scatter(Toronto_restaurants['Population, 2016'], Toronto_restaurants['Rating'])
15/1080:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils
from sklearn import linear_model

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/1081: Toronto_restaurants_details
15/1082: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/1083: Toronto_restaurants_details
15/1084: print(Toronto_restaurants.shape[0])
15/1085: print(Toronto_restaurants.shape[0])
15/1086: print(Toronto_restaurants.shape[0])
15/1087:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df
15/1088:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/1089:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/1090:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1091:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1092: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1093:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto, Ontario are {}, {}.'.format(latitude, longitude))
15/1094:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario)  

map_Toronto_Ontario
15/1095: Toronto_venues = pd.read_csv('Toronto_venues.csv')
15/1096: Toronto_venues
15/1097: Toronto_venues.shape
15/1098: Toronto_venues.isnull().sum()
15/1099: Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/1100: Toronto_venues.dropna(inplace=True)
15/1101: Toronto_venues.shape
15/1102:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants
15/1103:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/1104: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/1105: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/1106: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/1107: print(Toronto_restaurants.shape[0])
15/1108: Toronto_restaurants_details = pd.DataFrame({})
15/1109: Toronto_restaurants_details
15/1110: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/1111: Toronto_restaurants_details
15/1112: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[450:455])).reset_index(drop=True)
15/1113: Toronto_restaurants_details
15/1114: Toronto_restaurants_details = pd.DataFrame({})
15/1115: Toronto_restaurants_details
15/1116: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/1117: Toronto_restaurants_details
15/1118: Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[450:894])).reset_index(drop=True)
15/1119: Toronto_restaurants_details
15/1120: Toronto_restaurants_details.to_csv('Toronto_restaurants_details.csv', index = False)
15/1121: Toronto_restaurants_details
15/1122: Toronto_restaurants_details.isnull().sum()
15/1123: Toronto_restaurants_details.isnull().sum()
15/1124: Toronto_restaurants_details.drop(['CheckIns'], axis = 1, inplace = True)
15/1125:
plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
plt.xlabel('Likes')
plt.ylabel('Counts')
15/1126:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/1127: Toronto_restaurants_details['Price'].fillna(value = 2.0, inplace = True)
15/1128:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1129: Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]
15/1130: Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating']
15/1131: Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]
15/1132: Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]
15/1133:
Test = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
Test
15/1134: plt.scatter(Test['Likes'], Test['Rating'])
15/1135: plt.scatter(Test['Price'], Test['Rating'])
15/1136: plt.scatter(Test['Tips'], Test['Rating'])
15/1137: plt.scatter(Test['Likes'], Test['Rating'])
15/1138:
Test = Test[Test['Rating']<100]
Test
15/1139:
Test = Test[Test['Likes']<100]
Test
15/1140: plt.scatter(Test['Likes'], Test['Rating'])
15/1141:
Test = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
Test
15/1142: plt.scatter(Test['Likes'], Test['Rating'])
15/1143:
Test = Test[Test['Likes']<100]
plt.scatter(Test['Likes'], Test['Rating'])
15/1144:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
temp
15/1145: plt.scatter(temp['Likes'], temp['Rating'])
15/1146:
temp = temp[temp['Likes']<100]
plt.scatter(temp['Likes'], temp['Rating'])
15/1147:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils
from sklearn import linear_model

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

import seaborn as sns

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/1148: sns.regplot(x="Likes", y="Rating", data=temp)
15/1149:
temp = temp[temp['Likes']<100]
sns.regplot(x="Likes", y="Rating", data=temp)
15/1150: temp[["Likes", "Rating"]].corr()
15/1151:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
temp
15/1152:
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1153:
temp = temp[temp['Likes']<100]
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1154:
sns.regplot(x="Tips", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1155:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
temp
15/1156:
sns.regplot(x="Tips", y="Rating", data=temp)
temp[["Tips", "Rating"]].corr()
15/1157: regr = LinearRegression()
15/1158: regr = linear_model.LinearRegression()
15/1159: x_train, x_test, y_train, y_test = train_test_split(temp['Likes'], temp['Rating'], test_size=0.2, random_state=0)
15/1160:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

import seaborn as sns

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/1161: x_train, x_test, y_train, y_test = train_test_split(temp['Likes'], temp['Rating'], test_size=0.2, random_state=0)
15/1162:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score()
15/1163:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1164: x_train, x_test, y_train, y_test = train_test_split(temp[['Likes']], temp[['Rating']], test_size=0.2, random_state=0)
15/1165:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1166:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_test, y_test)
15/1167: yhat = regr.predict(x_test)
15/1168:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

import seaborn as sns

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/1169:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1170: regr.score(yhat, y_test)
15/1171:
yhat = regr.predict(x_train)
r2_score(yhat, y_test)
15/1172:
yhat = regr.predict(x_train)
r2_score(yhat, y_train)
15/1173: regr.score(yhat, y_train)
15/1174:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1175:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1176:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1177:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1178: regr.score(yhat, y_test)
15/1179:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1180:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1181: yhat
15/1182: y_test
15/1183: x_train, x_test, y_train, y_test = train_test_split(temp[['Likes']], temp['Rating'], test_size=0.2, random_state=0)
15/1184:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1185:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1186: y_test
15/1187: y_hat
15/1188: yhat
15/1189: yhat
15/1190: x_train, x_test, y_train, y_test = train_test_split(temp[['Likes']], temp[['Rating']], test_size=0.2, random_state=0)
15/1191:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1192:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1193: y_test
15/1194:
temp = Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
temp
15/1195: Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]
15/1196: Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating']
15/1197:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
#Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'] = np.random.uniform(low =, high=)
temp.max()
15/1198:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
#Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'] = np.random.uniform(low =, high=)
temp.min()
15/1199:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
#Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=)
temp.shape
15/1200:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
#Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=)
temp.shape[0]
15/1201:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
#Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=)
Toronto_restaurants_details.shape[0]-temp.shape[0]
15/1202:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=Toronto_restaurants_details.shape[0]-temp.shape[0])
15/1203: Toronto_restaurants_details
15/1204: Toronto_restaurants_details.isnull()
15/1205: Toronto_restaurants_details[Toronto_restaurants_details.isnull()]
15/1206: Toronto_restaurants_details
15/1207: Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]
15/1208: Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'].iloc[:]
15/1209:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'].iloc[:] = np.random.uniform(low = temp.min(), high=temp.max(), size=Toronto_restaurants_details.shape[0]-temp.shape[0])
15/1210: Toronto_restaurants_details
15/1211: Toronto_restaurants_details['Rating'] == None
15/1212: Toronto_restaurants_details['Rating'] == isnull()
15/1213: Toronto_restaurants_details['Rating'] == null
15/1214: Toronto_restaurants_details['Rating'] == 'NaN'
15/1215: Toronto_restaurants_details['Rating']
15/1216: Toronto_restaurants_details
15/1217:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
#Toronto_restaurants_details[Toronto_restaurants_details['Rating'].isnull()]['Rating'].iloc[:] = np.random.uniform(low = temp.min(), high=temp.max(), size=Toronto_restaurants_details.shape[0]-temp.shape[0])
Toronto_restaurants_details.loc[Toronto_restaurants_details['Rating'].isnull(), 'Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=Toronto_restaurants_details.shape[0]-temp.shape[0])
15/1218: Toronto_restaurants_details
15/1219:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1220: Toronto_restaurants_details.isnull().sum()
15/1221: Toronto_restaurants = Toronto_restaurants.join(Toronto_restaurants_details.set_index('Venue_ID'), on= 'Venue_ID').reset_index(drop=True)
15/1222: Toronto_restaurants
15/1223: Toronto_restaurants.isnull().sum()
15/1224:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1, inplace = True)
15/1225: Toronto_restaurants
15/1226: Toronto_restaurants.isnull().sum()
15/1227:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/1228:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/1229:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighbourhood').count()
Toronto_neigh_restaurants_count
15/1230:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (18,18))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/1231:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index().join(Toronto_df.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/1232: Toronto_neigh_restaurants_count.reset_index()
15/1233:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index()[['Neighbourhood', 'Venue']].join(Toronto_df.set_index('Neighbourhood'), on='Neighbourhood').drop(['Neighborhood Latitude', 'Neighborhood Longitude', 'Venue Latitude', 'Venue Longitude', 'Venue Category'], axis = 1).rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/1234:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index()[['Neighbourhood', 'Venue']].join(Toronto_df.set_index('Neighbourhood'), on='Neighbourhood').rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/1235:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/1236: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Westmount']
15/1237: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Weston']
15/1238: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Stn A PO Boxes 25 The Esplanade']
15/1239: Toronto_restaurants['Neighbourhood'].unique().count()
15/1240: Toronto_restaurants['Neighbourhood'].unique().sum()
15/1241: Toronto_restaurants['Neighbourhood'].unique()
15/1242: Toronto_restaurants['Neighbourhood'].unique().shape
15/1243: Toronto_df['Neighbourhood'].unique().shape
15/1244:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_df['Population, 2016'].min(), Toronto_df['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_df,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_df['Latitude'], Toronto_df['Longitude'], Toronto_df['Borough'], Toronto_df['Neighbourhood'], Toronto_df['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/1245: Toronto_df
15/1246:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df
15/1247:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/1248:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/1249:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1250:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1251:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/1252: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1253:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto, Ontario are {}, {}.'.format(latitude, longitude))
15/1254:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario)  

map_Toronto_Ontario
15/1255: Toronto_geo = 'Data/Toronto_geo.geojson'
15/1256:
CLIENT_ID = 'DCFSCS0V02LYBBYV3K5ILCWJHPJNGIAZQHKECTF0PPKQVBUP' # your Foursquare ID
CLIENT_SECRET = 'KBJM2SSNVT1I2VUC3F4OJDDSLVXLOHQAVCNC0YRXDKZ4EVYF' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)
15/1257: Toronto_venues = pd.read_csv('Toronto_venues.csv')
15/1258: Toronto_venues
15/1259: Toronto_venues.shape
15/1260: Toronto_venues.isnull().sum()
15/1261: #Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/1262: Toronto_venues.dropna(inplace=True)
15/1263: Toronto_venues.shape
15/1264:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants
15/1265:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/1266: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/1267: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/1268: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/1269: print(Toronto_restaurants.shape[0])
15/1270: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/1271: Toronto_restaurants_details
15/1272: Toronto_restaurants.groupby(by='Neighbourhood')
15/1273: Toronto_restaurants.groupby(by='Neighbourhood').sum()
15/1274: Toronto_restaurants_details.isnull().sum()
15/1275: Toronto_restaurants_details.drop(['CheckIns'], axis = 1, inplace = True)
15/1276:
plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
plt.xlabel('Likes')
plt.ylabel('Counts')
15/1277:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/1278: Toronto_restaurants_details['Price'].fillna(value = 2.0, inplace = True)
15/1279:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1280:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
temp
15/1281:
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1282:
temp = temp[temp['Likes']<100]
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1283: x_train, x_test, y_train, y_test = train_test_split(temp[['Likes']], temp[['Rating']], test_size=0.2, random_state=0)
15/1284:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1285:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1286:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
Toronto_restaurants_details.loc[Toronto_restaurants_details['Rating'].isnull(), 'Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=Toronto_restaurants_details.shape[0]-temp.shape[0])
Toronto_restaurants_details
15/1287:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1288: Toronto_restaurants = Toronto_restaurants.join(Toronto_restaurants_details.set_index('Venue_ID'), on= 'Venue_ID').reset_index(drop=True)
15/1289: Toronto_restaurants
15/1290:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1, inplace = True)
15/1291: Toronto_restaurants
15/1292: Toronto_restaurants.isnull().sum()
15/1293: Toronto_restaurants.isnull().sum()
15/1294: Toronto_restaurants[Toronto_restaurants['Postal Code'].isnull()]
15/1295: Toronto_df['Neighbourhood'] == 'Canada Post Gateway Processing Centre'
15/1296: Toronto_df[Toronto_df['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1297: Toronto_df[Toronto_df['Neighbourhood'].str.contains('Canada')]
15/1298: Toronto_df[Toronto_df['Neighbourhood'].str.contains('Canada Post Gateway Processing Centre')]
15/1299: Toronto_venues[Toronto_venues['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1300: Toronto_venues
15/1301: Toronto_venues.groupby['Neighbourhood'].sum()
15/1302: Toronto_venues.groupby[by='Neighbourhood'].sum()
15/1303: Toronto_venues.groupby(by='Neighbourhood').sum()
15/1304: Toronto_venues
15/1305: Toronto_df.shape
15/1306: Toronto_df[Toronto_df['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1307:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df
15/1308: new_df[new_df['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1309:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/1310:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/1311:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1312:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1313: Toronto_df[Toronto_df['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1314: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1315: Toronto_df[Toronto_df['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1316:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto, Ontario are {}, {}.'.format(latitude, longitude))
15/1317:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario)  

map_Toronto_Ontario
15/1318: Toronto_venues = pd.read_csv('Toronto_venues.csv')
15/1319: Toronto_venues
15/1320: Toronto_venues[Toronto_venues[Neighbourhood] == 'Canada Post Gateway Processing Centre']
15/1321: Toronto_venues[Toronto_venues['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1322: Toronto_venues
15/1323: Toronto_venues.shape
15/1324: Toronto_venues.isnull().sum()
15/1325: Toronto_venues.dropna(inplace=True)
15/1326: Toronto_venues.shape
15/1327:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants
15/1328: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1329:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/1330: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/1331: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/1332: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/1333: print(Toronto_restaurants.shape[0])
15/1334: [Toronto_restaurants['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1335: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1336: print(Toronto_restaurants.shape[0])
15/1337: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/1338: Toronto_restaurants_details
15/1339: Toronto_restaurants_details.isnull().sum()
15/1340: Toronto_restaurants_details.drop(['CheckIns'], axis = 1, inplace = True)
15/1341:
plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
plt.xlabel('Likes')
plt.ylabel('Counts')
15/1342:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/1343: Toronto_restaurants_details['Price'].fillna(value = 2.0, inplace = True)
15/1344:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1345:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
temp
15/1346:
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1347:
temp = temp[temp['Likes']<100]
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1348: x_train, x_test, y_train, y_test = train_test_split(temp[['Likes']], temp[['Rating']], test_size=0.2, random_state=0)
15/1349:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1350:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1351:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
Toronto_restaurants_details.loc[Toronto_restaurants_details['Rating'].isnull(), 'Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=Toronto_restaurants_details.shape[0]-temp.shape[0])
Toronto_restaurants_details
15/1352:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1353: Toronto_restaurants = Toronto_restaurants.join(Toronto_restaurants_details.set_index('Venue_ID'), on= 'Venue_ID').reset_index(drop=True)
15/1354: Toronto_restaurants
15/1355: Toronto_restaurants.isnull().sum()
15/1356: Toronto_df[Toronto_df['Neighbourhood' == 'Canada Post Gateway Processing Centre']]
15/1357: Toronto_df[Toronto_df['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1358: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Canada Post Gateway Processing Centre']
15/1359: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
15/1360: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').isnull.sum()
15/1361: Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood').isnull().sum()
15/1362:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1, inplace = True)
15/1363: Toronto_restaurants
15/1364: Toronto_restaurants.isnull().sum()
15/1365:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/1366:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/1367:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighbourhood').count()
Toronto_neigh_restaurants_count
15/1368: Toronto_restaurants['Neighbourhood'].unique().shape
15/1369: Toronto_venues['Neighbourhood'].unique().shape
15/1370: Toronto_df['Neighbourhood'].unique().shape
15/1371: Toronto_restaurants['Neighbourhood'].unique().shape
15/1372: Toronto_Venues['Neighbourhood'].unique().shape
15/1373: Toronto_venues['Neighbourhood'].unique().shape
15/1374:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (18,18))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/1375:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index()[['Neighbourhood', 'Venue']].join(Toronto_df.set_index('Neighbourhood'), on='Neighbourhood').rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/1376:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/1377:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_df['Population, 2016'].min(), Toronto_df['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_df,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_df['Latitude'], Toronto_df['Longitude'], Toronto_df['Borough'], Toronto_df['Neighbourhood'], Toronto_df['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/1378: Toronto_df
15/1379: Toronto_df
15/1380:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df
15/1381:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/1382:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/1383:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1384:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1385:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/1386: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1387:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto, Ontario are {}, {}.'.format(latitude, longitude))
15/1388:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario)  

map_Toronto_Ontario
15/1389: Toronto_geo = 'Data/Toronto_geo.geojson'
15/1390:
CLIENT_ID = 'DCFSCS0V02LYBBYV3K5ILCWJHPJNGIAZQHKECTF0PPKQVBUP' # your Foursquare ID
CLIENT_SECRET = 'KBJM2SSNVT1I2VUC3F4OJDDSLVXLOHQAVCNC0YRXDKZ4EVYF' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)
15/1391: Toronto_venues = pd.read_csv('Toronto_venues.csv')
15/1392: Toronto_venues
15/1393: Toronto_venues.shape
15/1394: Toronto_venues.isnull().sum()
15/1395: Toronto_venues.dropna(inplace=True)
15/1396: Toronto_venues.shape
15/1397:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants
15/1398:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/1399: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/1400: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/1401: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/1402: print(Toronto_restaurants.shape[0])
15/1403: Toronto_restaurants_details = pd.DataFrame({})
15/1404: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/1405: Toronto_restaurants_details
15/1406: Toronto_restaurants_details.isnull().sum()
15/1407: Toronto_restaurants_details.drop(['CheckIns'], axis = 1, inplace = True)
15/1408:
plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
plt.xlabel('Likes')
plt.ylabel('Counts')
15/1409:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/1410: Toronto_restaurants_details['Price'].fillna(value = 2.0, inplace = True)
15/1411:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1412:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
temp
15/1413:
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1414:
temp = temp[temp['Likes']<100]
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1415: x_train, x_test, y_train, y_test = train_test_split(temp[['Likes']], temp[['Rating']], test_size=0.2, random_state=0)
15/1416:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1417:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1418:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
Toronto_restaurants_details.loc[Toronto_restaurants_details['Rating'].isnull(), 'Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=Toronto_restaurants_details.shape[0]-temp.shape[0])
Toronto_restaurants_details
15/1419:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1420: Toronto_restaurants = Toronto_restaurants.join(Toronto_restaurants_details.set_index('Venue_ID'), on= 'Venue_ID').reset_index(drop=True)
15/1421: Toronto_restaurants
15/1422:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1, inplace = True)
15/1423: Toronto_restaurants
15/1424: Toronto_restaurants.isnull().sum()
15/1425: Toronto_df
15/1426: Toronto_df[Toronto_df['Postal Code'] == 'M7R']
15/1427: Toronto_pop[Toronto_pop['Postal Code'] == 'M7R']
15/1428: Toronto_pop.join(Toronto_df.set_index('Postal Code'), on='Postal Code')
15/1429:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df
15/1430:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/1431:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/1432:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1433:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1434: Toronto_pop.join(Toronto_df.set_index('Postal Code'), on='Postal Code')
15/1435: Toronto_pop[Toronto_pop['Postal Code'] == 'M7R']
15/1436: Toronto_df[Toronto_df['Postal Code'] == 'M7R']
15/1437:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/1438: Toronto_df
15/1439: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1440:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_df['Population, 2016'].min(), Toronto_df['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_df,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_df['Latitude'], Toronto_df['Longitude'], Toronto_df['Borough'], Toronto_df['Neighbourhood'], Toronto_df['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/1441:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

import seaborn as sns

!conda install -c conda-forge folium=0.5.0 --yes
import folium

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
15/1442:
url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
result = requests.get(url)
soup = BeautifulSoup(result.content, 'lxml')
table = soup.find_all('table')[0]
df = pd.read_html(str(table))
df = df[0]

df = df[df.Borough != 'Not assigned'].reset_index(drop = True)

for i in range(df.shape[0]):
    if df.iloc[i]['Neighbourhood'] == 'Not assigned':
        df.iloc[i]['Neighbourhood'] = df.iloc[i]['Borough']
        
new_df = pd.DataFrame(columns = {})
new_df = new_df.append(df.iloc[0])

for i in range(1, df.shape[0]):
    for j in range(new_df.shape[0]):
        if df.Postcode.iloc[i] == new_df.Postcode.iloc[j]:
            new_df.Neighbourhood.iloc[j] = new_df.Neighbourhood.iloc[j] + ', ' + df.Neighbourhood.iloc[i] 
            found = 1
        else:
            found = 0
    if found == 0:
        new_df = new_df.append(df.iloc[i])
          
new_df = new_df[['Postcode', 'Borough', 'Neighbourhood']]
new_df.rename(columns={'Postcode' : 'Postal Code'}, inplace = True)
new_df
15/1443:
data = pd.read_csv('https://cocl.us/Geospatial_data')   
data
15/1444:
Toronto_df = new_df.join(data.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df
15/1445:
Toronto_pop = pd.read_csv('Data/Toronto_pop.csv')
Toronto_pop.head()
15/1446:
Toronto_pop = Toronto_pop[['Geographic code','Province or territory', 'Population, 2016']]
Toronto_pop.rename(columns = {'Geographic code' : 'Postal Code'}, inplace = True)
Toronto_pop=Toronto_pop[(Toronto_pop['Province or territory'] == "Ontario")].reset_index(drop=True)
Toronto_pop = Toronto_pop[Toronto_pop['Postal Code'].str.startswith('M')].reset_index(drop=True)
Toronto_pop
15/1447: Toronto_pop[Toronto_pop['Postal Code'] == 'M7R']
15/1448: Toronto_df[Toronto_df['Postal Code'] == 'M7R']
15/1449:
Toronto_df = Toronto_df.join(Toronto_pop.set_index('Postal Code'), on='Postal Code').reset_index(drop = True)
Toronto_df.drop('Province or territory', axis = 1, inplace = True)
Toronto_df.dropna(inplace=True)
Toronto_df
15/1450: Toronto_df[Toronto_df['Neighbourhood'].duplicated()]
15/1451:
address = 'Toronto, Ontario'
geolocator = Nominatim()
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto, Ontario are {}, {}.'.format(latitude, longitude))
15/1452:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)

for lat, lng, borough, neighborhood in zip(df_merged['Latitude'], df_merged['Longitude'], df_merged['Borough'], df_merged['Neighbourhood']):
    label = '{}, {}'.format(neighborhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_Toronto_Ontario)  

map_Toronto_Ontario
15/1453: Toronto_geo = 'Data/Toronto_geo.geojson'
15/1454:
CLIENT_ID = 'DCFSCS0V02LYBBYV3K5ILCWJHPJNGIAZQHKECTF0PPKQVBUP' # your Foursquare ID
CLIENT_SECRET = 'KBJM2SSNVT1I2VUC3F4OJDDSLVXLOHQAVCNC0YRXDKZ4EVYF' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)
15/1455:
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

    
def get_postal_code(row):
    try:
        postal_code = row['location']['postalCode']
    except:
        postal_code = None
        
    return postal_code
    
    
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    LIMIT = 1000
    intent = 'browse'
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&intent={}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            intent,
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venues']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['id'],
            v['name'], 
            v['location']['lat'], 
            v['location']['lng'], 
            v['location']['distance'],
            get_postal_code(v),
            get_category_type(v)) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighbourhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue_ID',
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Distance',
                  'Postal Code',
                  'Venue Category']
    
    return(nearby_venues)
15/1456:
#Toronto_venues = getNearbyVenues(names=Toronto_df['Neighbourhood'],
#                                   latitudes=Toronto_df['Latitude'],
#                                   longitudes=Toronto_df['Longitude'],
#                                    radius = 1000
#                                  )
15/1457: #Toronto_venues.to_csv('Toronto_venues.csv', index = False)
15/1458: Toronto_venues = pd.read_csv('Toronto_venues.csv')
15/1459: Toronto_venues
15/1460: Toronto_venues.shape
15/1461: Toronto_venues.isnull().sum()
15/1462: #Toronto_venues.drop(['Postal Code'], axis = 1, inplace = True)
15/1463: Toronto_venues.dropna(inplace=True)
15/1464: Toronto_venues.shape
15/1465:
Toronto_restaurants = Toronto_venues[Toronto_venues['Venue Category'].str.contains("Restaurant")].reset_index(drop=True)
Toronto_restaurants
15/1466:
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants[['Venue','Venue Latitude', 'Venue Longitude']].drop_duplicates().shape[0])
15/1467: Toronto_restaurants[Toronto_restaurants[['Venue', 'Venue Latitude', 'Venue Longitude']].duplicated()]
15/1468: Toronto_restaurants[Toronto_restaurants['Venue'] == 'Jerk Delight']
15/1469: Toronto_restaurants.drop_duplicates(subset =['Venue','Venue Latitude', 'Venue Longitude'], inplace=True)
15/1470: print(Toronto_restaurants.shape[0])
15/1471:
def get_rating(row):
    try:
        rating = row['rating']
    except:
        rating = None        
    return rating

def get_likes(row):
    try:
        likes = row['likes']['count']
    except:
        likes = None        
    return likes

def get_checkins(row):
    try:
        checkins = row['stats']['checkinsCount']
    except:
        checkins = None        
    return checkins

def get_price(row):
    try:
        price = row['price']['tier']
    except:
        price = None        
    return price

def get_tips(row):
    try:
        tips = row['tips']['count']
    except:
        tips = None        
    return tips
    
def get_venue_details(Venue_ID):
    venues_list=[]
    for venue_id in Venue_ID:
        print('. ')
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/{}?client_id={}&client_secret={}&v={}'.format(venue_id, CLIENT_ID, CLIENT_SECRET, VERSION)
            
        # make the GET request
        results = requests.get(url).json()["response"]['venue']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            venue_id, 
            get_rating(results), 
            get_likes(results),
            get_checkins(results),
            get_price(results), 
            get_tips(results))])

    venues_details = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    venues_details.columns = ['Venue_ID', 
                             'Rating',
                             'Likes',
                             'CheckIns',
                             'Price',
                             'Tips']
                            
    
    return(venues_details)
15/1472: Toronto_restaurants_details = pd.DataFrame({})
15/1473: #Toronto_restaurants_details = Toronto_restaurants_details.append(get_venue_details(Toronto_restaurants['Venue_ID'].iloc[450:894])).reset_index(drop=True)
15/1474: #Toronto_restaurants_details.to_csv('Toronto_restaurants_details.csv', index = False)
15/1475: Toronto_restaurants_details = pd.read_csv('Toronto_restaurants_details.csv')
15/1476: Toronto_restaurants_details
15/1477: Toronto_restaurants_details.isnull().sum()
15/1478: Toronto_restaurants_details.drop(['CheckIns'], axis = 1, inplace = True)
15/1479:
plt.hist(Toronto_restaurants_details['Likes'], bins = 10)
plt.xlabel('Likes')
plt.ylabel('Counts')
15/1480:
plt.hist(Toronto_restaurants_details['Price'], bins = 10)
plt.xlabel('Price Tier (1-4)')
plt.ylabel('Counts')
15/1481: Toronto_restaurants_details['Price'].fillna(value = 2.0, inplace = True)
15/1482:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1483:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()].reset_index(drop=True)
temp
15/1484:
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1485:
temp = temp[temp['Likes']<100]
sns.regplot(x="Likes", y="Rating", data=temp)
temp[["Likes", "Rating"]].corr()
15/1486: x_train, x_test, y_train, y_test = train_test_split(temp[['Likes']], temp[['Rating']], test_size=0.2, random_state=0)
15/1487:
regr = linear_model.LinearRegression()
regr.fit(x_train, y_train)
regr.score(x_train, y_train)
15/1488:
yhat = regr.predict(x_test)
r2_score(yhat, y_test)
15/1489:
temp = Toronto_restaurants_details[~Toronto_restaurants_details['Rating'].isnull()]['Rating'].reset_index(drop=True)
Toronto_restaurants_details.loc[Toronto_restaurants_details['Rating'].isnull(), 'Rating'] = np.random.uniform(low = temp.min(), high=temp.max(), size=Toronto_restaurants_details.shape[0]-temp.shape[0])
Toronto_restaurants_details
15/1490:
plt.hist(Toronto_restaurants_details['Rating'], bins = 10)
plt.xlabel('Raing (0-10)')
plt.ylabel('Counts')
15/1491: #Toronto_restaurants_details['Rating'].fillna(value = Toronto_restaurants_details['Rating'].mean(), inplace = True)
15/1492: Toronto_restaurants = Toronto_restaurants.join(Toronto_restaurants_details.set_index('Venue_ID'), on= 'Venue_ID').reset_index(drop=True)
15/1493: Toronto_restaurants
15/1494:
Toronto_restaurants = Toronto_restaurants.join(Toronto_df.set_index('Neighbourhood'), on= 'Neighbourhood')
Toronto_restaurants.drop(['Latitude', 'Longitude'], axis =1, inplace = True)
15/1495: Toronto_restaurants
15/1496: Toronto_restaurants.isnull().sum()
15/1497: Toronto_restaurants[Toronto_restaurants.isnull().sum()]
15/1498:
Toronto_restaurants[Toronto_restaurants.isnull()
                   ]
15/1499: Toronto_restaurants[Toronto_restaurants['Postal Code'].isnull()]
15/1500:
Toronto_restaurants.dropna(inplace=True).reset_index(drop=True)
Toronto_restaurants
15/1501:
Toronto_restaurants.dropna(inplace=True)
Toronto_restaurants.reset_index(drop=True)
15/1502:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/1503:
Toronto_restaurants.dropna(inplace=True)
Toronto_restaurants.reset_index(drop=True)
print(Toronto_restaurants.groupby(by='Neighbourhood').sum().shape)
15/1504:
Toronto_restaurants.dropna(inplace=True)
Toronto_restaurants.reset_index(drop=True)
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants.groupby(by='Neighbourhood').sum().shape[0])
15/1505:
Toronto_restaurants_count = Toronto_restaurants.groupby(by='Venue Category').count()
Toronto_restaurants_count
15/1506:
Toronto_restaurants_count[['Venue']].plot(kind='barh', figsize = (12,12))
plt.xlabel('Total Number of Restaurants')
#plt.annotate('',                      
#             xy=(10, 48),           
#             xytext=(10, 0),    
#             xycoords='data',         
#             arrowprops=dict(arrowstyle='-', connectionstyle='arc3', color='red', lw=2)
#            )
plt.show()
15/1507:
Toronto_neigh_restaurants_count = Toronto_restaurants.groupby(by='Neighbourhood').count()
Toronto_neigh_restaurants_count
15/1508: Toronto_venues['Neighbourhood'].unique().shape
15/1509:
Toronto_neigh_restaurants_count[['Venue']].plot(kind='barh', figsize = (18,18))
plt.xlabel('Total Number of Restaurants')

plt.show()
15/1510:
Restaurants_per_neigh = Toronto_neigh_restaurants_count.reset_index()[['Neighbourhood', 'Venue']].join(Toronto_df.set_index('Neighbourhood'), on='Neighbourhood').rename(columns = {'Venue' : 'Restaurants'}).dropna().reset_index(drop=True)
Restaurants_per_neigh
15/1511:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Restaurants'].min(), Restaurants_per_neigh['Restaurants'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Restaurants'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Number of Restaurants in each Neighbourhood'
)

for lat, lng, borough, neighborhood, pop in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Restaurants']):
    label = '{}, {} - No. of Restaurants = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/1512:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Toronto_df['Population, 2016'].min(), Toronto_df['Population, 2016'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Toronto_df,
    columns=['Postal Code', 'Population, 2016'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Population of Toronto, Ontario'
)

for lat, lng, borough, neighborhood, pop in zip(Toronto_df['Latitude'], Toronto_df['Longitude'], Toronto_df['Borough'], Toronto_df['Neighbourhood'], Toronto_df['Population, 2016']):
    label = '{}, {} - Population = {}'.format(neighborhood, borough, pop)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

# display map
map_Toronto_Ontario
15/1513:
min_max_scaler = preprocessing.MinMaxScaler()
Restaurants_per_neigh['Ratio_pop_neigh'] = min_max_scaler.fit_transform((Restaurants_per_neigh['Population, 2016']/Restaurants_per_neigh['Restaurants']).values.reshape(-1,1).astype(float)) * 100
Restaurants_per_neigh
15/1514:
map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'
threshold_scale = np.linspace(Restaurants_per_neigh['Ratio_pop_neigh'].min(), Restaurants_per_neigh['Ratio_pop_neigh'].max(), 6, dtype = int).tolist()
threshold_scale[-1] = threshold_scale[-1] + 1

map_Toronto_Ontario.choropleth(
    geo_data=Toronto_geo,
    data=Restaurants_per_neigh,
    columns=['Postal Code', 'Ratio_pop_neigh'],
    key_on='feature.properties.CFSAUID',
    threshold_scale = threshold_scale,
    fill_color='YlOrRd', 
    fill_opacity=0.7, 
    line_opacity=0.2,
    legend_name='Ratio of the Population per No. of Restaurants'
)

for lat, lng, borough, neighborhood, ratio in zip(Restaurants_per_neigh['Latitude'], Restaurants_per_neigh['Longitude'], Restaurants_per_neigh['Borough'], Restaurants_per_neigh['Neighbourhood'], Restaurants_per_neigh['Ratio_pop_neigh']):
    label = '{}, {} - Ratio = {}\%'.format(neighborhood, borough, round(ratio,2))
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=1,
        popup=label,
        color='blue',
        fill=False,
        fill_color='#3186cc',
        fill_opacity=0.0,
        parse_html=False).add_to(map_Toronto_Ontario) 

map_Toronto_Ontario
15/1515: Restaurants_per_neigh[Restaurants_per_neigh['Neighbourhood'] =='Willowdale West']
15/1516: Toronto_restaurants[Toronto_restaurants['Neighbourhood'] == 'Willowdale West']
15/1517:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/1518:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.2, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/1519:
Toronto_restaurants.dropna(inplace=True)
Toronto_restaurants.reset_index(drop=True)
print(Toronto_restaurants.shape[0])
print(Toronto_restaurants.groupby(by='Neighbourhood').sum().shape[0])
print(Toronto_restaurants.groupby(by='Borough').sum().shape[0])
15/1520:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1521:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.4, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1522:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1523:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.2, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1524:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1525:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.6, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1526:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.7, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#000000') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1527:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.7, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#000000') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1528:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.7, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1529:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.6, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1530:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1531:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.6, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1532:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.1, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/1533:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/1534:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.6, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/1535:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
    


# show map
map_Toronto_Ontario
15/1536:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.6, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1537:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.5, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1538:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.7, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1539:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.4, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1540:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1541:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.2, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1542:
Clus_dataSet = Toronto_restaurants[['Venue Latitude','Venue Longitude', 'Rating', 'Likes', 'Tips', 'Price']]
Clus_dataSet = preprocessing.StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.6, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
Toronto_restaurants["Clus_Db"]=labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels)) 

print(Toronto_restaurants['Clus_Db'].unique())
Toronto_restaurants.head(10)


map_Toronto_Ontario = folium.Map(location=[latitude, longitude], zoom_start=10)
Toronto_geo = 'Data/Toronto_geo.geojson'


colo = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))
colo_hex = []
for i in range(clusterNum):
    colo_hex.append(colors.to_hex(colo[i]))
 

for lat, lng, label, boro, neigh in zip(Toronto_restaurants['Venue Latitude'], Toronto_restaurants['Venue Longitude'], Toronto_restaurants['Clus_Db'], Toronto_restaurants['Borough'], Toronto_restaurants['Neighbourhood']):
    c=(('#808080') if label == -1 else colo_hex[np.int(label)])
    popuplabel = '{}, {} - Cluster: {}'.format(neigh, boro, label)
    popuplabel = folium.Popup(popuplabel, parse_html=True)
    folium.features.CircleMarker(
        [lat, lng],
        popup=popuplabel,
        radius=5, 
        color=c,
        fill=True,
        fill_color=c,
        fill_opacity=1.0
    ).add_to(map_Toronto_Ontario)


for clust_number in  set(Toronto_restaurants['Clus_Db']):
    c=(('#808080') if clust_number == -1 else colo_hex[np.int(clust_number)])
    clust_set = Toronto_restaurants[Toronto_restaurants['Clus_Db'] == clust_number]
    if clust_number != -1:
        cenx=np.mean(clust_set['Venue Latitude']) 
        ceny=np.mean(clust_set['Venue Longitude']) 

        folium.map.Marker(
            [cenx, ceny],
            icon=DivIcon(
                icon_size=(150,36),
                icon_anchor=(0,0),
                html='<div style="font-size: 24pt">' + str(clust_number) + '</div>',
            )
        ).add_to(map_Toronto_Ontario)
        
        print('Cluster ' + str(clust_number) + ', Avg Rating: ' + str(round(np.mean(clust_set['Rating']),2))
                                             + ', Avg Likes: ' + str(round(np.mean(clust_set['Likes']),2)) 
                                             + ', Avg Tips: ' + str(round(np.mean(clust_set['Tips']),2))                                           
                                             + ', Avg Price: ' + str(round(np.mean(clust_set['Price']),2))
                                            
             )
    


# show map
map_Toronto_Ontario
15/1543: plt.scatter(Toronto_restaurants['Distance'], Toronto_restaurants['Likes'])
15/1544: plt.scatter(Toronto_restaurants['Distance'], Toronto_restaurants['Rating'])
24/1: from watson_machine_learning_client import WatsonMachineLearningAPIClient
26/1:
%%bash
mkdir labb4
cd labb4
git clone https://github.com/sophiajwchoi/Lab4-Adding-Discovery-to-Chatbot.git
26/2:
%%bash
mkdir labb4
cd labb4
git clone https://github.com/sophiajwchoi/Lab4-Adding-Discovery-to-Chatbot.git
27/1:
%%bash
mkdir labb4
cd labb4
git clone https://github.com/sophiajwchoi/Lab4-Adding-Discovery-to-Chatbot.git
27/2:
%%bash
mkdir labb4
cd labb4
git clone https://github.com/sophiajwchoi/Lab4-Adding-Discovery-to-Chatbot.git
28/1:
import pandas as pd
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
28/2:
import pandas as pd
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
28/3:
concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')
concrete_data.head()
28/4: concrete_data.shape
28/5: concrete_data.isnull().sum()
28/6:
concrete_data_columns = concrete_data.columns
predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength
target = concrete_data['Strength'] # Strength column
28/7:
x = predictors
x.head()
28/8:
y = target
y.head()
28/9:
n_cols = x.shape[1] # number of predictors
n_cols
28/10:
# define regression model
def regression_model():
    # create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(1))
    
    # compile model
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model
28/11:
loops = 50
epochs = 50
MSE = np.zeros(loops)
for i in range(loops):
    
    #Spliting the data into 70% training and 30% testing
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)
    
    # build the model
    model = regression_model()
    
    # fit the model
    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, verbose=0)
    
    #predict the strength values
    y_hat = model.predict(x_test)
    
    #calculate the mean squared error
    MSE[i] = mean_squared_error(y_test, y_hat)
28/12:
Mean_MSE = MSE.mean()
Std_MSE = MSE.std()
print('Part A')
print('Mean: ' + str(round(Mean_MSE,2)) + ', Standard deviation: ' + str(round(Std_MSE, 2)))
28/13:
# define regression model
def regression_model():
    # create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(1))
    
    # compile model
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model
28/14:
loops = 50
epochs = 50
MSE = np.zeros(loops)
for i in range(loops):
    
    #normalize the data
    x_norm = (x - x.mean()) / x.std()
    
    #Spliting the data into 70% training and 30% testing
    x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=0.3, random_state=4)
    
    # build the model
    model = regression_model()
    
    # fit the model
    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, verbose=0)
    
    #predict the strength values
    y_hat = model.predict(x_test)
    
    #calculate the mean squared error
    MSE[i] = mean_squared_error(y_test, y_hat)
28/15:
Mean_MSE = MSE.mean()
Std_MSE = MSE.std()
print('Part B')
print('Mean: ' + str(round(Mean_MSE,2)) + ', Standard deviation: ' + str(round(Std_MSE, 2)))
28/16:
# define regression model
def regression_model():
    # create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(1))
    
    # compile model
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model
28/17:
loops = 50
epochs = 100
MSE = np.zeros(loops)
for i in range(loops):
    
    #normalize the data
    x_norm = (x - x.mean()) / x.std()
    
    #Spliting the data into 70% training and 30% testing
    x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=0.3, random_state=4)
    
    # build the model
    model = regression_model()
    
    # fit the model
    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, verbose=0)
    
    #predict the strength values
    y_hat = model.predict(x_test)
    
    #calculate the mean squared error
    MSE[i] = mean_squared_error(y_test, y_hat)
28/18:
Mean_MSE = MSE.mean()
Std_MSE = MSE.std()
print('Part C')
print('Mean: ' + str(round(Mean_MSE,2)) + ', Standard deviation: ' + str(round(Std_MSE, 2)))
28/19: __Result: The mean on MSE in Part C is smaller than that in Part B__
28/20:
# define regression model
def regression_model():
    # create model
    model = Sequential()
    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(1))
    
    # compile model
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model
28/21:
loops = 50
epochs = 50
MSE = np.zeros(loops)
for i in range(loops):
    
    #normalize the data
    x_norm = (x - x.mean()) / x.std()
    
    #Spliting the data into 70% training and 30% testing
    x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=0.3, random_state=4)
    
    # build the model
    model = regression_model()
    
    # fit the model
    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, verbose=0)
    
    #predict the strength values
    y_hat = model.predict(x_test)
    
    #calculate the mean squared error
    MSE[i] = mean_squared_error(y_test, y_hat)
28/22:
Mean_MSE = MSE.mean()
Std_MSE = MSE.std()
print('Part D')
print('Mean: ' + str(round(Mean_MSE,2)) + ', Standard deviation: ' + str(round(Std_MSE, 2)))
34/1:
# Import the libraries we need for this lab

# Using the following line code to install the torchvision library
!conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np
torch.manual_seed(2)
34/2:
# Import the libraries we need for this lab

# Using the following line code to install the torchvision library
!conda install -c pytorch pytorch
!conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np
torch.manual_seed(2)
35/1: <!--Empty Space for separating topics-->
35/2:
# Import the libraries we need for this lab

# Using the following line code to install the torchvision library
!conda install -c pytorch pytorch
!conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np
torch.manual_seed(2)
36/1:
# Import the libraries we need for this lab

# Using the following line code to install the torchvision library
#!conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np
torch.manual_seed(2)
36/2:
# Create the model class using sigmoid as the activation function

class Net(nn.Module):
    
    # Constructor
    def __init__(self, D_in, H1, H2, D_out):
        super(Net, self).__init__()
        self.linear1 = nn.Linear(D_in, H1)
        self.linear2 = nn.Linear(H1, H2)
        self.linear3 = nn.Linear(H2, D_out)
    
    # Prediction
    def forward(self,x):
        x = torch.sigmoid(self.linear1(x)) 
        x = torch.sigmoid(self.linear2(x))
        x = self.linear3(x)
        return x
36/3:
# Create the model class using Tanh as a activation function

class NetTanh(nn.Module):
    
    # Constructor
    def __init__(self, D_in, H1, H2, D_out):
        super(NetTanh, self).__init__()
        self.linear1 = nn.Linear(D_in, H1)
        self.linear2 = nn.Linear(H1, H2)
        self.linear3 = nn.Linear(H2, D_out)
    
    # Prediction
    def forward(self, x):
        x = torch.tanh(self.linear1(x))
        x = torch.tanh(self.linear2(x))
        x = self.linear3(x)
        return x
36/4:
# Create the model class using Relu as a activation function

class NetRelu(nn.Module):
    
    # Constructor
    def __init__(self, D_in, H1, H2, D_out):
        super(NetRelu, self).__init__()
        self.linear1 = nn.Linear(D_in, H1)
        self.linear2 = nn.Linear(H1, H2)
        self.linear3 = nn.Linear(H2, D_out)
    
    # Prediction
    def forward(self, x):
        x = torch.relu(self.linear1(x))  
        x = torch.relu(self.linear2(x))
        x = self.linear3(x)
        return x
36/5:
# Train the model

def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):
    i = 0
    useful_stuff = {'training_loss': [], 'validation_accuracy': []}  
    
    for epoch in range(epochs):
        for i, (x, y) in enumerate(train_loader):
            optimizer.zero_grad()
            z = model(x.view(-1, 28 * 28))
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            useful_stuff['training_loss'].append(loss.data.item())
        
        correct = 0
        for x, y in validation_loader:
            z = model(x.view(-1, 28 * 28))
            _, label = torch.max(z, 1)
            correct += (label == y).sum().item()
    
        accuracy = 100 * (correct / len(validation_dataset))
        useful_stuff['validation_accuracy'].append(accuracy)
    
    return useful_stuff
36/6:
# Create the training dataset

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())
36/7:
# Create the validating dataset

validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())
36/8:
# Create the criterion function

criterion = nn.CrossEntropyLoss()
36/9:
# Create the training data loader and validation data loader object

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)
36/10:
# Set the parameters for create the model

input_dim = 28 * 28
hidden_dim1 = 50
hidden_dim2 = 50
output_dim = 10
36/11:
# Set the number of iterations

cust_epochs = 10
36/12:
# Train the model with sigmoid function

learning_rate = 0.01
model = Net(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)
36/13:
# Train the model with tanh function

learning_rate = 0.01
model_Tanh = NetTanh(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate)
training_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)
36/14:
# Train the model with relu function

learning_rate = 0.01
modelRelu = NetRelu(input_dim, hidden_dim1, hidden_dim2, output_dim)
optimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate)
training_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)
36/15:
# Compare the training loss

plt.plot(training_results_tanch['training_loss'], label='tanh')
plt.plot(training_results['training_loss'], label='sigmoid')
plt.plot(training_results_relu['training_loss'], label='relu')
plt.ylabel('loss')
plt.title('training loss iterations')
plt.legend()
36/16:
# Compare the validation loss

plt.plot(training_results_tanch['validation_accuracy'], label = 'tanh')
plt.plot(training_results['validation_accuracy'], label = 'sigmoid')
plt.plot(training_results_relu['validation_accuracy'], label = 'relu') 
plt.ylabel('validation accuracy')
plt.xlabel('Iteration')   
plt.legend()
37/1:
# Import the libraries we need for this lab

import matplotlib.pyplot as plt 
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader

torch.manual_seed(1)
37/2:
# Define the function to plot the diagram

def plot_decision_regions_3class(model, data_set):
    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])
    _, yhat = torch.max(model(XX), 1)
    yhat = yhat.numpy().reshape(xx.shape)
    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
    plt.plot(X[y[:] == 0, 0], X[y[:] == 0, 1], 'ro', label = 'y=0')
    plt.plot(X[y[:] == 1, 0], X[y[:] == 1, 1], 'go', label = 'y=1')
    plt.plot(X[y[:] == 2, 0], X[y[:] == 2, 1], 'o', label = 'y=2')
    plt.title("decision region")
    plt.legend()
37/3:
# Create Data Class

class Data(Dataset):
    
    #  modified from: http://cs231n.github.io/neural-networks-case-study/
    # Constructor
    def __init__(self, K=3, N=500):
        D = 2
        X = np.zeros((N * K, D)) # data matrix (each row = single example)
        y = np.zeros(N * K, dtype='uint8') # class labels
        for j in range(K):
          ix = range(N * j, N * (j + 1))
          r = np.linspace(0.0, 1, N) # radius
          t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta
          X[ix] = np.c_[r * np.sin(t), r*np.cos(t)]
          y[ix] = j
        self.y = torch.from_numpy(y).type(torch.LongTensor)
        self.x = torch.from_numpy(X).type(torch.FloatTensor)
        self.len = y.shape[0]
    
    # Getter
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot_stuff(self):
        plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label="y = 0")
        plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label="y = 1")
        plt.plot(self.x[self.y[:] == 2, 0].numpy(), self.x[self.y[:] == 2, 1].numpy(), 'go', label="y = 2")
        plt.legend()
37/4:
# Create Net model class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, Layers):
        super(Net, self).__init__()
        self.hidden = nn.ModuleList()
        for input_size, output_size in zip(Layers, Layers[1:]):
            self.hidden.append(nn.Linear(input_size, output_size))
    
    # Prediction
    def forward(self, activation):
        L = len(self.hidden)
        for (l, linear_transform) in zip(range(L), self.hidden):
            if l < L - 1:
                activation = F.relu(linear_transform(activation))
            else:
                activation = linear_transform(activation)
        return activation
37/5:
# Define the function for training the model

def train(data_set, model, criterion, train_loader, optimizer, epochs=100):
    LOSS = []
    ACC = []
    for epoch in range(epochs):
        for x, y in train_loader:
            optimizer.zero_grad()
            yhat = model(x)
            loss = criterion(yhat, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            LOSS.append(loss.item())
        ACC.append(accuracy(model, data_set))
    
    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(LOSS, color = color)
    ax1.set_xlabel('Iteration', color = color)
    ax1.set_ylabel('total loss', color = color)
    ax1.tick_params(axis = 'y', color = color)
    
    ax2 = ax1.twinx()  
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color = color)  # we already handled the x-label with ax1
    ax2.plot(ACC, color = color)
    ax2.tick_params(axis = 'y', color = color)
    fig.tight_layout()  # otherwise the right y-label is slightly clipped
    
    plt.show()
    return LOSS
37/6:
# The function to calculate the accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
37/7:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
37/8:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
#data_set.y = data_set.y.view(-1)
37/9:
# Train the model with 1 hidden layer with 50 neurons

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
LOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=100)

plot_decision_regions_3class(model, data_set)
37/10:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
37/11:
# Train the model with 1 hidden layer with 50 neurons

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
LOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=100)

plot_decision_regions_3class(model, data_set)
37/12: Net([3,3,4,3]).parameters
37/13:
# Train the model with 2 hidden layers with 20 neurons

Layers = [2, 10, 10, 3]
model = Net(Layers)
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
LOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=1000)

plot_decision_regions_3class(model, data_set)
37/14:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y
37/15:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
data_set.y
37/16:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
data_set.y
37/17:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y.view(-1)
37/18:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
print(data_set.y)
37/19:
# Create a Dataset object

data_set = Data()
#data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
print(data_set.y)
37/20:
# Create a Dataset object

data_set = Data()
#data_set.plot_stuff()
print(data_set.y)
37/21:
# Create a Dataset object

data_set = Data()
#data_set.plot_stuff()
print(data_set.y)
37/22:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
37/23:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
print(data_set.x)
37/24:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
print(data_set.x)
print(data_set.y)
37/25:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
#data_set.y = data_set.y.view(-1)
print(data_set.x)
print(data_set.y)
37/26:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
print(data_set.x)
print(data_set.y)
37/27:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1,1)
print(data_set.x)
print(data_set.y)
37/28:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
print(data_set.x)
print(data_set.y)
37/29:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1,1)
print(data_set.x)
print(data_set.y)
37/30:
# Train the model with 1 hidden layer with 50 neurons

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
LOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=100)

plot_decision_regions_3class(model, data_set)
37/31:
# Create a Dataset object

data_set = Data()
data_set.plot_stuff()
data_set.y = data_set.y.view(-1)
print(data_set.x)
print(data_set.y)
37/32:
# Train the model with 1 hidden layer with 50 neurons

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
LOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=100)

plot_decision_regions_3class(model, data_set)
39/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
39/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
39/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
39/4:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
39/5:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
39/6:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
39/7:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
39/8:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
39/9:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
39/10:
# Set the model to evaluation model

model_drop.eval()
39/11:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
39/12:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
39/13:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
39/14:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
39/15:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
39/16:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
39/17:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
39/18:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
39/19:
# Set the model to training mode

model_drop.train()
39/20:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
39/21:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
39/22:
# Train the model

epochs = 500

def train_model(epochs):
    
    for epoch in range(epochs):
        #all the samples are used for training 
        yhat = model(data_set.x)
        yhat_drop = model_drop(data_set.x)
        loss = criterion(yhat, data_set.y)
        loss_drop = criterion(yhat_drop, data_set.y)

        #store the loss for both the training and validation data for both models 
        LOSS['training data no dropout'].append(loss.item())
        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
        LOSS['training data dropout'].append(loss_drop.item())
        model_drop.eval()
        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
        model_drop.train()

        optimizer_ofit.zero_grad()
        optimizer_drop.zero_grad()
        loss.backward()
        loss_drop.backward()
        optimizer_ofit.step()
        optimizer_drop.step()
        
train_model(epochs)
41/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
41/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
41/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
41/4:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
41/5:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
41/6:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
41/7:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
41/8:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
41/9:
# Set the model to training mode

model_drop.train()
41/10:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
41/11:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
41/12:
# Train the model

epochs = 500

def train_model(epochs):
    
    for epoch in range(epochs):
        #all the samples are used for training 
        yhat = model(data_set.x)
        yhat_drop = model_drop(data_set.x)
        loss = criterion(yhat, data_set.y)
        loss_drop = criterion(yhat_drop, data_set.y)

        #store the loss for both the training and validation data for both models 
        LOSS['training data no dropout'].append(loss.item())
        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
        LOSS['training data dropout'].append(loss_drop.item())
        model_drop.eval()
        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
        model_drop.train()

        optimizer_ofit.zero_grad()
        optimizer_drop.zero_grad()
        loss.backward()
        loss_drop.backward()
        optimizer_ofit.step()
        optimizer_drop.step()
        
train_model(epochs)
42/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
42/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
42/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
42/4:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
42/5:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
42/6:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
42/7:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
42/8:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
42/9:
# Set the model to training mode

model_drop.train()
42/10:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
42/11:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
42/12:
# Train the model

epochs = 500

def train_model(epochs):
    
    for epoch in range(epochs):
        #all the samples are used for training 
        yhat = model(data_set.x)
        yhat_drop = model_drop(data_set.x)
        loss = criterion(yhat, data_set.y)
        loss_drop = criterion(yhat_drop, data_set.y)

        #store the loss for both the training and validation data for both models 
        LOSS['training data no dropout'].append(loss.item())
        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
        LOSS['training data dropout'].append(loss_drop.item())
        model_drop.eval()
        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
        model_drop.train()

        optimizer_ofit.zero_grad()
        optimizer_drop.zero_grad()
        loss.backward()
        loss_drop.backward()
        optimizer_ofit.step()
        optimizer_drop.step()
        
train_model(epochs)
43/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
43/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
43/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
43/4:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
43/5:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
43/6:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
43/7:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
43/8:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
43/9:
# Set the model to training mode

model_drop.train()
43/10:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
43/11:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
46/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
46/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
46/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
46/4:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
46/5:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
46/6:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
46/7:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
46/8:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
46/9:
# Set the model to training mode

model_drop.train()
46/10:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
46/11:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
47/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
47/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
47/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
47/4:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
47/5:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
47/6:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
47/7:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
47/8:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
47/9:
# Set the model to training mode

model_drop.train()
47/10:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
47/11:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
47/12:
# Train the model

epochs = 500

def train_model(epochs):
    
    for epoch in range(epochs):
        #all the samples are used for training 
        yhat = model(data_set.x)
        yhat_drop = model_drop(data_set.x)
        loss = criterion(yhat, data_set.y)
        loss_drop = criterion(yhat_drop, data_set.y)

        #store the loss for both the training and validation data for both models 
        LOSS['training data no dropout'].append(loss.item())
        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
        LOSS['training data dropout'].append(loss_drop.item())
        model_drop.eval()
        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
        model_drop.train()

        optimizer_ofit.zero_grad()
        optimizer_drop.zero_grad()
        loss.backward()
        loss_drop.backward()
        optimizer_ofit.step()
        optimizer_drop.step()
        
train_model(epochs)
49/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
49/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
49/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
49/4:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
49/5:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
49/6:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
49/7:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
49/8:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
49/9:
# Set the model to training mode

model_drop.train()
49/10:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
49/11:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
50/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
50/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
50/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
50/4:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
50/5:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
50/6:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
50/7:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
50/8:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
50/9:
# Set the model to training mode

model_drop.train()
50/10:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
50/11:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
50/12:
# Train the model

epochs = 500

def train_model(epochs):
    
    for epoch in range(epochs):
        #all the samples are used for training 
        yhat = model(data_set.x)
        yhat_drop = model_drop(data_set.x)
        loss = criterion(yhat, data_set.y)
        loss_drop = criterion(yhat_drop, data_set.y)

        #store the loss for both the training and validation data for both models 
        LOSS['training data no dropout'].append(loss.item())
        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
        LOSS['training data dropout'].append(loss_drop.item())
        model_drop.eval()
        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
        model_drop.train()

        optimizer_ofit.zero_grad()
        optimizer_drop.zero_grad()
        loss.backward()
        loss_drop.backward()
        optimizer_ofit.step()
        optimizer_drop.step()
        
train_model(epochs)
51/1:
# Train the model

epochs = 500

def train_model(epochs):
    
    for epoch in range(epochs):
        #all the samples are used for training 
        yhat = model(data_set.x)
        yhat_drop = model_drop(data_set.x)
        loss = criterion(yhat, data_set.y)
        loss_drop = criterion(yhat_drop, data_set.y)

        #store the loss for both the training and validation data for both models 
        LOSS['training data no dropout'].append(loss.item())
        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
        LOSS['training data dropout'].append(loss_drop.item())
        model_drop.eval()
        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
        model_drop.train()

        optimizer_ofit.zero_grad()
        optimizer_drop.zero_grad()
        loss.backward()
        loss_drop.backward()
        optimizer_ofit.step()
        optimizer_drop.step()
        
train_model(epochs)
52/1:
# Import the libraries we need for this lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
52/2:
# The function for plotting the diagram

def plot_decision_regions_3class(data_set, model=None):
    cmap_light = ListedColormap([ '#0000FF','#FF0000'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])
    X = data_set.x.numpy()
    y = data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    newdata = np.c_[xx.ravel(), yy.ravel()]
    
    Z = data_set.multi_dim_poly(newdata).flatten()
    f = np.zeros(Z.shape)
    f[Z > 0] = 1
    f = f.reshape(xx.shape)
    if model != None:
        model.eval()
        XX = torch.Tensor(newdata)
        _, yhat = torch.max(model(XX), 1)
        yhat = yhat.numpy().reshape(xx.shape)
        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
    else:
        plt.contour(xx, yy, f, cmap=plt.cm.Paired)
        plt.pcolormesh(xx, yy, f, cmap=cmap_light) 

    plt.title("decision region vs True decision boundary")
52/3:
# The function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
52/4:
# Create data class for creating dataset object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):
        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T
        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))
        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()
        self.a = a
       
        self.y = np.zeros(N_SAMPLES)
        self.y[self.f > 0] = 1
        self.y = torch.from_numpy(self.y).type(torch.LongTensor)
        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)
        self.x = self.x + noise_std * torch.randn(self.x.size())
        self.f = torch.from_numpy(self.f)
        self.a = a
        if train == True:
            torch.manual_seed(1)
            self.x = self.x + noise_std * torch.randn(self.x.size())
            torch.manual_seed(0)
        
    # Getter        
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot(self):
        X = data_set.x.numpy()
        y = data_set.y.numpy()
        h = .02
        x_min, x_max = X[:, 0].min(), X[:, 0].max()
        y_min, y_max = X[:, 1].min(), X[:, 1].max() 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()
        f = np.zeros(Z.shape)
        f[Z > 0] = 1
        f = f.reshape(xx.shape)
        
        plt.title('True decision boundary  and sample points with noise ')
        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') 
        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')
        plt.contour(xx, yy, f,cmap=plt.cm.Paired)
        plt.xlim(0,1)
        plt.ylim(0,1)
        plt.legend()
    
    # Make a multidimension ploynomial function
    def multi_dim_poly(self, x):
        x = np.matrix(x)
        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])
        out = np.array(out)
        return out
52/5:
# Create a dataset object

data_set = Data(noise_std=0.2)
data_set.plot()
52/6:
# Get some validation data

torch.manual_seed(0) 
validation_set = Data(train=False)
validation_set.plot()
52/7:
# Create Net Class

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
    
    # Prediction function
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
52/8:
# Create two model objects: model without dropout and model with dropout

model = Net(2, 300, 2)
model_drop = Net(2, 300, 2, p=0.01)
52/9:
# Set the model to training mode

model_drop.train()
52/10:
# Set optimizer functions and criterion functions

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.CrossEntropyLoss()
52/11:
# Initialize the LOSS dictionary to store the loss

LOSS = {}
LOSS['training data no dropout'] = []
LOSS['validation data no dropout'] = []
LOSS['training data dropout'] = []
LOSS['validation data dropout'] = []
52/12:
# Train the model

epochs = 500

def train_model(epochs):
    
    for epoch in range(epochs):
        #all the samples are used for training 
        yhat = model(data_set.x)
        yhat_drop = model_drop(data_set.x)
        loss = criterion(yhat, data_set.y)
        loss_drop = criterion(yhat_drop, data_set.y)

        #store the loss for both the training and validation data for both models 
        LOSS['training data no dropout'].append(loss.item())
        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
        LOSS['training data dropout'].append(loss_drop.item())
        model_drop.eval()
        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
        model_drop.train()

        optimizer_ofit.zero_grad()
        optimizer_drop.zero_grad()
        loss.backward()
        loss_drop.backward()
        optimizer_ofit.step()
        optimizer_drop.step()
        
train_model(epochs)
54/1:
# Import the libraries we need for the lab

import torch
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader

torch.manual_seed(0)
54/2:
# Create Data object

class Data(Dataset):
    
    # Constructor
    def __init__(self, N_SAMPLES=40, noise_std=1, train=True):
        self.x = torch.linspace(-1, 1, N_SAMPLES).view(-1, 1)
        self.f = self.x ** 2
        if train != True:
            torch.manual_seed(1)
            self.y = self.f + noise_std * torch.randn(self.f.size())
            self.y = self.y.view(-1, 1)
            torch.manual_seed(0)
        else:
            self.y = self.f + noise_std * torch.randn(self.f.size())
            self.y = self.y.view(-1, 1)
            
    # Getter
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the data
    def plot(self):
        plt.figure(figsize = (6.1, 10))
        plt.scatter(self.x.numpy(), self.y.numpy(), label="Samples")
        plt.plot(self.x.numpy(), self.f.numpy() ,label="True Function", color='orange')
        plt.xlabel("x")
        plt.ylabel("y")
        plt.xlim((-1, 1))
        plt.ylim((-2, 2.5))
        plt.legend(loc="best")
        plt.show()
54/3:
# Create the dataset object and plot the dataset

data_set = Data()
data_set.plot()
54/4:
# Create validation dataset object

validation_set = Data(train=False)
54/5:
# Create validation dataset object

validation_set = Data(train=False)
validation_set.plot()
54/6:
# Create the class for model

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden, out_size, p=0):
        super(Net, self).__init__()
        self.drop = nn.Dropout(p=p)
        self.linear1 = nn.Linear(in_size, n_hidden)
        self.linear2 = nn.Linear(n_hidden, n_hidden)
        self.linear3 = nn.Linear(n_hidden, out_size)
        
    def forward(self, x):
        x = F.relu(self.drop(self.linear1(x)))
        x = F.relu(self.drop(self.linear2(x)))
        x = self.linear3(x)
        return x
54/7:
# Create the model objects

model = Net(1, 300, 1)
model_drop = Net(1, 300, 1, p=0.5)
54/8:
# Set the model to train mode

model_drop.train()
54/9:
# Set the optimizer and criterion function

optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)
optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)
criterion = torch.nn.MSELoss()
54/10:
# Initialize the dict to contain the loss results

LOSS={}
LOSS['training data no dropout']=[]
LOSS['validation data no dropout']=[]
LOSS['training data dropout']=[]
LOSS['validation data dropout']=[]
54/11:
# Train the model

epochs = 500

def train_model(epochs):
    for epoch in range(epochs):
        yhat = model(data_set.x)
        yhat_drop = model_drop(data_set.x)
        loss = criterion(yhat, data_set.y)
        loss_drop = criterion(yhat_drop, data_set.y)

        #store the loss for  both the training and validation  data for both models 
        LOSS['training data no dropout'].append(loss.item())
        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())
        LOSS['training data dropout'].append(loss_drop.item())
        model_drop.eval()
        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())
        model_drop.train()

        optimizer_ofit.zero_grad()
        optimizer_drop.zero_grad()
        loss.backward()
        loss_drop.backward()
        optimizer_ofit.step()
        optimizer_drop.step()
        
train_model(epochs)
54/12:
# Set the model with dropout to evaluation mode

model_drop.eval()
54/13:
# Make the prediction

yhat = model(data_set.x)
yhat_drop = model_drop(data_set.x)
54/14:
# Plot the predictions for both models

plt.figure(figsize=(6.1, 10))

plt.scatter(data_set.x.numpy(), data_set.y.numpy(), label="Samples")
plt.plot(data_set.x.numpy(), data_set.f.numpy(), label="True function", color='orange')
plt.plot(data_set.x.numpy(), yhat.detach().numpy(), label='no dropout', c='r')
plt.plot(data_set.x.numpy(), yhat_drop.detach().numpy(), label="dropout", c ='g')

plt.xlabel("x")
plt.ylabel("y")
plt.xlim((-1, 1))
plt.ylim((-2, 2.5))
plt.legend(loc = "best")
plt.show()
54/15:
# Plot the loss

plt.figure(figsize=(6.1, 10))
for key, value in LOSS.items():
    plt.plot(np.log(np.array(value)), label=key)
    plt.legend()
    plt.xlabel("iterations")
    plt.ylabel("Log of cost or total loss")
55/1:
# Import the libraries we need for this lab

import torch 
import torch.nn as nn
from torch import sigmoid
import matplotlib.pylab as plt
import numpy as np
torch.manual_seed(0)
55/2:
# The function for plotting the model

def PlotStuff(X, Y, model, epoch, leg=True):
    
    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))
    plt.plot(X.numpy(), Y.numpy(), 'r')
    plt.xlabel('x')
    if leg == True:
        plt.legend()
    else:
        pass
55/3:
# Define the class Net

class Net(nn.Module):
    
    # Constructor
    def __init__(self, D_in, H, D_out):
        super(Net, self).__init__()
        # hidden layer 
        self.linear1 = nn.Linear(D_in, H)
        self.linear2 = nn.Linear(H, D_out)
        # Define the first linear layer as an attribute, this is not good practice
        self.a1 = None
        self.l1 = None
        self.l2=None
    
    # Prediction
    def forward(self, x):
        self.l1 = self.linear1(x)
        self.a1 = sigmoid(self.l1)
        self.l2=self.linear2(self.a1)
        yhat = sigmoid(self.linear2(self.a1))
        return yhat
55/4:
# Make some data

X = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)
Y = torch.zeros(X.shape[0])
Y[(X[:, 0] > -4) & (X[:, 0] < 4)] = 1.0
55/5:
# The loss function

def criterion_cross(outputs, labels):
    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))
    return out
55/6:
# Train the model
# size of input 
D_in = 1
# size of hidden layer 
H = 2
# number of outputs 
D_out = 1
# learning rate 
learning_rate = 0.1
# create the model 
model = Net(D_in, H, D_out)
55/7: model.state_dict()
55/8:
model.state_dict()['linear1.weight'][0]=1.0
model.state_dict()['linear1.weight'][1]=1.0
model.state_dict()['linear1.bias'][0]=0.0
model.state_dict()['linear1.bias'][1]=0.0
model.state_dict()['linear2.weight'][0]=1.0
model.state_dict()['linear2.bias'][0]=0.0
model.state_dict()
55/9:
#optimizer 
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
#train the model usein
cost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)
#plot the loss
plt.plot(cost_cross)
plt.xlabel('epoch')
plt.title('cross entropy loss')
55/10:
# Define the training function

def train(Y, X, model, optimizer, criterion, epochs=1000):
    cost = []
    total=0
    for epoch in range(epochs):
        total=0
        for y, x in zip(Y, X):
            yhat = model(x)
            loss = criterion(yhat, y)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            #cumulative loss 
            total+=loss.item() 
        cost.append(total)
        if epoch % 300 == 0:    
            PlotStuff(X, Y, model, epoch, leg=True)
            plt.show()
            model(X)
            plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))
            plt.title('activations')
            plt.show()
    return cost
55/11:
#optimizer 
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
#train the model usein
cost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)
#plot the loss
plt.plot(cost_cross)
plt.xlabel('epoch')
plt.title('cross entropy loss')
55/12: model.state_dict()
55/13:
# Train the model
# size of input 
D_in = 1
# size of hidden layer 
H = 2
# number of outputs 
D_out = 1
# learning rate 
learning_rate = 0.1
# create the model 
model = Net(D_in, H, D_out)
55/14:
#optimizer 
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
#train the model usein
cost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)
#plot the loss
plt.plot(cost_cross)
plt.xlabel('epoch')
plt.title('cross entropy loss')
56/1:
# Import the libraries we need to use in this lab

# Using the following line code to install the torchvision library
# !conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np

torch.manual_seed(0)
56/2:
# Define the neural network with Xavier initialization

class Net_Xavier(nn.Module):
    
    # Constructor
    def __init__(self, Layers):
        super(Net_Xavier, self).__init__()
        self.hidden = nn.ModuleList()

        for input_size, output_size in zip(Layers, Layers[1:]):
            linear = nn.Linear(input_size, output_size)
            torch.nn.init.xavier_uniform_(linear.weight)
            self.hidden.append(linear)
    
    # Prediction
    def forward(self, x):
        L = len(self.hidden)
        for (l, linear_transform) in zip(range(L), self.hidden):
            if l < L - 1:
                x = torch.tanh(linear_transform(x))
            else:
                x = linear_transform(x)
        return x
56/3:
# Define the neural network with Uniform initialization

class Net_Uniform(nn.Module):
    
    # Constructor
    def __init__(self, Layers):
        super(Net_Uniform, self).__init__()
        self.hidden = nn.ModuleList()

        for input_size, output_size in zip(Layers, Layers[1:]):
            linear = nn.Linear(input_size, output_size)
            linear.weight.data.uniform_(0, 1)
            self.hidden.append(linear)
    
    # Prediction
    def forward(self, x):
        L = len(self.hidden)
        for (l, linear_transform) in zip(range(L), self.hidden):
            if l < L - 1:
                x = torch.tanh(linear_transform(x))
            else:
                x = linear_transform(x)
        return x
56/4:
# Define the neural network with Default initialization

class Net(nn.Module):
    
    # Constructor
    def __init__(self, Layers):
        super(Net, self).__init__()
        self.hidden = nn.ModuleList()

        for input_size, output_size in zip(Layers, Layers[1:]):
            linear = nn.Linear(input_size, output_size)
            self.hidden.append(linear)
    
    # Prediction
    def forward(self, x):
        L = len(self.hidden)
        for (l, linear_transform) in zip(range(L), self.hidden):
            if l < L - 1:
                x = torch.tanh(linear_transform(x))
            else:
                x = linear_transform(x)
        return x
56/5:
# function to Train the model

def train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):
    i = 0
    loss_accuracy = {'training_loss':[], 'validation_accuracy':[]}  
    
    for epoch in range(epochs):
        for i,(x, y) in enumerate(train_loader):
            optimizer.zero_grad()
            z = model(x.view(-1, 28 * 28))
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            loss_accuracy['training_loss'].append(loss.data.item())
            
        correct = 0
        for x, y in validation_loader:
            yhat = model(x.view(-1, 28 * 28))
            _, label = torch.max(yhat, 1)
            correct += (label==y).sum().item()
        accuracy = 100 * (correct / len(validation_dataset))
        loss_accuracy['validation_accuracy'].append(accuracy)
        
    return loss_accuracy
56/6:
# Create the train dataset

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())
56/7:
# Create the validation dataset

validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())
56/8:
# Create Dataloader for both train dataset and validation dataset

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)
56/9:
# Define criterion function

criterion = nn.CrossEntropyLoss()
56/10:
# Set the parameters

input_dim = 28 * 28
output_dim = 10
layers = [input_dim, 100, 10, 100, 10, 100, output_dim]
epochs = 15
56/11:
# Train the model with default initialization

model = Net(layers)
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=epochs)
56/12:
# Train the model with Xavier initialization

model_Xavier = Net_Xavier(layers)
optimizer = torch.optim.SGD(model_Xavier.parameters(), lr=learning_rate)
training_results_Xavier = train(model_Xavier, criterion, train_loader, validation_loader, optimizer, epochs=epochs)
56/13:
# Train the model with Uniform initialization

model_Uniform = Net_Uniform(layers)
optimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate)
training_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=epochs)
56/14:
# Plot the loss

plt.plot(training_results_Xavier['training_loss'], label='Xavier')
plt.plot(training_results['training_loss'], label='Default')
plt.plot(training_results_Uniform['training_loss'], label='Uniform')
plt.ylabel('loss')
plt.xlabel('iteration ')  
plt.title('training loss iterations')
plt.legend()
56/15:
# Plot the accuracy

plt.plot(training_results_Xavier['validation_accuracy'], label='Xavier')
plt.plot(training_results['validation_accuracy'], label='Default')
plt.plot(training_results_Uniform['validation_accuracy'], label='Uniform') 
plt.ylabel('validation accuracy')
plt.xlabel('epochs')   
plt.legend()
58/1:
# These are the libraries that will be used for this lab.

import torch 
import torch.nn as nn
import matplotlib.pylab as plt
import numpy as np

torch.manual_seed(0)
58/2:
# Plot the cubic

def plot_cubic(w, optimizer):
    LOSS = []
    # parameter values 
    W = torch.arange(-4, 4, 0.1)
    # plot the loss fuction 
    for w.state_dict()['linear.weight'][0] in W:
        LOSS.append(cubic(w(torch.tensor([[1.0]]))).item())
    w.state_dict()['linear.weight'][0] = 4.0
    n_epochs = 10
    parameter = []
    loss_list = []

    # n_epochs
    # Use PyTorch custom module to implement a ploynomial function
    for n in range(n_epochs):
        optimizer.zero_grad() 
        loss = cubic(w(torch.tensor([[1.0]])))
        loss_list.append(loss)
        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())
        loss.backward()
        optimizer.step()
    plt.plot(parameter, loss_list, 'ro', label='parameter values')
    plt.plot(W.numpy(), LOSS, label='objective function')
    plt.xlabel('w')
    plt.ylabel('l(w)')
    plt.legend()
58/3:
# Plot the cubic

def plot_cubic(w, optimizer):
    LOSS = []
    # parameter values 
    W = torch.arange(-4, 4, 0.1)
    # plot the loss fuction 
    for w.state_dict()['linear.weight'][0] in W:
        LOSS.append(cubic(w(torch.tensor([[1.0]]))).item())
    w.state_dict()['linear.weight'][0] = 4.0
    n_epochs = 10
    parameter = []
    loss_list = []

    # n_epochs
    # Use PyTorch custom module to implement a ploynomial function
    for n in range(n_epochs):
        optimizer.zero_grad() 
        loss = cubic(w(torch.tensor([[1.0]])))
        loss_list.append(loss)
        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())
        loss.backward()
        optimizer.step()
    plt.plot(parameter, loss_list, 'ro', label='parameter values')
    plt.plot(W.numpy(), LOSS, label='objective function')
    plt.xlabel('w')
    plt.ylabel('l(w)')
    plt.legend()
58/4:
# Plot the fourth order function and the parameter values

def plot_fourth_order(w, optimizer, std=0, color='r', paramlabel='parameter values', objfun=True):
    W = torch.arange(-4, 6, 0.1)
    LOSS = []
    for w.state_dict()['linear.weight'][0] in W:
        LOSS.append(fourth_order(w(torch.tensor([[1.0]]))).item())
    w.state_dict()['linear.weight'][0] = 6
    n_epochs = 100
    parameter = []
    loss_list = []

    #n_epochs
    for n in range(n_epochs):
        optimizer.zero_grad()
        loss = fourth_order(w(torch.tensor([[1.0]]))) + std * torch.randn(1, 1)
        loss_list.append(loss)
        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())
        loss.backward()
        optimizer.step()
    
    # Plotting
    if objfun:
        plt.plot(W.numpy(), LOSS, label='objective function')
    plt.plot(parameter, loss_list, 'ro',label=paramlabel, color=color)
    plt.xlabel('w')
    plt.ylabel('l(w)')
    plt.legend()
58/5:
# Create a linear model

class one_param(nn.Module):
    
    # Constructor
    def __init__(self, input_size, output_size):
        super(one_param, self).__init__()
        self.linear = nn.Linear(input_size, output_size, bias=False)
        
    # Prediction
    def forward(self, x):
        yhat = self.linear(x)
        return yhat
58/6:
# Create a one_param object

w = one_param(1, 1)
58/7:
# Define a function to output a cubic 

def cubic(yhat):
    out = yhat ** 3
    return out
58/8:
# Create a optimizer without momentum

optimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0)
58/9:
# Plot the model

plot_cubic(w, optimizer)
58/10:
# Create a optimizer with momentum

optimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0.9)
58/11:
# Plot the model

plot_cubic(w, optimizer)
58/12:
# Create a function to calculate the fourth order polynomial 

def fourth_order(yhat): 
    out = torch.mean(2 * (yhat ** 4) - 9 * (yhat ** 3) - 21 * (yhat ** 2) + 88 * yhat + 48)
    return out
58/13:
# Make the prediction without momentum

optimizer = torch.optim.SGD(w.parameters(), lr=0.001)
plot_fourth_order(w, optimizer)
58/14:
# Make the prediction with momentum

optimizer = torch.optim.SGD(w.parameters(), lr=0.001, momentum=0.9)
plot_fourth_order(w, optimizer)
58/15:
# Make the prediction without momentum when there is noise

optimizer = torch.optim.SGD(w.parameters(), lr=0.001)
plot_fourth_order(w, optimizer, std=10)
58/16:
# Make the prediction with momentum when there is noise

optimizer = torch.optim.SGD(w.parameters(), lr=0.001,momentum=0.9)
plot_fourth_order(w, optimizer, std=10)
58/17:
# Practice: Create two SGD optimizer with lr = 0.001, and one without momentum and the other with momentum = 0.9. Plot the result out.

optimizer1 = torch.optim.SGD(w.parameters(), lr = 0.001)
plot_fourth_order(w, optimizer1, std = 100, color = 'black', paramlabel = 'parameter values with optimizer 1')

optimizer2 = torch.optim.SGD(w.parameters(), lr = 0.001, momentum = 0.9)
plot_fourth_order(w, optimizer2, std = 100, color = 'red', paramlabel = 'parameter values with optimizer 2', objfun = False)
59/1:
# Import the libraries for this lab

import matplotlib.pyplot as plt 
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader

torch.manual_seed(1)
np.random.seed(1)
59/2:
# Define a function for plot the decision region

def plot_decision_regions_3class(model, data_set):
    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00','#00AAFF'])
    X=data_set.x.numpy()
    y=data_set.y.numpy()
    h = .02
    x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1 
    y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1 
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))
    XX=torch.torch.Tensor(np.c_[xx.ravel(), yy.ravel()])
    _,yhat=torch.max(model(XX),1)
    yhat=yhat.numpy().reshape(xx.shape)
    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)
    plt.plot(X[y[:]==0,0], X[y[:]==0,1], 'ro', label='y=0')
    plt.plot(X[y[:]==1,0], X[y[:]==1,1], 'go', label='y=1')
    plt.plot(X[y[:]==2,0], X[y[:]==2,1], 'o', label='y=2')
    plt.title("decision region")
    plt.legend()
59/3:
# Create the dataset class

class Data(Dataset):
    
    #  modified from: http://cs231n.github.io/neural-networks-case-study/
    # Constructor
    def __init__(self, K=3, N=500):
        D = 2
        X = np.zeros((N * K, D)) # data matrix (each row = single example)
        y = np.zeros(N * K, dtype='uint8') # class labels
        for j in range(K):
          ix = range(N * j, N * (j + 1))
          r = np.linspace(0.0, 1, N) # radius
          t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta
          X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]
          y[ix] = j
    
        self.y = torch.from_numpy(y).type(torch.LongTensor)
        self.x = torch.from_numpy(X).type(torch.FloatTensor)
        self.len = y.shape[0]
            
    # Getter
    def __getitem__(self, index):    
        return self.x[index], self.y[index]
    
    # Get Length
    def __len__(self):
        return self.len
    
    # Plot the diagram
    def plot_data(self):
        plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label="y=0")
        plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label="y=1")
        plt.plot(self.x[self.y[:] == 2, 0].numpy(),self.x[self.y[:] == 2, 1].numpy(), 'go',label="y=2")
        plt.legend()
59/4:
# Create dataset object

class Net(nn.Module):
    
    # Constructor
    def __init__(self, Layers):
        super(Net, self).__init__()
        self.hidden = nn.ModuleList()
        for input_size, output_size in zip(Layers, Layers[1:]):
            self.hidden.append(nn.Linear(input_size, output_size))
    
    # Prediction
    def forward(self, activation):
        L = len(self.hidden)
        for (l, linear_transform) in zip(range(L), self.hidden):
            if l < L - 1:
                activation = F.relu(linear_transform(activation))    
            else:
                activation = linear_transform(activation)
        return activation
59/5:
# Define the function for training the model

def train(data_set, model, criterion, train_loader, optimizer, epochs=100):
    LOSS = []
    ACC = []
    for epoch in range(epochs):
        for x, y in train_loader:
            optimizer.zero_grad()
            yhat = model(x)
            loss = criterion(yhat, y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        LOSS.append(loss.item())
        ACC.append(accuracy(model,data_set))
        
    results ={"Loss":LOSS, "Accuracy":ACC}
    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(LOSS,color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis = 'y', color=color)
    
    ax2 = ax1.twinx()  
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout()  # otherwise the right y-label is slightly clipped
    
    plt.show()
    return results
59/6:
# Define a function for calculating accuracy

def accuracy(model, data_set):
    _, yhat = torch.max(model(data_set.x), 1)
    return (yhat == data_set.y).numpy().mean()
59/7:
# Create the dataset and plot it

data_set = Data()
data_set.plot_data()
data_set.y = data_set.y.view(-1)
59/8:
# Initialize a dictionary to contain the cost and accuracy

Results = {"momentum 0": {"Loss": 0, "Accuracy:": 0}, "momentum 0.1": {"Loss": 0, "Accuracy:": 0}}
59/9:
# Train a model with 1 hidden layer and 50 neurons

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model, data_set)
59/10:
# Train a model with 1 hidden layer and 50 neurons with 0.1 momentum

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.1)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0.1"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model, data_set)
59/11:
# Train a model with 1 hidden layer and 50 neurons with 0.2 momentum

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.2)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0.2"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model, data_set)
59/12:
# Train a model with 1 hidden layer and 50 neurons with 0.4 momentum

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.4)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0.4"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model, data_set)
59/13:
# Train a model with 1 hidden layer and 50 neurons with 0.5 momentum

Layers = [2, 50, 3]
model = Net(Layers)
learning_rate = 0.10
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)
train_loader = DataLoader(dataset=data_set, batch_size=20)
criterion = nn.CrossEntropyLoss()
Results["momentum 0.5"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)
plot_decision_regions_3class(model,data_set)
59/14:
# Plot the Loss result for each term

for key, value in Results.items():
    plt.plot(value['Loss'],label=key)
    plt.legend()
    plt.xlabel('epoch')
    plt.ylabel('Total Loss or Cost')
59/15:
# Plot the Accuracy result for each term

for key, value in Results.items():
    plt.plot(value['Accuracy'],label=key)
    plt.legend()
    plt.xlabel('epoch')
    plt.ylabel('Accuracy')
60/1:
# These are the libraries will be used for this lab.

# Using the following line code to install the torchvision library
# !conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np
torch.manual_seed(0)
60/2:
# Define the Neural Network Model using Batch Normalization

class NetBatchNorm(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):
        super(NetBatchNorm, self).__init__()
        self.linear1 = nn.Linear(in_size, n_hidden1)
        self.linear2 = nn.Linear(n_hidden1, n_hidden2)
        self.linear3 = nn.Linear(n_hidden2, out_size)
        self.bn1 = nn.BatchNorm1d(n_hidden1)
        self.bn2 = nn.BatchNorm1d(n_hidden2)
        
    # Prediction
    def forward(self, x):
        x = self.bn1(torch.sigmoid(self.linear1(x)))
        x = self.bn2(torch.sigmoid(self.linear2(x)))
        x = self.linear3(x)
        return x
    
    # Activations, to analyze results 
    def activation(self, x):
        out = []
        z1 = self.bn1(self.linear1(x))
        out.append(z1.detach().numpy().reshape(-1))
        a1 = torch.sigmoid(z1)
        out.append(a1.detach().numpy().reshape(-1).reshape(-1))
        z2 = self.bn2(self.linear2(a1))
        out.append(z2.detach().numpy().reshape(-1))
        a2 = torch.sigmoid(z2)
        out.append(a2.detach().numpy().reshape(-1))
        return out
60/3:
# Class Net for Neural Network Model

class Net(nn.Module):
    
    # Constructor
    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):

        super(Net, self).__init__()
        self.linear1 = nn.Linear(in_size, n_hidden1)
        self.linear2 = nn.Linear(n_hidden1, n_hidden2)
        self.linear3 = nn.Linear(n_hidden2, out_size)
    
    # Prediction
    def forward(self, x):
        x = torch.sigmoid(self.linear1(x))
        x = torch.sigmoid(self.linear2(x))
        x = self.linear3(x)
        return x
    
    # Activations, to analyze results 
    def activation(self, x):
        out = []
        z1 = self.linear1(x)
        out.append(z1.detach().numpy().reshape(-1))
        a1 = torch.sigmoid(z1)
        out.append(a1.detach().numpy().reshape(-1).reshape(-1))
        z2 = self.linear2(a1)
        out.append(z2.detach().numpy().reshape(-1))
        a2 = torch.sigmoid(z2)
        out.append(a2.detach().numpy().reshape(-1))
        return out
60/4:
# Define the function to train model

def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):
    i = 0
    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  

    for epoch in range(epochs):
        for i, (x, y) in enumerate(train_loader):
            model.train()
            optimizer.zero_grad()
            z = model(x.view(-1, 28 * 28))
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            useful_stuff['training_loss'].append(loss.data.item())
            
        correct = 0
        for x, y in validation_loader:
            model.eval()
            yhat = model(x.view(-1, 28 * 28))
            _, label = torch.max(yhat, 1)
            correct += (label == y).sum().item()
            
        accuracy = 100 * (correct / len(validation_dataset))
        useful_stuff['validation_accuracy'].append(accuracy)
    
    return useful_stuff
60/5:
# load the train dataset

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())
60/6:
# load the train dataset

validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())
60/7:
# Create Data Loader for both train and validating

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)
60/8:
# Create the criterion function

criterion = nn.CrossEntropyLoss()
60/9:
# Set the parameters

input_dim = 28 * 28
hidden_dim = 100
output_dim = 10
60/10:
# Create model, optimizer and train the model

model_norm  = NetBatchNorm(input_dim, hidden_dim, hidden_dim, output_dim)
optimizer = torch.optim.Adam(model_norm.parameters(), lr = 0.1)
training_results_Norm=train(model_norm , criterion, train_loader, validation_loader, optimizer, epochs=5)
60/11:
# Create model without Batch Normalization, optimizer and train the model

model = Net(input_dim, hidden_dim, hidden_dim, output_dim)
optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)
training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=5)
60/12:
model.eval()
model_norm.eval()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[2],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[2],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()
60/13:
model.eval()
model_norm.eval()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[2],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[2],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()
60/14:
model.train()
model_norm.train()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[2],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[2],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()
60/15:
# Plot the diagram to show the loss

plt.plot(training_results['training_loss'], label='No Batch Normalization')
plt.plot(training_results_Norm['training_loss'], label='Batch Normalization')
plt.ylabel('Cost')
plt.xlabel('iterations ')   
plt.legend()
plt.show()
60/16:
model.eval()
model_norm.eval()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[2],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[2],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()
60/17:
# Plot the diagram to show the loss

plt.plot(training_results['training_loss'], label='No Batch Normalization')
plt.plot(training_results_Norm['training_loss'], label='Batch Normalization')
plt.ylabel('Cost')
plt.xlabel('iterations ')   
plt.legend()
plt.show()
60/18:
# Plot the diagram to show the accuracy

plt.plot(training_results['validation_accuracy'],label='No Batch Normalization')
plt.plot(training_results_Norm['validation_accuracy'],label='Batch Normalization')
plt.ylabel('validation accuracy')
plt.xlabel('epochs ')   
plt.legend()
plt.show()
60/19:
model.eval()
model_norm.eval()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[2],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[2],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()
out
60/20:
model.eval()
model_norm.eval()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[2],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[2],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()
60/21:
model.eval()
model_norm.eval()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[3],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[3],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()
60/22:
model.eval()
model_norm.eval()
out=model.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out[2],label='model with no batch normalization' )
out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))
plt.hist(out_norm[2],label='model with normalization')
plt.xlabel("activation ")
plt.legend()
plt.show()
61/1:
import torch 
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage, misc
61/2:
conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)
conv
61/3:
conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])
conv.state_dict()['bias'][0]=0.0
conv.state_dict()
61/4:
image=torch.zeros(1,1,5,5)
image[0,0,:,2]=1
image
61/5:
z=conv(image)
z
61/6:
K=2
conv1 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=K)
conv1.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])
conv1.state_dict()['bias'][0]=0.0
conv1.state_dict()
conv1
61/7:
M=4
image1=torch.ones(1,1,M,M)
61/8:
z1=conv1(image1)
print("z1:",z1)
print("shape:",z1.shape[2:4])
61/9:
z1=conv1(image1)
print("z1:",z1)
print("shape:",z1.shape[2:4])
61/10:
conv3 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=2)

conv3.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])
conv3.state_dict()['bias'][0]=0.0
conv3.state_dict()
61/11:
z3=conv3(image1)

print("z3:",z3)
print("shape:",z3.shape[2:4])
61/12: image1
61/13:
conv4 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)
conv4.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])
conv4.state_dict()['bias'][0]=0.0
conv4.state_dict()
z4=conv4(image1)
print("z4:",z4)
print("z4:",z4.shape[2:4])
61/14:
conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=1)

conv5.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])
conv5.state_dict()['bias'][0]=0.0
conv5.state_dict()
z5=conv5(image1)
print("z5:",z5)
print("z5:",z4.shape[2:4])
61/15:
Image=torch.randn((1,1,4,4))
Image
61/16:
conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)
conv.state_dict()['weight'][0][0]=torch.tensor([[0,0,0],[0,0,0],[0,0.0,0]])
conv.state_dict()['bias'][0]=0.0
61/17: conv(Image)
62/1:
import torch 
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage, misc
62/2:
conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)
Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0,-1.0]])
conv.state_dict()['weight'][0][0]=Gx
conv.state_dict()['bias'][0]=0.0
conv.state_dict()
62/3:
image=torch.zeros(1,1,5,5)
image[0,0,:,2]=1
image
62/4:
Z=conv(image)
Z
62/5:
A=torch.relu(Z)
A
62/6:
relu = nn.ReLU()
relu(Z)
62/7:
image1=torch.zeros(1,1,4,4)
image1[0,0,0,:]=torch.tensor([1.0,2.0,3.0,-4.0])
image1[0,0,1,:]=torch.tensor([0.0,2.0,-3.0,0.0])
image1[0,0,2,:]=torch.tensor([0.0,2.0,3.0,1.0])

image1
62/8:
max1=torch.nn.MaxPool2d(2,stride=1)
max1(image1)
62/9:
max1=torch.nn.MaxPool2d(2)
max1(image1)
63/1:
import torch 
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage, misc
63/2: conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3)
63/3:
Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])
Gy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])

conv1.state_dict()['weight'][0][0]=Gx
conv1.state_dict()['weight'][1][0]=Gy
conv1.state_dict()['weight'][2][0]=torch.ones(3,3)
63/4:
conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])
conv1.state_dict()['bias']
63/5:
for x in conv1.state_dict()['weight']:
    print(x)
63/6:
image=torch.zeros(1,1,5,5)
image[0,0,:,2]=1
image
63/7:
plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)
plt.colorbar()
plt.show()
63/8: out=conv1(image)
63/9: out.shape
63/10:
for channel,image in enumerate(out[0]):
    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
    print(image)
    plt.title("channel {}".format(channel))
    plt.colorbar()
    plt.show()
63/11:
image1=torch.zeros(1,1,5,5)
image1[0,0,2,:]=1
print(image1)
plt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
plt.show()
63/12:
out1=conv1(image1)
for channel,image in enumerate(out1[0]):
    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
    print(image)
    plt.title("channel {}".format(channel))
    plt.colorbar()
    plt.show()
63/13:
image2=torch.zeros(1,2,5,5)
image2[0,0,2,:]=-2
image2[0,1,2,:]=1
image2
63/14:
for channel,image in enumerate(image2[0]):
    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
    print(image)
    plt.title("channel {}".format(channel))
    plt.colorbar()
    plt.show()
63/15: conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)
63/16:
Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])
conv3.state_dict()['weight'][0][0]=1*Gx1
conv3.state_dict()['weight'][0][1]=-2*Gx1
conv3.state_dict()['bias'][:]=torch.tensor([0.0])
63/17: conv3.state_dict()['weight']
63/18: conv3(image2)
63/19:
conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)
conv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])
conv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])


conv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])
conv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])

conv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])
conv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])
63/20: conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])
63/21:
image4=torch.zeros(1,2,5,5)
image4[0][0]=torch.ones(5,5)
image4[0][1][2][2]=1
for channel,image in enumerate(image4[0]):
    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)
    print(image)
    plt.title("channel {}".format(channel))
    plt.colorbar()
    plt.show()
63/22:
z=conv4(image4)
z
63/23:
imageA=torch.zeros(1,1,5,5)
imageB=torch.zeros(1,1,5,5)
imageA[0,0,2,:]=-2
imageB[0,0,2,:]=1


conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)
conv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)


Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])
conv5.state_dict()['weight'][0][0]=1*Gx1
conv6.state_dict()['weight'][0][0]=-2*Gx1
conv5.state_dict()['bias'][:]=torch.tensor([0.0])
conv6.state_dict()['bias'][:]=torch.tensor([0.0])
63/24:
imageA=torch.zeros(1,1,5,5)
imageB=torch.zeros(1,1,5,5)
imageA[0,0,2,:]=-2
imageB[0,0,2,:]=1


conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)
conv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)


Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])
conv5.state_dict()['weight'][0][0]=1*Gx1
conv6.state_dict()['weight'][0][0]=-2*Gx1
conv5.state_dict()['bias'][:]=torch.tensor([0.0])
conv6.state_dict()['bias'][:]=torch.tensor([0.0])

conv5(imageA)+conv6(imageB)
64/1:
import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np
import pandas as pd
64/2: torch.manual_seed(4)
64/3:
def plot_channels(W):
    #number of output channels 
    n_out=W.shape[0]
    #number of input channels 
    n_in=W.shape[1]
    w_min=W.min().item()
    w_max=W.max().item()
    fig, axes = plt.subplots(n_out,n_in)
    fig.subplots_adjust(hspace = 0.1)
    out_index=0
    in_index=0
    #plot outputs as rows inputs as columns 
    for ax in axes.flat:
    
        if in_index>n_in-1:
            out_index=out_index+1
            in_index=0
              
        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')
        ax.set_yticklabels([])
        ax.set_xticklabels([])
        in_index=in_index+1

    plt.show()
64/4:
def show_data(dataset,sample):

    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')
    plt.title('y='+str(dataset.y[sample].item()))
    plt.show()
64/5:
from torch.utils.data import Dataset, DataLoader
class Data(Dataset):
    def __init__(self,N_images=100,offset=0,p=0.9, train=False):
        """
        p:portability that pixel is wight  
        N_images:number of images 
        offset:set a random vertical and horizontal offset images by a sample should be less than 3 
        """
        if train==True:
            np.random.seed(1)  
        
        #make images multiple of 3 
        N_images=2*(N_images//2)
        images=np.zeros((N_images,1,11,11))
        start1=3
        start2=1
        self.y=torch.zeros(N_images).type(torch.long)

        for n in range(N_images):
            if offset>0:
        
                low=int(np.random.randint(low=start1, high=start1+offset, size=1))
                high=int(np.random.randint(low=start2, high=start2+offset, size=1))
            else:
                low=4
                high=1
        
            if n<=N_images//2:
                self.y[n]=0
                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))
            elif  n>N_images//2:
                self.y[n]=1
                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))
           
        
        
        self.x=torch.from_numpy(images).type(torch.FloatTensor)
        self.len=self.x.shape[0]
        del(images)
        np.random.seed(0)
    def __getitem__(self,index):      
        return self.x[index],self.y[index]
    def __len__(self):
        return self.len
64/6:
def plot_activations(A,number_rows= 1,name=""):
    A=A[0,:,:,:].detach().numpy()
    n_activations=A.shape[0]
    
    
    print(n_activations)
    A_min=A.min().item()
    A_max=A.max().item()

    if n_activations==1:

        # Plot the image.
        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')

    else:
        fig, axes = plt.subplots(number_rows, n_activations//number_rows)
        fig.subplots_adjust(hspace = 0.4)
        for i,ax in enumerate(axes.flat):
            if i< n_activations:
                # Set the label for the sub-plot.
                ax.set_xlabel( "activation:{0}".format(i+1))

                # Plot the image.
                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')
                ax.set_xticks([])
                ax.set_yticks([])
    plt.show()
64/7:

def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):
    #by Duane Nielsen
    from math import floor
    if type(kernel_size) is not tuple:
        kernel_size = (kernel_size, kernel_size)
    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)
    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)
    return h, w
64/8:
N_images=10000
train_dataset=Data(N_images=N_images)
64/9:
validation_dataset=Data(N_images=1000,train=False)
validation_dataset
64/10: show_data(train_dataset,0)
64/11: show_data(train_dataset,N_images//2+2)
64/12: show_data(train_dataset,1)
64/13: show_data(train_dataset,2)
64/14: show_data(train_dataset,5)
64/15: show_data(train_dataset,100)
64/16: show_data(train_dataset,0)
64/17: 1000//4
64/18: N_images//4
64/19: show_data(train_dataset,2500)
64/20: N_images//2+2
64/21: show_data(train_dataset,5002)
64/22: show_data(train_dataset,5000)
64/23: show_data(train_dataset,5001)
64/24: show_data(train_dataset,5701)
64/25: show_data(train_dataset,0)
64/26:
out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)
print(out)
out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)
print(out1)
out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)
print(out2)

out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)
print(out3)
64/27:
class CNN(nn.Module):
    def __init__(self,out_1=2,out_2=1):
        
        super(CNN,self).__init__()
        #first Convolutional layers 
        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)
        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)

        #second Convolutional layers
        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)
        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)
        #max pooling 

        #fully connected layer 
        self.fc1=nn.Linear(out_2*7*7,2)
        
    def forward(self,x):
        #first Convolutional layers
        x=self.cnn1(x)
        #activation function 
        x=torch.relu(x)
        #max pooling 
        x=self.maxpool1(x)
        #first Convolutional layers
        x=self.cnn2(x)
        #activation function
        x=torch.relu(x)
        #max pooling
        x=self.maxpool2(x)
        #flatten output 
        x=x.view(x.size(0),-1)
        #fully connected layer
        x=self.fc1(x)
        return x
    
    def activations(self,x):
        #outputs activation this is not necessary just for fun 
        z1=self.cnn1(x)
        a1=torch.relu(z1)
        out=self.maxpool1(a1)
        
        z2=self.cnn2(out)
        a2=torch.relu(z2)
        out=self.maxpool2(a2)
        out=out.view(out.size(0),-1)
        return z1,a1,z2,a2,out
64/28: model=CNN(2,1)
64/29: model
64/30:

plot_channels(model.state_dict()['cnn1.weight'])
64/31: plot_channels(model.state_dict()['cnn2.weight'])
64/32: criterion=nn.CrossEntropyLoss()
64/33:
learning_rate=0.001

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
64/34:

train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)
validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)
64/35:
n_epochs=10
cost_list=[]
accuracy_list=[]
N_test=len(validation_dataset)
cost=0
#n_epochs
for epoch in range(n_epochs):
    cost=0    
    for x, y in train_loader:
      

        #clear gradient 
        optimizer.zero_grad()
        #make a prediction 
        z=model(x)
        # calculate loss 
        loss=criterion(z,y)
        # calculate gradients of parameters 
        loss.backward()
        # update parameters 
        optimizer.step()
        cost+=loss.item()
    cost_list.append(cost)
        
        
    correct=0
    #perform a prediction on the validation  data  
    for x_test, y_test in validation_loader:

        z=model(x_test)
        _,yhat=torch.max(z.data,1)

        correct+=(yhat==y_test).sum().item()
        

    accuracy=correct/N_test

    accuracy_list.append(accuracy)
64/36:
fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.plot(cost_list,color=color)
ax1.set_xlabel('epoch',color=color)
ax1.set_ylabel('total loss',color=color)
ax1.tick_params(axis='y', color=color)
    
ax2 = ax1.twinx()  
color = 'tab:blue'
ax2.set_ylabel('accuracy', color=color)  
ax2.plot( accuracy_list, color=color)
ax2.tick_params(axis='y', labelcolor=color)
fig.tight_layout()
64/37: model.state_dict()['cnn1.weight']
64/38: plot_channels(model.state_dict()['cnn1.weight'])
64/39: model.state_dict()['cnn1.weight']
64/40: plot_channels(model.state_dict()['cnn2.weight'])
64/41: show_data(train_dataset,N_images//2+2)
64/42:
out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))
out=model.activations(train_dataset[0][0].view(1,1,11,11))
64/43:
plot_activations(out[0],number_rows=1,name=" feature map")
plt.show()
64/44:
out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))
#out=model.activations(train_dataset[0][0].view(1,1,11,11))
64/45:
plot_activations(out[0],number_rows=1,name=" feature map")
plt.show()
64/46:
out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))
#out=model.activations(train_dataset[0][0].view(1,1,11,11))
64/47:
plot_activations(out[0],number_rows=1,name=" feature map")
plt.show()
64/48:
plot_activations(out[2],number_rows=1,name="2nd feature map")
plt.show()
64/49:
plot_activations(out[3],number_rows=1,name="first feature map")
plt.show()
64/50: out1=out[4][0].detach().numpy()
64/51:
out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()
out0
64/52:
plt.subplot(2, 1, 1)
plt.plot( out1, 'b')
plt.title('Flatted Activation Values  ')
plt.ylabel('Activation')
plt.xlabel('index')
plt.subplot(2, 1, 2)
plt.plot(out0, 'r')
plt.xlabel('index')
plt.ylabel('Activation')
64/53:
out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))
#out=model.activations(train_dataset[0][0].view(1,1,11,11))
our
64/54:
out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))
#out=model.activations(train_dataset[0][0].view(1,1,11,11))
out
64/55:
plot_activations(out[1],number_rows=1,name=" feature map")
plt.show()
64/56:
plot_activations(out[0],number_rows=1,name=" feature map")
plt.show()
64/57:
plot_activations(out[1],number_rows=1,name="first feature map")
plt.show()
64/58:
plot_activations(out[3],number_rows=1,name="first feature map")
plt.show()
64/59:
out1=out[4][0].detach().numpy()
out1
64/60:
out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()
out0
64/61:
plt.subplot(2, 1, 1)
plt.plot( out1, 'b')
plt.title('Flatted Activation Values  ')
plt.ylabel('Activation')
plt.xlabel('index')
plt.subplot(2, 1, 2)
plt.plot(out0, 'r')
plt.xlabel('index')
plt.ylabel('Activation')
64/62: train_dataset
64/63: print(train_dataset)
64/64: print(list(train_dataset)
64/65: print(list(train_dataset))
64/66: print(list(train_dataset[0]))
64/67: print(list(train_dataset[1]))
64/68: print(list(train_dataset[5000]))
64/69: print(list(train_dataset[5001]))
64/70: print(list(train_dataset[5002]))
64/71: print(list(train_dataset[2]))
65/1:


# Import the libraries we need to use in this lab

# Using the following line code to install the torchvision library
# !conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np
65/2:
# Define the function for plotting the channels

def plot_channels(W):
    n_out = W.shape[0]
    n_in = W.shape[1]
    w_min = W.min().item()
    w_max = W.max().item()
    fig, axes = plt.subplots(n_out, n_in)
    fig.subplots_adjust(hspace=0.1)
    out_index = 0
    in_index = 0
    
    #plot outputs as rows inputs as columns 
    for ax in axes.flat:
        if in_index > n_in-1:
            out_index = out_index + 1
            in_index = 0
        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')
        ax.set_yticklabels([])
        ax.set_xticklabels([])
        in_index = in_index + 1

    plt.show()
65/3:
# Define the function for plotting the parameters

def plot_parameters(W, number_rows=1, name="", i=0):
    W = W.data[:, i, :, :]
    n_filters = W.shape[0]
    w_min = W.min().item()
    w_max = W.max().item()
    fig, axes = plt.subplots(number_rows, n_filters // number_rows)
    fig.subplots_adjust(hspace=0.4)

    for i, ax in enumerate(axes.flat):
        if i < n_filters:
            # Set the label for the sub-plot.
            ax.set_xlabel("kernel:{0}".format(i + 1))

            # Plot the image.
            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')
            ax.set_xticks([])
            ax.set_yticks([])
    plt.suptitle(name, fontsize=10)    
    plt.show()
65/4:
# Define the function for plotting the activations

def plot_activations(A, number_rows=1, name="", i=0):
    A = A[0, :, :, :].detach().numpy()
    n_activations = A.shape[0]
    A_min = A.min().item()
    A_max = A.max().item()
    fig, axes = plt.subplots(number_rows, n_activations // number_rows)
    fig.subplots_adjust(hspace = 0.4)

    for i, ax in enumerate(axes.flat):
        if i < n_activations:
            # Set the label for the sub-plot.
            ax.set_xlabel("activation:{0}".format(i + 1))

            # Plot the image.
            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')
            ax.set_xticks([])
            ax.set_yticks([])
    plt.show()
65/5:
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1].item()))
65/6:


IMAGE_SIZE = 16


composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])
65/7:

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)
65/8:
# Make the validating 

validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)
65/9:
# Show the data type for each element in dataset

train_dataset[0][1].type()
65/10:
# Show the data type for each element in dataset

# train_dataset[0][1].type()   doesn't work
dtype(train_dataset[0][1])
65/11:
# Show the data type for each element in dataset

# train_dataset[0][1].type()   doesn't work
type(train_dataset[0][1])
65/12:
# The label for the fourth data element

train_dataset[3][1]
65/13:
# The image for the fourth data element
show_data(train_dataset[3])
65/14:
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1])
65/15:
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1]))
65/16:
# The image for the fourth data element
show_data(train_dataset[3])
65/17:
class CNN(nn.Module):
    
    # Contructor
    def __init__(self, out_1=16, out_2=32):
        super(CNN, self).__init__()
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)
        self.maxpool1=nn.MaxPool2d(kernel_size=2)

        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)
        self.maxpool2=nn.MaxPool2d(kernel_size=2)
        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)
    
    # Prediction
    def forward(self, x):
        x = self.cnn1(x)
        x = torch.relu(x)
        x = self.maxpool1(x)
        x = self.cnn2(x)
        x = torch.relu(x)
        x = self.maxpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x
    
    # Outputs in each steps
    def activations(self, x):
        #outputs activation this is not necessary
        z1 = self.cnn1(x)
        a1 = torch.relu(z1)
        out = self.maxpool1(a1)
        
        z2 = self.cnn2(out)
        a2 = torch.relu(z2)
        out1 = self.maxpool2(a2)
        out = out.view(out.size(0),-1)
        return z1, a1, z2, a2, out1,out
65/18:

def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):
    #by Duane Nielsen
    from math import floor
    if type(kernel_size) is not tuple:
        kernel_size = (kernel_size, kernel_size)
    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)
    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)
    return h, w
65/19:
out=conv_output_shape((16,16), kernel_size=5, stride=1, pad=2, dilation=1)
print(out)
out1=conv_output_shape(out, kernel_size=2, stride=2, pad=0, dilation=1)
print(out1)
out2=conv_output_shape(out1, kernel_size=5, stride=1, pad=2, dilation=1)
print(out2)

out3=conv_output_shape(out2, kernel_size=2, stride=2, pad=0, dilation=1)
print(out3)
65/20:
class CNN(nn.Module):
    
    # Contructor
    def __init__(self, out_1=16, out_2=32):
        super(CNN, self).__init__()
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)
        self.maxpool1=nn.MaxPool2d(kernel_size=2)

        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)
        self.maxpool2=nn.MaxPool2d(kernel_size=2)
        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)
    
    # Prediction
    def forward(self, x):
        x = self.cnn1(x)
        x = torch.relu(x)
        x = self.maxpool1(x)
        x = self.cnn2(x)
        x = torch.relu(x)
        x = self.maxpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x
    
    # Outputs in each steps
    def activations(self, x):
        #outputs activation this is not necessary
        z1 = self.cnn1(x)
        a1 = torch.relu(z1)
        out = self.maxpool1(a1)
        
        z2 = self.cnn2(out)
        a2 = torch.relu(z2)
        out1 = self.maxpool2(a2)
        out = out.view(out.size(0),-1)
        return z1, a1, z2, a2, out1,out
65/21:
# Create the model object using CNN class

model = CNN(out_1=16, out_2=32)
65/22:
# Plot the parameters

plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name="1st layer kernels before training ")
plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )
65/23:
criterion = nn.CrossEntropyLoss()
learning_rate = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)
65/24:
# Train the model

n_epochs=3
cost_list=[]
accuracy_list=[]
N_test=len(validation_dataset)
COST=0

def train_model(n_epochs):
    for epoch in range(n_epochs):
        COST=0
        for x, y in train_loader:
            optimizer.zero_grad()
            z = model(x)
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            COST+=loss.data
        
        cost_list.append(COST)
        correct=0
        #perform a prediction on the validation  data  
        for x_test, y_test in validation_loader:
            z = model(x_test)
            _, yhat = torch.max(z.data, 1)
            correct += (yhat == y_test).sum().item()
        accuracy = correct / N_test
        accuracy_list.append(accuracy)
     
train_model(n_epochs)
65/25:
# Plot the loss and accuracy

fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.plot(cost_list, color=color)
ax1.set_xlabel('epoch', color=color)
ax1.set_ylabel('Cost', color=color)
ax1.tick_params(axis='y', color=color)
    
ax2 = ax1.twinx()  
color = 'tab:blue'
ax2.set_ylabel('accuracy', color=color) 
ax2.set_xlabel('epoch', color=color)
ax2.plot( accuracy_list, color=color)
ax2.tick_params(axis='y', color=color)
fig.tight_layout()
65/26:
# Plot the channels

plot_channels(model.state_dict()['cnn1.weight'])
plot_channels(model.state_dict()['cnn2.weight'])
66/1:
# Plot the channels

plot_channels(model.state_dict()['cnn1.weight'])
plot_channels(model.state_dict()['cnn2.weight'])
66/2:


# Import the libraries we need to use in this lab

# Using the following line code to install the torchvision library
# !conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np
66/3:
# Define the function for plotting the channels

def plot_channels(W):
    n_out = W.shape[0]
    n_in = W.shape[1]
    w_min = W.min().item()
    w_max = W.max().item()
    fig, axes = plt.subplots(n_out, n_in)
    fig.subplots_adjust(hspace=0.1)
    out_index = 0
    in_index = 0
    
    #plot outputs as rows inputs as columns 
    for ax in axes.flat:
        if in_index > n_in-1:
            out_index = out_index + 1
            in_index = 0
        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')
        ax.set_yticklabels([])
        ax.set_xticklabels([])
        in_index = in_index + 1

    plt.show()
66/4:
# Define the function for plotting the parameters

def plot_parameters(W, number_rows=1, name="", i=0):
    W = W.data[:, i, :, :]
    n_filters = W.shape[0]
    w_min = W.min().item()
    w_max = W.max().item()
    fig, axes = plt.subplots(number_rows, n_filters // number_rows)
    fig.subplots_adjust(hspace=0.4)

    for i, ax in enumerate(axes.flat):
        if i < n_filters:
            # Set the label for the sub-plot.
            ax.set_xlabel("kernel:{0}".format(i + 1))

            # Plot the image.
            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')
            ax.set_xticks([])
            ax.set_yticks([])
    plt.suptitle(name, fontsize=10)    
    plt.show()
66/5:
# Define the function for plotting the activations

def plot_activations(A, number_rows=1, name="", i=0):
    A = A[0, :, :, :].detach().numpy()
    n_activations = A.shape[0]
    A_min = A.min().item()
    A_max = A.max().item()
    fig, axes = plt.subplots(number_rows, n_activations // number_rows)
    fig.subplots_adjust(hspace = 0.4)

    for i, ax in enumerate(axes.flat):
        if i < n_activations:
            # Set the label for the sub-plot.
            ax.set_xlabel("activation:{0}".format(i + 1))

            # Plot the image.
            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')
            ax.set_xticks([])
            ax.set_yticks([])
    plt.show()
66/6:
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1]))
66/7:


IMAGE_SIZE = 16


composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])
66/8:

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)
66/9:
# Make the validating 

validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)
66/10:
# Show the data type for each element in dataset

# train_dataset[0][1].type()   doesn't work
type(train_dataset[0][1])
66/11:
# The label for the fourth data element

train_dataset[3][1]
66/12:
# The image for the fourth data element
show_data(train_dataset[3])
66/13:

def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):
    #by Duane Nielsen
    from math import floor
    if type(kernel_size) is not tuple:
        kernel_size = (kernel_size, kernel_size)
    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)
    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)
    return h, w
66/14:
out=conv_output_shape((16,16), kernel_size=5, stride=1, pad=2, dilation=1)
print(out)
out1=conv_output_shape(out, kernel_size=2, stride=2, pad=0, dilation=1)
print(out1)
out2=conv_output_shape(out1, kernel_size=5, stride=1, pad=2, dilation=1)
print(out2)

out3=conv_output_shape(out2, kernel_size=2, stride=2, pad=0, dilation=1)
print(out3)
66/15:
class CNN(nn.Module):
    
    # Contructor
    def __init__(self, out_1=16, out_2=32):
        super(CNN, self).__init__()
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)
        self.maxpool1=nn.MaxPool2d(kernel_size=2)

        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)
        self.maxpool2=nn.MaxPool2d(kernel_size=2)
        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)
    
    # Prediction
    def forward(self, x):
        x = self.cnn1(x)
        x = torch.relu(x)
        x = self.maxpool1(x)
        x = self.cnn2(x)
        x = torch.relu(x)
        x = self.maxpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x
    
    # Outputs in each steps
    def activations(self, x):
        #outputs activation this is not necessary
        z1 = self.cnn1(x)
        a1 = torch.relu(z1)
        out = self.maxpool1(a1)
        
        z2 = self.cnn2(out)
        a2 = torch.relu(z2)
        out1 = self.maxpool2(a2)
        out = out.view(out.size(0),-1)
        return z1, a1, z2, a2, out1,out
66/16:
# Create the model object using CNN class

model = CNN(out_1=16, out_2=32)
66/17:
# Plot the parameters

plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name="1st layer kernels before training ")
plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )
66/18:
criterion = nn.CrossEntropyLoss()
learning_rate = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)
66/19:
# Train the model

n_epochs=3
cost_list=[]
accuracy_list=[]
N_test=len(validation_dataset)
COST=0

def train_model(n_epochs):
    for epoch in range(n_epochs):
        COST=0
        for x, y in train_loader:
            optimizer.zero_grad()
            z = model(x)
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            COST+=loss.data
        
        cost_list.append(COST)
        correct=0
        #perform a prediction on the validation  data  
        for x_test, y_test in validation_loader:
            z = model(x_test)
            _, yhat = torch.max(z.data, 1)
            correct += (yhat == y_test).sum().item()
        accuracy = correct / N_test
        accuracy_list.append(accuracy)
     
train_model(n_epochs)
66/20:
# Plot the loss and accuracy

fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.plot(cost_list, color=color)
ax1.set_xlabel('epoch', color=color)
ax1.set_ylabel('Cost', color=color)
ax1.tick_params(axis='y', color=color)
    
ax2 = ax1.twinx()  
color = 'tab:blue'
ax2.set_ylabel('accuracy', color=color) 
ax2.set_xlabel('epoch', color=color)
ax2.plot( accuracy_list, color=color)
ax2.tick_params(axis='y', color=color)
fig.tight_layout()
66/21:
# Plot the channels

plot_channels(model.state_dict()['cnn1.weight'])
plot_channels(model.state_dict()['cnn2.weight'])
68/1:

# Import the libraries we need to use in this lab

# Using the following line code to install the torchvision library
# !conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1].item()))
68/2:

# Import the libraries we need to use in this lab

# Using the following line code to install the torchvision library
# !conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1])
68/3:

# Import the libraries we need to use in this lab

# Using the following line code to install the torchvision library
# !conda install -y torchvision

import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import matplotlib.pylab as plt
import numpy as np
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1]))
68/4:

IMAGE_SIZE = 16

composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])
68/5:

train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)
68/6:
# Make the validating 

validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)
68/7:
# Show the data type for each element in dataset

train_dataset[0][1].type()
68/8:
# Show the data type for each element in dataset

train_dataset[0][1]
68/9:
# The label for the fourth data element

train_dataset[3][1]
68/10:
# The image for the fourth data element
show_data(train_dataset[3])
68/11:
class CNN(nn.Module):
    
    # Contructor
    def __init__(self, out_1=16, out_2=32):
        super(CNN, self).__init__()
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)
        self.maxpool1=nn.MaxPool2d(kernel_size=2)

        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)
        self.maxpool2=nn.MaxPool2d(kernel_size=2)
        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)
    
    # Prediction
    def forward(self, x):
        x = self.cnn1(x)
        x = torch.relu(x)
        x = self.maxpool1(x)
        x = self.cnn2(x)
        x = torch.relu(x)
        x = self.maxpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x
68/12:
class CNN_batch(nn.Module):
    
    # Contructor
    def __init__(self, out_1=16, out_2=32,number_of_classes=10):
        super(CNN_batch, self).__init__()
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)
        self.conv1_bn = nn.BatchNorm2d(out_1)

        self.maxpool1=nn.MaxPool2d(kernel_size=2)
        
        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)
        self.conv2_bn = nn.BatchNorm2d(out_2)

        self.maxpool2=nn.MaxPool2d(kernel_size=2)
        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)
        self.bn_fc1 = nn.BatchNorm1d(10)
    
    # Prediction
    def forward(self, x):
        x = self.cnn1(x)
        x=self.conv1_bn(x)
        x = torch.relu(x)
        x = self.maxpool1(x)
        x = self.cnn2(x)
        x=self.conv2_bn(x)
        x = torch.relu(x)
        x = self.maxpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x=self.bn_fc1(x)
        return x
68/13:
def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4):
    
    #global variable 
    N_test=len(validation_dataset)
    accuracy_list=[]
    loss_list=[]
    for epoch in range(n_epochs):
        for x, y in train_loader:
            model.train()
            optimizer.zero_grad()
            z = model(x)
            loss = criterion(z, y)
            loss.backward()
            optimizer.step()
            loss_list.append(loss.data)

        correct=0
        #perform a prediction on the validation  data  
        for x_test, y_test in validation_loader:
            model.eval()
            z = model(x_test)
            _, yhat = torch.max(z.data, 1)
            correct += (yhat == y_test).sum().item()
        accuracy = correct / N_test
        accuracy_list.append(accuracy)
     
    return accuracy_list, loss_list
68/14:
# Create the model object using CNN class
model = CNN(out_1=16, out_2=32)
68/15:
criterion = nn.CrossEntropyLoss()
learning_rate = 0.1
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)
validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)
68/16:
# Train the model
accuracy_list_normal, loss_list_normal=train_model(model=model,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)
69/1:
# PyTorch Modules you need for this lab

from torch.utils.data import Dataset, DataLoader

from torchvision import transforms
import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
torch.manual_seed(0)
69/2:
# Other non-PyTorch Modules

from matplotlib.pyplot import imshow
import matplotlib.pylab as plt

from PIL import Image
69/3:
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1])
69/4:
def show_data(data_sample):
    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')
    plt.title('y = '+ str(data_sample[1]))
68/17:
model_batch=CNN_batch(out_1=16, out_2=32)
criterion = nn.CrossEntropyLoss()
learning_rate = 0.1
optimizer = torch.optim.SGD(model_batch.parameters(), lr = learning_rate)
accuracy_list_batch, loss_list_batch=train_model(model=model_batch,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)
69/5:
#Hint:

IMAGE_SIZE = 16

transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
transforms.ToTensor()#
composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])
69/6:

dataset_train=dsets.FashionMNIST(root= '.fashion/data', train=True, transform=composed,  download=True)
dataset_val=dsets.FashionMNIST(root= '.fashion/data', train=False, transform=composed, download=True)
69/7:
for n,data_sample in enumerate(dataset_val):

    show_data(data_sample)
    plt.show()
    if n==2:
        break
69/8:
class CNN_batch(nn.Module):
    
    # Contructor
    def __init__(self, out_1=16, out_2=32,number_of_classes=10):
        super(CNN_batch, self).__init__()
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)
        self.conv1_bn = nn.BatchNorm2d(out_1)

        self.maxpool1=nn.MaxPool2d(kernel_size=2)
        
        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)
        self.conv2_bn = nn.BatchNorm2d(out_2)

        self.maxpool2=nn.MaxPool2d(kernel_size=2)
        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)
        self.bn_fc1 = nn.BatchNorm1d(10)
    
    # Prediction
    def forward(self, x):
        x = self.cnn1(x)
        x=self.conv1_bn(x)
        x = torch.relu(x)
        x = self.maxpool1(x)
        x = self.cnn2(x)
        x=self.conv2_bn(x)
        x = torch.relu(x)
        x = self.maxpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x=self.bn_fc1(x)
        return x
69/9:
class CNN(nn.Module):
    
    # Contructor
    def __init__(self, out_1=16, out_2=32,number_of_classes=10):
        super(CNN, self).__init__()
        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)
        self.maxpool1=nn.MaxPool2d(kernel_size=2)

        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)
        self.maxpool2=nn.MaxPool2d(kernel_size=2)
        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)
    
    # Prediction
    def forward(self, x):
        x = self.cnn1(x)
        x = torch.relu(x)
        x = self.maxpool1(x)
        x = self.cnn2(x)
        x = torch.relu(x)
        x = self.maxpool2(x)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x
69/10:
train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=100 )
test_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=100 )
69/11:
#model = CNN(out_1=16, out_2=32,number_of_classes=10)
model =CNN_batch(out_1=16, out_2=32,number_of_classes=10)
69/12:
import time
start_time = time.time()

cost_list=[]
accuracy_list=[]
N_test=len(dataset_val)
learning_rate =0.1
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
criterion = nn.CrossEntropyLoss()
n_epochs=5
for epoch in range(n_epochs):
    cost=0
    model.train()
    for x, y in train_loader:
        optimizer.zero_grad()
        z = model(x)
        loss = criterion(z, y)
        loss.backward()
        optimizer.step()
        cost+=loss.item()
    correct=0
    #perform a prediction on the validation  data 
    model.eval()
    for x_test, y_test in test_loader:
        z = model(x_test)
        _, yhat = torch.max(z.data, 1)
        correct += (yhat == y_test).sum().item()
    accuracy = correct / N_test
    accuracy_list.append(accuracy)
    cost_list.append(cost)
69/13:
fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.plot(cost_list, color=color)
ax1.set_xlabel('epoch', color=color)
ax1.set_ylabel('Cost', color=color)
ax1.tick_params(axis='y', color=color)
    
ax2 = ax1.twinx()  
color = 'tab:blue'
ax2.set_ylabel('accuracy', color=color) 
ax2.set_xlabel('epoch', color=color)
ax2.plot( accuracy_list, color=color)
ax2.tick_params(axis='y', color=color)
fig.tight_layout()
70/1:
import time
import numpy as np
import tensorflow as tf
70/2:
!mkdir data
!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip
!unzip -o data/ptb.zip -d data
!cp data/ptb/reader.py .

import reader
70/3:
import time
import numpy as np
import tensorflow as tf
import wget
70/4:
import time
import numpy as np
import tensorflow as tf
import wget
70/5:
!mkdir data
!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip
!unzip -o data/ptb.zip -d data
!cp data/ptb/reader.py .

import reader
70/6:
!mkdir data
!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip
!unzip -o data/ptb.zip -d data
!cp data/ptb/reader.py .

import reader
70/7:
import time
import numpy as np
import tensorflow as tf
import wget
70/8:
import time
import numpy as np
import tensorflow as tf
70/9:
!mkdir data
!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip
!unzip -o data/ptb.zip -d data
!cp data/ptb/reader.py .

import reader
70/10:
import time
import numpy as np
import tensorflow as tf
70/11:
import time
import numpy as np
import tensorflow as tf
import wget
70/12:
!mkdir data
!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip
!unzip -o data/ptb.zip -d data
!cp data/ptb/reader.py .

import reader
70/13:
import time
import numpy as np
import tensorflow as tf
import wget
70/14:
!mkdir data
!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip
!unzip -o data/ptb.zip -d data
!cp data/ptb/reader.py .

import reader
70/15:
!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz 
!tar xzf simple-examples.tgz -C data/
70/16:
#Initial weight scale
init_scale = 0.1
#Initial learning rate
learning_rate = 1.0
#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)
max_grad_norm = 5
#The number of layers in our model
num_layers = 2
#The total number of recurrence steps, also known as the number of layers when our RNN is "unfolded"
num_steps = 20
#The number of processing units (neurons) in the hidden layers
hidden_size_l1 = 256
hidden_size_l2 = 128
#The maximum number of epochs trained with the initial learning rate
max_epoch_decay_lr = 4
#The total number of epochs in training
max_epoch = 15
#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)
#At 1, we ignore the Dropout Layer wrapping.
keep_prob = 1
#The decay for the learning rate
decay = 0.5
#The size for each batch of data
batch_size = 60
#The size of our vocabulary
vocab_size = 10000
embeding_vector_size = 200
#Training flag to separate training from testing
is_training = 1
#Data directory for our dataset
data_dir = "data/simple-examples/data/"
70/17: session = tf.InteractiveSession()
70/18:
import time
import numpy as np
import tensorflow as tf
import wget
70/19: session = tf.InteractiveSession()
70/20: session = tf.InteractiveSession()
70/21:
tf.__version__
session = tf.InteractiveSession()
70/22:
tf.__version__
#session = tf.InteractiveSession()
70/23:
import time
import numpy as np
import tensorflow as tf
import wget
70/24:
#Initial weight scale
init_scale = 0.1
#Initial learning rate
learning_rate = 1.0
#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)
max_grad_norm = 5
#The number of layers in our model
num_layers = 2
#The total number of recurrence steps, also known as the number of layers when our RNN is "unfolded"
num_steps = 20
#The number of processing units (neurons) in the hidden layers
hidden_size_l1 = 256
hidden_size_l2 = 128
#The maximum number of epochs trained with the initial learning rate
max_epoch_decay_lr = 4
#The total number of epochs in training
max_epoch = 15
#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)
#At 1, we ignore the Dropout Layer wrapping.
keep_prob = 1
#The decay for the learning rate
decay = 0.5
#The size for each batch of data
batch_size = 60
#The size of our vocabulary
vocab_size = 10000
embeding_vector_size = 200
#Training flag to separate training from testing
is_training = 1
#Data directory for our dataset
data_dir = "data/simple-examples/data/"
70/25: tf.__version__
70/26: session = tf.InteractiveSession()
70/27:
import time
import numpy as np
import tensorflow as tfc
import tensorflow.compat.v1 as tf

import wget
70/28:
import time
import numpy as np
import tensorflow as tfc
import tensorflow.compat.v1 as tfcc

import wget
70/29:
import time
import numpy as np
import tensorflow as tfc
#import tensorflow.compat.v1 as tfcc

import wget
70/30:
import time
import numpy as np
import tensorflow as tf
#import tensorflow.compat.v1 as tfcc

import wget
70/31: tf.__version__
70/32: session = tf.compat.v1.InteractiveSession()
70/33:
# Reads the data and separates it into training data, validation data and testing data
raw_data = reader.ptb_raw_data(data_dir)
train_data, valid_data, test_data, vocab, word_to_id = raw_data
70/34:
import time
import numpy as np
import tensorflow as tf
#import tensorflow.compat.v1 as tfcc

import wget
70/35: tf.__version__
70/36: tf.__version__
70/37: session = tf.InteractiveSession()
70/38:
#!conda install tensorflow==1.8.0
import time
import numpy as np
import tensorflow as tf
#import tensorflow.compat.v1 as tfcc

import wget
70/39: tf.__version__
70/40: tf.__version__
70/41:
!conda install tensorflow==1.8.0
import time
import numpy as np
import tensorflow as tf
#import tensorflow.compat.v1 as tfcc

import wget
70/42: tf.__version__
70/43:
!conda install tensorflow==1.8.0
import time
import numpy as np
import tensorflow as tf
#import tensorflow.compat.v1 as tfcc

import wget
71/1:
#!conda install tensorflow==1.8.0
import time
import numpy as np
import tensorflow as tf
#import tensorflow.compat.v1 as tfcc

import wget
72/1:
import urllib.request
with urllib.request.urlopen("http://deeplearning.net/tutorial/code/utils.py") as url:
    response = url.read()
target = open('utils.py', 'w')
target.write(response.decode('utf-8'))
target.close()
72/2:
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
#!pip install pillow
from PIL import Image
from utils import tile_raster_images
import matplotlib.pyplot as plt
%matplotlib inline
72/3:
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
#!pip install pillow
from PIL import Image
from utils import tile_raster_images
import matplotlib.pyplot as plt
%matplotlib inline
72/4:
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
#!pip install pillow
from PIL import Image
from utils import tile_raster_images
import matplotlib.pyplot as plt
%matplotlib inline
72/5:
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
#!pip install pillow
from PIL import Image
from utils import tile_raster_images
import matplotlib.pyplot as plt
%matplotlib inline
73/1:
import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data
#!pip install pillow
from PIL import Image
from utils import tile_raster_images
import matplotlib.pyplot as plt
%matplotlib inline
73/2:
v_bias = tf.placeholder("float", [7])
h_bias = tf.placeholder("float", [2])
73/3: W = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(7, 2)).astype(np.float32))
73/4:
sess = tf.Session()
X = tf.constant([[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]])
v_state = X
print ("Input: ", sess.run(v_state))

h_bias = tf.constant([0.1, 0.1])
print ("hb: ", sess.run(h_bias))
print ("w: ", sess.run(W))

# Calculate the probabilities of turning the hidden units on:
h_prob = tf.nn.sigmoid(tf.matmul(v_state, W) + h_bias)  #probabilities of the hidden units
print ("p(h|v): ", sess.run(h_prob))

# Draw samples from the distribution:
h_state = tf.nn.relu(tf.sign(h_prob - tf.random_uniform(tf.shape(h_prob)))) #states
print ("h0 states:", sess.run(h_state))
73/5:
vb = tf.constant([0.1, 0.2, 0.1, 0.1, 0.1, 0.2, 0.1])
print ("b: ", sess.run(vb))
v_prob = sess.run(tf.nn.sigmoid(tf.matmul(h_state, tf.transpose(W)) + vb))
print ("p(vi∣h): ", v_prob)
v_state = tf.nn.relu(tf.sign(v_prob - tf.random_uniform(tf.shape(v_prob))))
print ("v probability states: ", sess.run(v_state))
73/6:
inp = sess.run(X)
print(inp)
print(v_prob[0])
v_probability = 1
for elm, p in zip(inp[0],v_prob[0]) :
    if elm ==1:
        v_probability *= p
    else:
        v_probability *= (1-p)
v_probability
73/7:
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels
73/8: trX[1].shape
73/9:
vb = tf.placeholder("float", [784])
hb = tf.placeholder("float", [50])
73/10: W = tf.placeholder("float", [784, 50])
73/11: v0_state = tf.placeholder("float", [None, 784])
73/12:
h0_prob = tf.nn.sigmoid(tf.matmul(v0_state, W) + hb)  #probabilities of the hidden units
h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random_uniform(tf.shape(h0_prob)))) #sample_h_given_X
73/13:
v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb) 
v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random_uniform(tf.shape(v1_prob)))) #sample_v_given_h
73/14: err = tf.reduce_mean(tf.square(v0_state - v1_state))
73/15:
h1_prob = tf.nn.sigmoid(tf.matmul(v1_state, W) + hb)
h1_state = tf.nn.relu(tf.sign(h1_prob - tf.random_uniform(tf.shape(h1_prob)))) #sample_h_given_X
73/16:
alpha = 0.01
W_Delta = tf.matmul(tf.transpose(v0_state), h0_prob) - tf.matmul(tf.transpose(v1_state), h1_prob)
update_w = W + alpha * W_Delta
update_vb = vb + alpha * tf.reduce_mean(v0_state - v1_state, 0)
update_hb = hb + alpha * tf.reduce_mean(h0_state - h1_state, 0)
73/17:
cur_w = np.zeros([784, 50], np.float32)
cur_vb = np.zeros([784], np.float32)
cur_hb = np.zeros([50], np.float32)
prv_w = np.zeros([784, 50], np.float32)
prv_vb = np.zeros([784], np.float32)
prv_hb = np.zeros([50], np.float32)
sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)
73/18: sess.run(err, feed_dict={v0_state: trX, W: prv_w, vb: prv_vb, hb: prv_hb})
73/19:
#Parameters
epochs = 5
batchsize = 100
weights = []
errors = []

for epoch in range(epochs):
    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):
        batch = trX[start:end]
        cur_w = sess.run(update_w, feed_dict={ v0_state: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_vb = sess.run(update_vb, feed_dict={v0_state: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_hb = sess.run(update_hb, feed_dict={ v0_state: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        prv_w = cur_w
        prv_vb = cur_vb
        prv_hb = cur_hb
        if start % 10000 == 0:
            errors.append(sess.run(err, feed_dict={v0_state: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))
            weights.append(cur_w)
    print ('Epoch: %d' % epoch,'reconstruction error: %f' % errors[-1])
plt.plot(errors)
plt.xlabel("Batch Number")
plt.ylabel("Error")
plt.show()
73/20:
uw = weights[-1].T
print (uw) # a weight matrix of shape (50,784)
73/21: weights
73/22: weights.shape
73/23: weights.size()
73/24: weights.size
73/25: weights.size
73/26: weights.shape
73/27: weights.shape()
73/28: size(weights)
73/29: np.size(weights)
73/30: weights
73/31:
tile_raster_images(X=cur_w.T, img_shape=(28, 28), tile_shape=(5, 10), tile_spacing=(1, 1))
import matplotlib.pyplot as plt
from PIL import Image
%matplotlib inline
image = Image.fromarray(tile_raster_images(X=cur_w.T, img_shape=(28, 28) ,tile_shape=(5, 10), tile_spacing=(1, 1)))
### Plot image
plt.rcParams['figure.figsize'] = (18.0, 18.0)
imgplot = plt.imshow(image)
imgplot.set_cmap('gray')
73/32:
from PIL import Image
image = Image.fromarray(tile_raster_images(X =cur_w.T[10:11], img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))
### Plot image
plt.rcParams['figure.figsize'] = (4.0, 4.0)
imgplot = plt.imshow(image)
imgplot.set_cmap('gray')
73/33:
!wget -O destructed3.jpg  https://ibm.box.com/shared/static/vvm1b63uvuxq88vbw9znpwu5ol380mco.jpg
img = Image.open('destructed3.jpg')
img
73/34:
# convert the image to a 1d numpy array
sample_case = np.array(img.convert('I').resize((28,28))).ravel().reshape((1, -1))/255.0
73/35: sample_case
73/36: sample_case.shape
73/37: weights.shape
73/38: weights
73/39:
hh0_p = tf.nn.sigmoid(tf.matmul(v0_state, W) + hb)
#hh0_s = tf.nn.relu(tf.sign(hh0_p - tf.random_uniform(tf.shape(hh0_p)))) 
hh0_s = tf.round(hh0_p)
hh0_p_val,hh0_s_val  = sess.run((hh0_p, hh0_s), feed_dict={ v0_state: sample_case, W: prv_w, hb: prv_hb})
print("Probability nodes in hidden layer:" ,hh0_p_val)
print("activated nodes in hidden layer:" ,hh0_s_val)

# reconstruct
vv1_p = tf.nn.sigmoid(tf.matmul(hh0_s_val, tf.transpose(W)) + vb)
rec_prob = sess.run(vv1_p, feed_dict={ hh0_s: hh0_s_val, W: prv_w, vb: prv_vb})
73/40:
img = Image.fromarray(tile_raster_images(X=rec_prob, img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))
plt.rcParams['figure.figsize'] = (4.0, 4.0)
imgplot = plt.imshow(img)
imgplot.set_cmap('gray')
74/1:
!wget -O ./data/moviedataset.zip http://files.grouplens.org/datasets/movielens/ml-1m.zip
!unzip -o ./data/moviedataset.zip -d ./data
74/2:
#Tensorflow library. Used to implement machine learning models
import tensorflow as tf
#Numpy contains helpful functions for efficient mathematical calculations
import numpy as np
#Dataframe manipulation library
import pandas as pd
#Graph plotting library
import matplotlib.pyplot as plt
%matplotlib inline
74/3:
#Loading in the movies dataset
movies_df = pd.read_csv('./data/ml-1m/movies.dat', sep='::', header=None, engine='python')
movies_df.head()
74/4:
#Loading in the ratings dataset
ratings_df = pd.read_csv('./data/ml-1m/ratings.dat', sep='::', header=None, engine='python')
ratings_df.head()
74/5:
movies_df.columns = ['MovieID', 'Title', 'Genres']
movies_df.head()
74/6:
ratings_df.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']
ratings_df.head()
74/7: len(movies_df)
74/8:
user_rating_df = ratings_df.pivot(index='UserID', columns='MovieID', values='Rating')
user_rating_df.head()
74/9:
norm_user_rating_df = user_rating_df.fillna(0) / 5.0
trX = norm_user_rating_df.values
trX[0:5]
74/10:
hiddenUnits = 20
visibleUnits =  len(user_rating_df.columns)
vb = tf.placeholder("float", [visibleUnits]) #Number of unique movies
hb = tf.placeholder("float", [hiddenUnits]) #Number of features we're going to learn
W = tf.placeholder("float", [visibleUnits, hiddenUnits])
74/11:
#Phase 1: Input Processing
v0 = tf.placeholder("float", [None, visibleUnits])
_h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)
h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))
#Phase 2: Reconstruction
_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) 
v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))
h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)
74/12:
#Learning rate
alpha = 1.0
#Create the gradients
w_pos_grad = tf.matmul(tf.transpose(v0), h0)
w_neg_grad = tf.matmul(tf.transpose(v1), h1)
#Calculate the Contrastive Divergence to maximize
CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])
#Create methods to update the weights and biases
update_w = W + alpha * CD
update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)
update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)
74/13:
err = v0 - v1
err_sum = tf.reduce_mean(err * err)
74/14:
#Current weight
cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Current visible unit biases
cur_vb = np.zeros([visibleUnits], np.float32)
#Current hidden unit biases
cur_hb = np.zeros([hiddenUnits], np.float32)
#Previous weight
prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Previous visible unit biases
prv_vb = np.zeros([visibleUnits], np.float32)
#Previous hidden unit biases
prv_hb = np.zeros([hiddenUnits], np.float32)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
74/15:
epochs = 15
batchsize = 100
errors = []
for i in range(epochs):
    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):
        batch = trX[start:end]
        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        prv_w = cur_w
        prv_vb = cur_vb
        prv_hb = cur_hb
    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))
    print (errors[-1])
plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.show()
75/1:
#Loading in the movies dataset
movies_df = pd.read_csv('./data/ml-1m/movies.dat', sep='::', header=None, engine='python')
movies_df.head()
75/2:
#Tensorflow library. Used to implement machine learning models
import tensorflow as tf
#Numpy contains helpful functions for efficient mathematical calculations
import numpy as np
#Dataframe manipulation library
import pandas as pd
#Graph plotting library
import matplotlib.pyplot as plt
%matplotlib inline
75/3:
#Loading in the movies dataset
movies_df = pd.read_csv('./data/ml-1m/movies.dat', sep='::', header=None, engine='python')
movies_df.head()
75/4:
#Loading in the ratings dataset
ratings_df = pd.read_csv('./data/ml-1m/ratings.dat', sep='::', header=None, engine='python')
ratings_df.head()
75/5:
movies_df.columns = ['MovieID', 'Title', 'Genres']
movies_df.head()
75/6:
ratings_df.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']
ratings_df.head()
75/7: len(movies_df)
75/8:
user_rating_df = ratings_df.pivot(index='UserID', columns='MovieID', values='Rating')
user_rating_df.head()
75/9:
norm_user_rating_df = user_rating_df.fillna(0) / 5.0
trX = norm_user_rating_df.values
trX[0:5]
75/10:
hiddenUnits = 20
visibleUnits =  len(user_rating_df.columns)
vb = tf.placeholder("float", [visibleUnits]) #Number of unique movies
hb = tf.placeholder("float", [hiddenUnits]) #Number of features we're going to learn
W = tf.placeholder("float", [visibleUnits, hiddenUnits])
75/11:
#Phase 1: Input Processing
v0 = tf.placeholder("float", [None, visibleUnits])
_h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)
h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))
#Phase 2: Reconstruction
_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) 
v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))
h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)
75/12:
#Learning rate
alpha = 1.0
#Create the gradients
w_pos_grad = tf.matmul(tf.transpose(v0), h0)
w_neg_grad = tf.matmul(tf.transpose(v1), h1)
#Calculate the Contrastive Divergence to maximize
CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])
#Create methods to update the weights and biases
update_w = W + alpha * CD
update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)
update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)
75/13:
err = v0 - v1
err_sum = tf.reduce_mean(err * err)
75/14:
#Current weight
cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Current visible unit biases
cur_vb = np.zeros([visibleUnits], np.float32)
#Current hidden unit biases
cur_hb = np.zeros([hiddenUnits], np.float32)
#Previous weight
prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Previous visible unit biases
prv_vb = np.zeros([visibleUnits], np.float32)
#Previous hidden unit biases
prv_hb = np.zeros([hiddenUnits], np.float32)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
75/15:
epochs = 15
batchsize = 100
errors = []
for i in range(epochs):
    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):
        batch = trX[start:end]
        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        prv_w = cur_w
        prv_vb = cur_vb
        prv_hb = cur_hb
    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))
    print (errors[-1])
plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.show()
77/1:
#Tensorflow library. Used to implement machine learning models
import tensorflow as tf
#Numpy contains helpful functions for efficient mathematical calculations
import numpy as np
#Dataframe manipulation library
import pandas as pd
#Graph plotting library
import matplotlib.pyplot as plt
%matplotlib inline
77/2:
#Loading in the movies dataset
movies_df = pd.read_csv('./data/ml-1m/movies.dat', sep='::', header=None, engine='python')
movies_df.head()
77/3:
#Loading in the ratings dataset
ratings_df = pd.read_csv('./data/ml-1m/ratings.dat', sep='::', header=None, engine='python')
ratings_df.head()
77/4:
movies_df.columns = ['MovieID', 'Title', 'Genres']
movies_df.head()
77/5:
ratings_df.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']
ratings_df.head()
77/6: len(movies_df)
77/7:
user_rating_df = ratings_df.pivot(index='UserID', columns='MovieID', values='Rating')
user_rating_df.head()
77/8:
norm_user_rating_df = user_rating_df.fillna(0) / 5.0
trX = norm_user_rating_df.values
trX[0:5]
77/9:
hiddenUnits = 20
visibleUnits =  len(user_rating_df.columns)
vb = tf.placeholder("float", [visibleUnits]) #Number of unique movies
hb = tf.placeholder("float", [hiddenUnits]) #Number of features we're going to learn
W = tf.placeholder("float", [visibleUnits, hiddenUnits])
77/10:
#Phase 1: Input Processing
v0 = tf.placeholder("float", [None, visibleUnits])
_h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)
h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))
#Phase 2: Reconstruction
_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) 
v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))
h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)
77/11:
#Learning rate
alpha = 1.0
#Create the gradients
w_pos_grad = tf.matmul(tf.transpose(v0), h0)
w_neg_grad = tf.matmul(tf.transpose(v1), h1)
#Calculate the Contrastive Divergence to maximize
CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])
#Create methods to update the weights and biases
update_w = W + alpha * CD
update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)
update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)
77/12:
err = v0 - v1
err_sum = tf.reduce_mean(err * err)
77/13:
#Current weight
cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Current visible unit biases
cur_vb = np.zeros([visibleUnits], np.float32)
#Current hidden unit biases
cur_hb = np.zeros([hiddenUnits], np.float32)
#Previous weight
prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Previous visible unit biases
prv_vb = np.zeros([visibleUnits], np.float32)
#Previous hidden unit biases
prv_hb = np.zeros([hiddenUnits], np.float32)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
77/14:
epochs = 15
batchsize = 100
errors = []
for i in range(epochs):
    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):
        batch = trX[start:end]
        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        prv_w = cur_w
        prv_vb = cur_vb
        prv_hb = cur_hb
    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))
    print (errors[-1])
plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.show()
78/1:
!wget -O ./data/moviedataset.zip http://files.grouplens.org/datasets/movielens/ml-1m.zip
!unzip -o ./data/moviedataset.zip -d ./data
78/2:
#Tensorflow library. Used to implement machine learning models
import tensorflow as tf
#Numpy contains helpful functions for efficient mathematical calculations
import numpy as np
#Dataframe manipulation library
import pandas as pd
#Graph plotting library
import matplotlib.pyplot as plt
%matplotlib inline
78/3:
#Loading in the movies dataset
movies_df = pd.read_csv('./data/ml-1m/movies.dat', sep='::', header=None, engine='python')
movies_df.head()
78/4:
#Loading in the ratings dataset
ratings_df = pd.read_csv('./data/ml-1m/ratings.dat', sep='::', header=None, engine='python')
ratings_df.head()
78/5:
movies_df.columns = ['MovieID', 'Title', 'Genres']
movies_df.head()
78/6:
ratings_df.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']
ratings_df.head()
78/7: len(movies_df)
78/8:
user_rating_df = ratings_df.pivot(index='UserID', columns='MovieID', values='Rating')
user_rating_df.head()
78/9:
norm_user_rating_df = user_rating_df.fillna(0) / 5.0
trX = norm_user_rating_df.values
trX[0:5]
78/10:
hiddenUnits = 20
visibleUnits =  len(user_rating_df.columns)
vb = tf.placeholder("float", [visibleUnits]) #Number of unique movies
hb = tf.placeholder("float", [hiddenUnits]) #Number of features we're going to learn
W = tf.placeholder("float", [visibleUnits, hiddenUnits])
78/11:
#Phase 1: Input Processing
v0 = tf.placeholder("float", [None, visibleUnits])
_h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)
h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))
#Phase 2: Reconstruction
_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) 
v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))
h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)
78/12:
#Learning rate
alpha = 1.0
#Create the gradients
w_pos_grad = tf.matmul(tf.transpose(v0), h0)
w_neg_grad = tf.matmul(tf.transpose(v1), h1)
#Calculate the Contrastive Divergence to maximize
CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])
#Create methods to update the weights and biases
update_w = W + alpha * CD
update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)
update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)
78/13:
err = v0 - v1
err_sum = tf.reduce_mean(err * err)
78/14:
#Current weight
cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Current visible unit biases
cur_vb = np.zeros([visibleUnits], np.float32)
#Current hidden unit biases
cur_hb = np.zeros([hiddenUnits], np.float32)
#Previous weight
prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)
#Previous visible unit biases
prv_vb = np.zeros([visibleUnits], np.float32)
#Previous hidden unit biases
prv_hb = np.zeros([hiddenUnits], np.float32)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
78/15:
epochs = 15
batchsize = 100
errors = []
for i in range(epochs):
    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):
        batch = trX[start:end]
        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})
        prv_w = cur_w
        prv_vb = cur_vb
        prv_hb = cur_hb
    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))
    print (errors[-1])
plt.plot(errors)
plt.ylabel('Error')
plt.xlabel('Epoch')
plt.show()
82/1: !wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip -P /resources/data
83/1:
# get the data
!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip
83/2: !unzip concrete_crack_images_for_classification.zip
83/3:
import os
import numpy as np
import matplotlib.pyplot as plt

from PIL import Image
83/4:
negative_files = os.scandir('./Negative')
negative_files
83/5:
file_name = next(negative_files)
file_name
83/6: os.path.isfile(file_name)
83/7:
image_name = str(file_name).split("'")[1]
image_name
83/8:
image_data = plt.imread('./Negative/{}'.format(image_name))
image_data
83/9:
image_data = plt.imread('./Negative/{}'.format(image_name))
image_data.shape
83/10:
## You can use this cell to type your code to answer the above question

(227, 227, 3)
83/11: plt.imshow(image_data)
83/12:
negative_images = os.listdir('./Negative')
negative_images
83/13:
negative_images.sort()
negative_images
83/14: image_data = Image.open('./Negative/{}'.format(negative_images[0]))
83/15: image_data
83/16: plt.imshow(image_data)
83/17:
negative_images_dir = ['./Negative/{}'.format(image) for image in negative_images]
negative_images_dir
83/18: len(negative_images_dir)
83/19:
## You can use this cell to type your code to answer the above question

for i in range(1,5):
    image = negative_images_dir[i]
    plt.imshow(image)
    plt.show()
83/20:
## You can use this cell to type your code to answer the above question

for i in range(1,5):
    image = Image.open(negative_images_dir[i])
    plt.imshow(image)
83/21:
## You can use this cell to type your code to answer the above question

for i in range(1,5):
    image = Image.open(negative_images_dir[i])
    plt.imshow(image)
    plt.show()
83/22:
## Type your answer here
positive_images = os.listdir('./Positive')
positive_images.sort()
positive_images_dir = ['./Positive/{}'.format(image) for image in positive_images]

positive_images_dir[0:3]
83/23:
## You can use this cell to type your code to answer the above question
len(positive_images_dir)
83/24:
## You can use this cell to type your code to answer the above question


for i in range(4):
    image = Image.open(positive_images_dir[i])
    plt.imshow(image)
    plt.show()
88/1: from keras.preprocessing.image import ImageDataGenerator
88/2:
import keras
from keras.models import Sequential
from keras.layers import Dense
88/3:
from keras.applications import ResNet50
from keras.applications.resnet50 import preprocess_input
88/4:
## get the data
!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip
88/5: !unzip concrete_data_week3.zip
88/6:
num_classes = 2

image_resize = 224

batch_size_training = 100
batch_size_validation = 100
88/7:
data_generator = ImageDataGenerator(
    preprocessing_function=preprocess_input,
)
88/8:
train_generator = data_generator.flow_from_directory(
    'concrete_data_week3/train',
    target_size=(image_resize, image_resize),
    batch_size=batch_size_training,
    class_mode='categorical')
88/9:
## Type your answer here

validation_generator = data_generator.flow_from_directory(
    'concrete_data_week3/valid',
    target_size=(image_resize, image_resize),
    batch_size=batch_size_validation,
    class_mode='categorical')
88/10: model = Sequential()
88/11:
model.add(ResNet50(
    include_top=False,
    pooling='avg',
    weights='imagenet',
    ))
88/12: model.add(Dense(num_classes, activation='softmax'))
88/13: model.layers
88/14: model.layers[0].layers
88/15: model.layers[0].trainable = False
88/16: model.summary()
88/17: model.summary()
88/18: model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
88/19:
steps_per_epoch_training = len(train_generator)
steps_per_epoch_validation = len(validation_generator)
num_epochs = 2
88/20:
fit_history = model.fit_generator(
    train_generator,
    steps_per_epoch=steps_per_epoch_training,
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=steps_per_epoch_validation,
    verbose=1,
)
96/1: !wget https://s3-api.us-geo.objectstorage.softlayer.net/v2/projects/f49a60f7-c7c8-4af8-8411-57dcd42c3aff/test.h5
96/2: !wget https://s3-api.us-geo.objectstorage.softlayer.net/v2/projects/Test/test.h5
96/3: !wget https://s3-api.us-geo.objectstorage.softlayer.net/v2/projects/test.h5
96/4: !wget https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/159cfec1-0555-4c32-9b36-ad85e03aa752/view?access_token=9b7e582fac4c27ed398291b29f1daf77356b0e6f5b558970200ab8a91b1e364a/test.h5
96/5: !wget https://s3-api.us-geo.objectstorage.softlayer.net/analytics/notebooks/v2/test.h5
96/6: !wget https://s3-api.us-geo.objectstorage.softlayer.net/analytics/notebooks/v2/projects/test.h5
96/7: !wget https://s3-api.us-geo.objectstorage.softlayer.net/analytics/notebooks/v2/projects/f49a60f7-c7c8-4af8-8411-57dcd42c3aff/test.h5
97/1:
import tensorflow
import keras
97/2: tensorflow.__version()
97/3: tensorflow.__version__()
97/4: tensorflow.__version__
97/5: keras.__version__
88/21: model.save('classifier_resnet_model.h5')
98/1: !wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip
99/1: !wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip
99/2: !unzip concrete_data_week4.zip
99/3:
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.preprocessing.image import ImageDataGenerator
from keras.models import load_model
99/4:
from keras.applications import VGG16
from keras.applications.vgg16 import preprocess_input
99/5:
num_classes = 2
image_resize = 224
batch_size_training = 100
batch_size_validation = 100
99/6:
data_generator = ImageDataGenerator(
    preprocessing_function=preprocess_input,
)
99/7:
train_generator = data_generator.flow_from_directory(
    'concrete_data_week4/train',
    target_size=(image_resize, image_resize),
    batch_size=batch_size_training,
    class_mode='categorical')
99/8:
validation_generator = data_generator.flow_from_directory(
    'concrete_data_week4/valid',
    target_size=(image_resize, image_resize),
    batch_size=batch_size_validation,
    class_mode='categorical')
99/9: model_VGG16 = Sequential()
99/10:
model_VGG16.add(VGG16(
    include_top=False,
    pooling='avg',
    weights='imagenet',
    ))
99/11:
model_VGG16.add(Dense(num_classes, activation='softmax'))
model_VGG16.layers[0].trainable = False
model_VGG16.layers
99/12: model_VGG16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
99/13: model_VGG16.summary()
99/14:
steps_per_epoch_training = len(train_generator)
steps_per_epoch_validation = len(validation_generator)
num_epochs = 2

fit_history = model_VGG16.fit_generator(
    train_generator,
    steps_per_epoch=steps_per_epoch_training,
    epochs=num_epochs,
    validation_data=validation_generator,
    validation_steps=steps_per_epoch_validation,
    verbose=1,
)
88/22: fit_history
88/23: fit_history[0]
88/24: fit_history.history
88/25:
with open('/trainHistoryDict_resent', 'wb') as file_pi:
        pickle.dump(fit_history.history, file_pi)
88/26:
with open('trainHistoryDict_resent', 'wb') as file_pi:
        pickle.dump(fit_history.history, file_pi)
100/1: !conda install -y pickle
100/2: pickle
100/3: import pickle
88/27:
import pickle
with open('trainHistoryDict_resent', 'wb') as file_pi:
        pickle.dump(fit_history.history, file_pi)
88/28:
import pickle
with open('trainHistoryDict_resent', 'wb') as file_pi:
        pickle.dump(fit_history, file_pi)
109/1: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/2: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv  /Data
109/3: !wget /Data https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/4: !wget -O /Data https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/5: !ls
109/6: !wget -O /Data/test.csv https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/7: !wget -O  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv /Data/test.csv
109/8: !wget -P /Data https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv /Data/test.csv
109/9: !wget -P /Data https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/10: !wget -P ~/Data/ https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/11: !wget -P /Data/ https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/12: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -P Data/
109/13: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -P Data/
109/14: !wget -O https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -P Data/
109/15: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -O -P Data/
109/16: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -O -P Data/
109/17: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -O -P Data/
109/18: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -O-P Data/
109/19: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -P -O Data/
109/20: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -P-O Data/
109/21: !wget https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv -O-P Data/
109/22: !wget -O-P Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/23: !wget -O -P Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/24: !wget -P -O Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/25: !wget -P Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/26: !wget -P Data/ -O https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/27: !wget -OP Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/28: !wget -PO Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/29: !wget -op Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/30: !wget -po Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/31: !wget -p-o Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/32: !wget -P-O Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/33: !wget -P-N Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/34: !wget -N -P Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/35: !wget -P Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/countrycode/CountryCodeQS.csv
109/36: !wget -P Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/CountyPopulation/County_Population.csv
109/37: !wget -P Data/  https://covid19-lake.s3.us-east-2.amazonaws.com/static-datasets/csv/state-abv/states_abv.csv
109/38:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

import seaborn as sns

#!conda install -c conda-forge folium=0.5.0 --yes
import folium

#!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
109/39: pd.read_csv('Data/County_Population.csv')
109/40: !wget -P Data/ https://covid19-lake.s3.us-east-2.amazonaws.com/rearc-usa-hospital-beds/json/usa-hospital-beds.geojson
109/41: pr.read_geojson()
109/42: pd.read_geojson()
109/43: pd.read_json('Data/usa-hospital-beds.geojson')
109/44: json_normalize('Data/usa-hospital-beds.geojson')
109/45: json_normalize.'Data/usa-hospital-beds.geojson'.
109/46: test = open('Data/usa-hospital-beds.geojson', 'r')
109/47:
test = open('Data/usa-hospital-beds.geojson', 'r')
pd.json_normalize(test)
109/48:
test = open('Data/usa-hospital-beds.geojson', 'r')
json_normalize(test)
109/49:  pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
109/50: pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
109/51: !wget -P Data/ https://covid19-lake.s3.us-east-2.amazonaws.com/rearc-covid-19-nyt-data-in-usa/csv/us-counties/us-counties.csv
109/52: !wget -P Data/ https://covid19-lake.s3.us-east-2.amazonaws.com/rearc-covid-19-nyt-data-in-usa/csv/us-states/us-states.csv
109/53: pd.read_json('https://covid19-lake.s3.us-east-2.amazonaws.com/alleninstitute/CORD19/comprehendmedical/comprehend_medical.json', lines=True)
109/54: pd.read_csv('Data/states_abv.csv')
109/55: !wget -P Data/ https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_500k.json
109/56: !wget -P Data/ https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_050_00_500k.json
109/57:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/58:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/59:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/60:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.count()
109/61:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
US_pop.summary()
109/62:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
US_pop.count()
109/63:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
state_abb.count()
109/64:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
print(US_pop.count())
109/65:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
state_abb.count()
109/66:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/67: state_abb.count()
109/68:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/69: state_abb.count()
109/70: US_hosp.count()
109/71: pd.read_csv('Data/us-counties.csv')
109/72: pd.read_csv('Data/us-states.csv')
109/73:
US_cases = pd.read_csv('Data/us-counties.csv')
US_cases.head()
109/74: US_cases.summary()
109/75: US_cases.count()
109/76: US_pop.drop(['Id', 'Id2'])
109/77: US_pop.drop(['id', 'id2'], axis = 1)
109/78: US_pop.drop(['Id', 'Id2'], axis = 1)
109/79:
US_pop_c = US_pop.drop(['Id', 'Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
109/80: US_hosp_c = US_hosp.drop(['OBJECTID' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'ADULT_ICU_BEDS' , 'PEDI_ICU_BEDS', 'latitude', 'longtitude'], axis = 1)
109/81:
US_hosp_c = US_hosp.drop(['OBJECTID' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'ADULT_ICU_BEDS' , 'PEDI_ICU_BEDS', 'latitude', 'longtitude'], axis = 1)
US_hosp_c
109/82:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'ADULT_ICU_BEDS' , 'PEDI_ICU_BEDS', 'latitude', 'longtitude'], axis = 1)
US_hosp_c
109/83:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'ADULT_ICU_BEDS' , 'PEDI_ICU_BEDS', 'latitude', 'longtitude'], axis = 1)
US_hosp_c
109/84:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'ADULT_ICU_BEDS' , 'PEDI_ICU_BEDS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c
109/85:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'ADULT_ICU_BEDS' , 'PEDI_ICU_BEDS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c.head()
109/86:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c.head()
109/87:
US_cases_c = US_cases.drop(['fips'], axis = 1).reset_index(drop=True)
US_cases_c.head()
109/88:
US_cases = pd.read_csv('Data/us-counties.csv')
US_cases
109/89:
US_cases_c = US_cases.drop(['fips'], axis = 1).reset_index(drop=True)
US_cases_c.head()
109/90: US_pop_c.join(state_abb, on = 'State')
109/91: US_pop_c.join(state_abb.set_index('State'), on = 'State')
109/92: US_pop.count()
109/93: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/94: US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna.count()
109/95: US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna
109/96: US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna(inplace = True)
109/97: US_pop_c.join(state_abb.set_index('State'), on = 'State')
109/98: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/99:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna
US_pop_c2
109/100:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna()
US_pop_c2
109/101:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
109/102:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c
109/103: US_hosp_c.join(US_pop_c2.set_index('County'), on = 'COUNTY_NAME')
109/104: US_hosp_c.join(US_pop_c2.set_index('County'), on = 'County')
109/105: US_hosp_c.join(US_pop_c2.set_index('County'), on = 'COUNTY_NAME')
109/106: US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = 'COUNTY_NAME')
109/107: US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE'])
109/108:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).reset_index(drop = True)
US_hosp_pop
109/109:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop
109/110: US_cases_c.groupby(by = 'county')
109/111: US_cases_c.groupby(by = 'county').count()
109/112: US_cases_c.pivot(index='county', columns=['date','cases', 'deaths'], values=[ 'state'])
109/113: US_cases_c.pivot(index='county', columns=['cases', 'deaths'], values=[ 'state'])
109/114: US_cases_c.pivot(index='county', columns=['cases'], values=['state'])
109/115: US_cases_c.pivot(index='county', columns=['date'], values=['state'])
109/116: US_cases_c.pivot(index='date', columns=['date'], values=['state'])
109/117: US_cases_c['county' == 'Snohomish']
109/118: US_cases_c[['county' == 'Snohomish']]
109/119: US_cases_c[['county'] == 'Snohomish']
109/120: US_cases_c['county'] == 'Snohomish'
109/121: US_cases_c[US_cases_c['county'] == 'Snohomish']
109/122: US_cases_c['county'].unique()
109/123: US_cases_c[US_cases_c['county'] == 'Cook']
109/124: US_cases_c[US_cases_c['county'] == 'Snohomish']
109/125: US_hosp_pop.groupby(by='COUNTY_NAME').count()
109/126: US_hosp_pop.groupby(by=['COUNTY_NAME', 'HQ_STATE']).count()
109/127:
US_hosp_pop.groupby(by=['COUNTY_NAME', 'HQ_STATE']).count()
US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Summit']
109/128: US_hosp_pop.groupby(by=['COUNTY_NAME', 'HQ_STATE']).count()
109/129: US_hosp_pop.groupby(by=['HQ_STATE']).count()
109/130: US_hosp_pop.groupby(by=['COUNTY_NAME', 'HQ_STATE']).count()
109/131:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/132: US_hosp_pop.isna()
109/133: US_hosp_pop[US_hosp_pop.isna()]
109/134: US_hosp_pop[US_hosp_pop.isnull()]
109/135: US_hosp_pop.isna()
109/136: US_hosp_pop['Population Estimate 2018'].isna()
109/137: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()]
109/138: US_pop_c2['County'] == 'Fairfax County'
109/139: US_pop_c2[US_pop_c2['County'] == 'Fairfax County']
109/140: US_pop_c2[US_pop_c2['County'] == 'Baltimore City']
109/141: US_pop_c2[US_pop_c2['Abbreviation'] == 'VA']
109/142: US_pop_c2[US_pop_c2['Abbreviation'] == 'MD']
109/143: US_pop_c2[US_pop_c2['County'] == 'Baltimore City']
109/144: US_pop_c2[US_pop_c2['County'] == 'Baltimore city']
109/145: US_pop_c2[US_pop_c2['County'] == 'Fairfax County']
109/146: US_pop_c2[US_pop_c2['County'] == 'Fairfax county']
109/147: US_pop_c2[US_pop_c2['County'] == 'Fairfax']
109/148: US_pop_c2[US_pop_c2['County'] == 'Fairfax']
109/149: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = 'COUNTY_NAME').count()
109/150: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = 'COUNTY_NAME').index
109/151: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = 'COUNTY_NAME').index.tolist()
109/152: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = 'COUNTY_NAME').unique
109/153: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = 'COUNTY_NAME').count().index.tolist()
109/154: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = 'COUNTY_NAME').count()
109/155: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = 'COUNTY_NAME').count().index.tolist()
109/156: US_pop_c2[US_pop_c2['County'] == 'Acadia']
109/157: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
109/158: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count()
109/159: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME']).count()
109/160: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
109/161: US_pop_c2[US_pop_c2['abbreviation'] == 'LA']
109/162: US_pop_c2[US_pop_c2['Abbreviation'] == 'LA']
109/163: US_pop_c2[US_pop_c2['County'].str.startswith('Acadia')]
109/164: US_pop_c2[US_pop_c2['County'].str.startswith('Adjuntas')]
109/165: US_pop_c2[US_pop_c2['County'] in'Acadia']
109/166: US_pop_c2['Acadia' in US_pop_c2['County']]
109/167: US_pop_c2['Acadia' in US_pop_c2['County'].str]
109/168: US_pop_c2['Acadia' in US_pop_c2['County'].any()]
109/169: US_pop_c2['Acadia' in US_pop_c2['County'].str]
109/170: US_pop_c2['Acadia' in US_pop_c2['County'].str.any()]
109/171: US_pop_c2['Acadia' in US_pop_c2['County'].all()]
109/172: US_pop_c2['Acadia' in US_pop_c2['County'].str]
109/173: US_pop_c2[US_pop_c2['County'].str.find('Acadia')]
109/174: US_pop_c2[US_pop_c2['County'] == ('Baltimore County')]
109/175: US_pop_c2[US_pop_c2['County'] == ('Baltimore county')]
109/176: US_pop_c2[US_pop_c2['County'] == ('Baltimore city')]
109/177: US_pop_c2[US_pop_c2['County'] == 'Baltimore city']
109/178: US_pop_c2[US_pop_c2['County'] == 'Baltimore City']
109/179: US_pop_c2[US_pop_c2['County'] == 'Baltimore City'.capitalize]
109/180: US_pop_c2[US_pop_c2['County'] == 'Baltimore City'.capitalize()]
109/181: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count()
109/182:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c['COUNTY_NAME'.capitalize()]
US_hosp_c
109/183:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c['COUNTY_NAME'].capitalize()
US_hosp_c
109/184:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c['COUNTY_NAME'].str.capitalize()
US_hosp_c
109/185:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/186: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count()
109/187:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c['COUNTY_NAME'] = US_hosp_c['COUNTY_NAME'].str.capitalize()
US_hosp_c
109/188:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/189:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/190: US_hosp.count()
109/191:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
#US_hosp_c['COUNTY_NAME'] = US_hosp_c['COUNTY_NAME'].str.capitalize()
US_hosp_c
109/192:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/193: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count()
109/194: US_pop_c2[US_pop_c2['County'] == 'St. Tammany']
109/195: US_pop_c2[US_pop_c2['County'] == 'St. tammany']
109/196: US_pop_c2[US_pop_c2['County'] == 'St.Tammany']
109/197: US_pop_c2[US_pop_c2['County'].str.startswith('St.Tammany')]
109/198: US_pop_c2[US_pop_c2['County'].str.startswith('St.')]
109/199:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/200: US_hosp.count()
109/201:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c['COUNTY_NAME'] = US_hosp_c['COUNTY_NAME'].str.capitalize()
US_hosp_c
109/202:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/203: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count()
109/204:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/205: US_hosp.count()
109/206:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
#US_hosp_c['COUNTY_NAME'] = US_hosp_c['COUNTY_NAME'].str.capitalize()
US_hosp_c
109/207:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/208: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count()
109/209: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
109/210: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).index.count()
109/211: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index
109/212: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.len()
109/213: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.length()
109/214: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size()
109/215: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/216: US_pop_c2[US_pop_c2['County'].str.startswith('Baltimore')]
109/217: US_pop_c2[US_pop_c2['County'].str.startswith('Beauregard')]
109/218: US_pop_c2[US_pop_c2['County'].str.startswith('Acadia')]
109/219: US_pop_c2[US_pop_c2['County'].str.startswith('Adjuntas')]
109/220: US_pop_c2[US_pop_c2['County'].str.startswith('Adjuntas') || US_pop_c2['County'].str.endsswith('Adjuntas')]
109/221: US_pop_c2[US_pop_c2['County'].str.startswith('Adjuntas') | US_pop_c2['County'].str.endsswith('Adjuntas')]
109/222: US_pop_c2[US_pop_c2['County'].str.startswith('Adjuntas') | US_pop_c2['County'].str.endswith('Adjuntas')]
109/223: US_pop_c2[US_pop_c2['County'].str.startswith('Adjunt') | US_pop_c2['County'].str.endswith('Adjuntas')]
109/224: US_pop_c2[US_pop_c2['Abbreviation'].str.startswith('LA')]
109/225: US_pop_c2[US_pop_c2['County'].str.contains('Adjunt') | US_pop_c2['County'].str.endswith('Adjuntas')]
109/226: US_pop_c2[US_pop_c2['County'].str.contains('junt') | US_pop_c2['County'].str.endswith('Adjuntas')]
109/227: US_pop_c2[US_pop_c2['County'].str.contains('Acadia') | US_pop_c2['County'].str.endswith('Adjuntas')]
109/228: US_pop_c2[US_pop_c2['County'].str.contains('Adjuntas') | US_pop_c2['County'].str.endswith('Adjuntas')]
109/229: US_pop_c2[US_pop_c2['Abbreviation'].str.startswith('PR')]
109/230: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State').isna()]
109/231: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/232: US_pop_c.join(state_abb.set_index('State'), on = 'State').isna()
109/233: US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviations']
109/234: US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation']
109/235: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]
109/236: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]['State'].unique()
109/237:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/238:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/239: US_hosp.count()
109/240:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/241:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/242: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/243: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
109/244: US_pop_c2[US_pop_c2['County'].str.contains('Alexandria') | US_pop_c2['County'].str.endswith('Alexandria')]
109/245: US_pop_c2[US_pop_c2['County'].str.contains('Alexandria')]
109/246: US_pop_c2[US_pop_c2['County'].str.contains('Acadia')]
109/247: US_pop_c2[US_pop_c2['County'].str.contains('Allen')]
109/248: US_pop_c2[US_pop_c2['County'].str.contains('Allen') & US_pop_c2['Abbreviation'].str.contains('LA')]
109/249:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/250:
for i in temp:
    print(i[0])
109/251:
for i in temp:
    print(i[1])
109/252:
for i in temp:
    #print(i[1])
109/253: US_pop_c2[US_pop_c2['County'].str.contains('St. Bernard') & US_pop_c2['Abbreviation'].str.contains('LA')]
109/254: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]
109/255:
for i in temp:
    new_county_name = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County']
    print(new_county_name)
    print(' ')
109/256: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]
109/257: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]['County']
109/258: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]['County'].value
109/259: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]['County'].value()
109/260: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]['County'].values
109/261: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]['County'].values()
109/262: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]['County'].values[0]
109/263:
for i in temp:
    new_county_name = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values[0]
    print(new_county_name)
    print(' ')
109/264:
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if new_county: new_county_name = new_county[0]
    print(new_county_name)
    print(' ')
109/265:
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if new_county.any() : new_county_name = new_county[0]
    print(new_county_name)
    print(' ')
109/266:
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = 'NOT FOUND' + i[0] + ' ' + i[1]
    print(new_county_name)
    print(' ')
109/267:
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = 'NOT FOUND: ' + i[0] + ' ' + i[1]
    print(new_county_name)
    print(' ')
109/268: US_pop_c2[US_pop_c2['County'].str.contains('Juneau') & US_pop_c2['Abbreviation'].str.contains('AK')]
109/269: US_pop_c2[US_pop_c2['County'].str.contains('Baltimore City') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/270: US_pop_c2[US_pop_c2['County'].str.contains('Baltimore city') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/271: US_pop_c2[US_pop_c2['County'].str.contains('Baltimore') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/272: US_pop_c2[US_pop_c2['County'].str.contains('Bedford') & US_pop_c2['Abbreviation'].str.contains('VA')]
109/273:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = 'NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/274: ''.isempty()
109/275: ''.all()
109/276: ''.any()
109/277: [].all()
109/278: [''].all()
109/279: [''].any()
109/280: [].any()
109/281:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0][0:5]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    #if not new_county.any(): 
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = 'NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/282:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0][0:7]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    #if not new_county.any(): 
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = 'NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/283:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    #if not new_county.any(): 
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = 'NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/284:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.startswith(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = 'NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/285:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.startswith(i[0].capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = 'NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/286:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.startswith(i[0].capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/287: US_pop_c2[US_pop_c2['Abbreviation'].str.startswith('VA')]
109/288: US_pop_c2[US_pop_c2['County'].str.contains('Fairfax') & US_pop_c2['Abbreviation'].str.contains('VA')]
109/289: 'abdedf'.remove('adc')
109/290:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.replace(' County', '').contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/291:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.replace(' County', '').str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/292: US_pop_c2[US_pop_c2['County'].str.contains('Baltimore County') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/293: US_pop_c2[US_pop_c2['County'].str.replace(' County', '').str.contains('Baltimore County') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/294: US_pop_c2[US_pop_c2['County'].str.replace('County', '').str.contains('Baltimore County') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/295: US_pop_c2[US_pop_c2['County'].str.replace('County', '').str.contains('Baltimore County') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/296: US_pop_c2[US_pop_c2['Abbreviation'].str.startswith('MO')]
109/297: US_pop_c2[US_pop_c2['County'].str.replace('County', '').str.replace(' ', '').str.contains('Baltimore County') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/298: US_pop_c2[US_pop_c2['County'].str.contains('Baltimore') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/299:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/300:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/301: US_pop_c2[US_pop_c2['County'].str.contains('St.') & US_pop_c2['Abbreviation'].str.contains('M0')]
109/302: US_pop_c2[US_pop_c2['County'].str.contains('Louis') & US_pop_c2['Abbreviation'].str.contains('M0')]
109/303: US_pop_c2[US_pop_c2['County'].str.contains('Louis') & US_pop_c2['Abbreviation'].str.contains('M0')]
109/304: US_pop_c2[US_pop_c2['County'].str.contains('Louis') & US_pop_c2['Abbreviation'].str.contains('MO')]
109/305: US_pop_c2[US_pop_c2['County'].str.contains('St. Louis') & US_pop_c2['Abbreviation'].str.contains('MO')]
109/306:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/307:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/308:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/309: US_pop_c2[US_pop_c2['County'].str.contains('Salle') & US_pop_c2['Abbreviation'].str.contains('LA')]
109/310: US_pop_c2[US_pop_c2['Abbreviation'].str.startswith('NM')]
109/311: US_pop_c2[US_pop_c2['County'] = 'Do�a Ana']
109/312: US_pop_c2[US_pop_c2['County'].str.contains('Ana')]
109/313: US_pop_c2.iloc[1802]
109/314: US_pop_c2.iloc[1802].County
109/315: US_pop_c2.iloc[1802].County = 'Dona Ana'
109/316: US_pop_c2[US_pop_c2['County'].str.contains('Ana')]
109/317: US_pop[US_pop['County'].str.contains('Ana')]
109/318: US_pop.iloc[1802].County = 'Dona Ana'
109/319: US_pop[US_pop['County'].str.contains('Ana')]
109/320: US_pop.iloc[1802].County = 'Dona Ana'
109/321: US_pop[US_pop['County'].str.contains('Ana')]
109/322: US_pop.iloc[1802]
109/323: US_pop.iloc[1802].County = 'Dona Ana'
109/324: US_pop[1802].County = 'Dona Ana'
109/325: US_pop.at[1802, 'County']
109/326: US_pop.at[1802, 'County'] = 'Dona Ana'
109/327: US_pop[US_pop['County'].str.contains('Ana')]
109/328:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/329: US_pop[US_pop['County'].str.contains('Ana')]
109/330: US_pop.at[1802, 'County'] = 'Dona Ana'
109/331: US_pop.count()
109/332:
US_pop_c = US_pop.drop(['Id', 'Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
109/333:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/334: state_abb.count()
109/335: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/336: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]['State'].unique()
109/337:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
109/338:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/339: US_hosp.count()
109/340:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/341:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/342: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/343:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/344:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/345: US_pop_c2[US_pop_c2['County'].str.contains('Salle') & US_pop_c2['Abbreviation'].str.contains('LA')]
109/346:
count_not_found = 0
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
109/347:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties))
109/348:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/349: US_pop_c2[US_pop_c2['County'].str.contains('Baltimore') & US_pop_c2['Abbreviation'].str.contains('MD')]
109/350:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace('COUNTY_NAME' : {i[0] : new_county_name})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/351:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/352:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop2 = US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/353: US_hosp_pop2
109/354: US_hosp_pop2[US_hosp_pop2['COUNTY_NAME'] == 'Acadia']
109/355: US_hosp_pop2[US_hosp_pop2['COUNTY_NAME'] == 'Acadia Parish']
109/356:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}}, inplace = True)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/357: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Acadia']
109/358: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Acadia Parish']
109/359:
US_hosp_pop = US_hosp_pop.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/360:
US_hosp_pop.drop(['Population Estimate 2018'], axis = 1, inplace = True)
US_hosp_pop = US_hosp_pop.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/361: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
109/362:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/363: US_pop[US_pop['County'].str.contains('Ana')]
109/364: US_pop.at[1802, 'County'] = 'Dona Ana'
109/365: US_pop.count()
109/366:
US_pop_c = US_pop.drop(['Id', 'Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
109/367:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/368: state_abb.count()
109/369: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/370: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]['State'].unique()
109/371:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
109/372:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/373: US_hosp.count()
109/374:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/375:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/376: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/377:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/378:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}}, inplace = True)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/379:
US_hosp_pop.drop(['Population Estimate 2018'], axis = 1, inplace = True)
US_hosp_pop = US_hosp_pop.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/380:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/381: US_pop[US_pop['County'].str.contains('Ana')]
109/382: US_pop.at[1802, 'County'] = 'Dona Ana'
109/383: US_pop.count()
109/384:
US_pop_c = US_pop.drop(['Id', 'Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
109/385:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/386: state_abb.count()
109/387: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/388: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]['State'].unique()
109/389:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
109/390:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/391: US_hosp.count()
109/392:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/393:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/394: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/395:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/396:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}}, inplace = True)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/397:
US_hosp_pop.drop(['Population Estimate 2018'], axis = 1, inplace = True)
US_hosp_pop_c = US_hosp_pop.join(US_pop_c2.set_index(['County']), on = ['COUNTY_NAME']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop_c.count()
109/398:
US_hosp_pop.drop(['Population Estimate 2018'], axis = 1, inplace = True)
US_hosp_pop_c = US_hosp_pop.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop_c.count()
109/399:
U#S_hosp_pop.drop(['Population Estimate 2018'], axis = 1, inplace = True)
US_hosp_pop_c = US_hosp_pop.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop_c.count()
109/400:
#US_hosp_pop.drop(['Population Estimate 2018'], axis = 1, inplace = True)
US_hosp_pop_c = US_hosp_pop.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop_c.count()
109/401: US_pop_c2
109/402:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/403:
temp = US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/404: US_pop_c2[US_pop_c2['County'] == 'Allen Parish']
109/405: US_pop_c2[US_pop_c2['County'] == 'Caddo Parish']
109/406: US_pop_c2[US_pop_c2['County'] == 'Washington Parish']
109/407: US_pop_c[US_pop_c['County'] == 'Washington Parish']
109/408: US_pop_c2[US_pop_c2['County'] == 'Washington Parish']
109/409: US_hosp_pop_c[US_hosp_pop_c['COUNTY_NAME'] == 'Washington Parish']
109/410: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington Parish']
109/411: US_hosp_c[US_hosp_c['COUNTY_NAME'] == 'Washington Parish']
109/412: US_hosp_c[US_hosp_c['COUNTY_NAME'] == 'Acadia']
109/413: US_hosp_c[US_hosp_c['COUNTY_NAME'] == 'Washington Parish']
109/414:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/415: US_pop[US_pop['County'].str.contains('Ana')]
109/416: US_pop.at[1802, 'County'] = 'Dona Ana'
109/417: US_pop.count()
109/418:
US_pop_c = US_pop.drop(['Id', 'Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
109/419:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/420: state_abb.count()
109/421: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/422: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]['State'].unique()
109/423:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
109/424:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/425: US_hosp.count()
109/426:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/427: US_hosp_c[US_hosp_c['COUNTY_NAME'] == 'Washington Parish']
109/428:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/429: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington Parish']
109/430: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/431:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/432: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington Parish']
109/433:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}}, inplace = True)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/434: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington Parish']
109/435:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}}, inplace = True)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print('Old: ' + i[0] + '>>>>> ' + 'New: ' + new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/436:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}}, inplace = True)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print('Old: ' + i[0] + ' >>>>> ' + 'New: ' + new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/437:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop.replace({'COUNTY_NAME' : {i[0] : new_county_name}}, inplace = True)
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/438: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington Parish']
109/439:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/440: US_pop[US_pop['County'].str.contains('Ana')]
109/441: US_pop.at[1802, 'County'] = 'Dona Ana'
109/442: US_pop.count()
109/443:
US_pop_c = US_pop.drop(['Id', 'Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
109/444:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/445: state_abb.count()
109/446: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/447: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]['State'].unique()
109/448:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
109/449:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/450: US_hosp.count()
109/451:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/452:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/453: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/454:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/455: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington Parish']
109/456: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington']
109/457: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington' & US_hosp_pop['HQ_STATE'] == 'LA']
109/458: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington']
109/459: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington' && US_hosp_pop['HQ_STATE'] == 'LA']
109/460: US_hosp_pop[US_hosp_pop['HQ_STATE'] == 'LA']
109/461: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington' & US_hosp_pop['HQ_STATE'] == 'LA']
109/462: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')]
109/463: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')].at['COUNTY_NAME']
109/464: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')]['COUNTY_NAME']
109/465: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')]
109/466: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')].replace({'COUNTY_NAME' : {'Washington' : 'test'}}, inplace = True)
109/467: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'test']
109/468: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington']
109/469: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')].loc[:,:].replace({'COUNTY_NAME' : {'Washington' : 'test'}}, inplace = True)
109/470: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'test']
109/471: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington']
109/472: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')].loc[:,:]
109/473: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')].loc[:,:].replace({'COUNTY_NAME' : {'Washington' : 'test'}}, inplace = True)
109/474: US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')] = US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == 'Washington') & (US_hosp_pop['HQ_STATE'] == 'LA')].replace({'COUNTY_NAME' : {'Washington' : 'test'}})
109/475: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'Washington']
109/476: US_hosp_pop[US_hosp_pop['COUNTY_NAME'] == 'test']
109/477:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/478: US_pop[US_pop['County'].str.contains('Ana')]
109/479: US_pop.at[1802, 'County'] = 'Dona Ana'
109/480: US_pop.count()
109/481:
US_pop_c = US_pop.drop(['Id', 'Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
109/482:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/483: state_abb.count()
109/484: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/485: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]['State'].unique()
109/486:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
109/487:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/488: US_hosp.count()
109/489:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/490:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/491: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/492:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/493:
count_not_found = 0
all_counties = []
for i in temp:
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        all_counties.append(new_county_name)
        US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])] = US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])].replace({'COUNTY_NAME' : {i[0] : new_county_name}})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
    print(' ')
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/494:
US_hosp_pop.drop(['Population Estimate 2018'], axis = 1, inplace = True)
US_hosp_pop_c = US_hosp_pop.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop_c.count()
109/495:
temp = US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/496: US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018']]
109/497: US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018'].isnotnull()]
109/498: US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018'].notna()]
109/499: US_hosp_pop_c = US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018'].notna()]
109/500: US_hosp_pop_c.count()
109/501:
US_hosp_pop_c = US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018'].notna()]
US_hosp_pop_c.fillna(value = 0, inplace = True).reset_index(drop = True)
109/502:
US_hosp_pop_c = US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018'].notna()]
US_hosp_pop_c.fillna(value = 0, inplace = True)
109/503: US_hosp_pop_c.count()
109/504: US_cases.head(30)
109/505: print(US_cases_c['county'].unique())
109/506: US_cases_c['county'].unique()
109/507: US_cases_c['county'].unique() == 'Snohomish'
109/508: (US_cases_c['county'].unique() == 'Snohomish').any()
109/509: US_cases_c['county'].unique()
109/510: (US_cases_c['county'].unique() == 'Acadia').any()
109/511: (US_cases_c['county'].unique() == 'Alexandria').any()
109/512: (US_cases_c['county'].unique() == 'Alexandria city').any()
109/513: US_cases_c.join(state_abb.set_index('State'), on = 'state')
109/514: US_cases_c.join(state_abb.set_index('State'), on = 'state').count()
109/515: US_cases_c.join(state_abb.set_index('State'), on = 'state').isna()
109/516: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state').isna()]
109/517: US_cases_c.join(state_abb.set_index('State'), on = 'state').isna()
109/518: US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()
109/519: US_cases_c.join(state_abb.set_index('State'), on = 'state')
109/520: US_cases_c.join(state_abb.set_index('State'), on = 'state').count()
109/521: US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()
109/522: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]
109/523:
US_cases = pd.read_csv('Data/us-counties.csv')
US_cases
109/524: US_cases.count()
109/525:
US_cases_c = US_cases.drop(['fips'], axis = 1).reset_index(drop=True)
US_cases_c = US_cases_c[US_cases_c['state'] != 'Puerto Rico']   
US_cases_c.head()
109/526: US_cases_c.join(state_abb.set_index('State'), on = 'state').count()
109/527: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]
109/528:
US_cases = pd.read_csv('Data/us-counties.csv')
US_cases
109/529: US_cases.count()
109/530:
US_cases_c = US_cases.drop(['fips'], axis = 1).reset_index(drop=True)  
US_cases_c.head()
109/531: US_cases_c.join(state_abb.set_index('State'), on = 'state').count()
109/532: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]
109/533: US_cases_c[US_cases_c['county'] = 'Unknown']
109/534: US_cases_c[US_cases_c['county'] == 'Unknown']
109/535: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]
109/536: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]['state']
109/537: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]['state'].tolist
109/538: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]['state'].tolist()
109/539: state_abb['State'].tolist()
109/540:
US_cases_c = US_cases_c.join(state_abb.set_index('State'), on = 'state').dropna()
US_cases_c
109/541:
US_cases_c = US_cases_c.join(state_abb.set_index('State'), on = 'state').dropna()
US_cases_c.count()
109/542:
US_cases = pd.read_csv('Data/us-counties.csv')
US_cases
109/543: US_cases.count()
109/544:
US_cases_c = US_cases.drop(['fips'], axis = 1).reset_index(drop=True)  
US_cases_c = US_cases_c.join(state_abb.set_index('State'), on = 'state').dropna()
US_cases_c.count()
109/545: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]
109/546:
US_cases = pd.read_csv('Data/us-counties.csv')
US_cases
109/547: US_cases.count()
109/548: US_cases_c[US_cases_c.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]
109/549:
US_cases = pd.read_csv('Data/us-counties.csv')
US_cases
109/550: US_cases.count()
109/551: US_cases[US_cases.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]
109/552:
US_cases_c = US_cases.drop(['fips'], axis = 1).reset_index(drop=True)  
US_cases_c = US_cases_c.join(state_abb.set_index('State'), on = 'state').dropna()
US_cases_c.count()
109/553: US_cases_c[US_cases_c['county'] == 'Unknown']
109/554: state_abb['State'].tolist()
109/555: US_pop_c2[US_pop_c2['State'] == 'Rhode Island']
109/556: US_pop_c2[US_pop_c2['State'] == 'Rhode Island']sort_values(by=['Population Estimate 2018'])
109/557: US_pop_c2[US_pop_c2['State'] == 'Rhode Island'].sort_values(by=['Population Estimate 2018'])
109/558: US_pop_c2[US_pop_c2['State'] == 'Rhode Island'].sort_values(by=['Population Estimate 2018'], ascending = False)
109/559: US_pop_c2[US_pop_c2['Abbreviation'] == 'RI'].sort_values(by=['Population Estimate 2018'], ascending = False)
109/560: US_cases_c[US_cases_c['county'] == 'Unknown']['Abbreviation']
109/561: US_cases_c[US_cases_c['county'] == 'Unknown']['Abbreviation'].tolist()
109/562: US_cases_c[US_cases_c['county'] == 'Unknown']['Abbreviation'].unique().tolist()
109/563:
temp = US_cases_c[US_cases_c['county'] == 'Unknown']['Abbreviation'].unique().tolist()
temp
109/564: US_pop_c2[US_pop_c2['Abbreviation'] == 'RI'].sort_values(by=['Population Estimate 2018'], ascending = False)['County']
109/565: US_pop_c2[US_pop_c2['Abbreviation'] == 'RI'].sort_values(by=['Population Estimate 2018'], ascending = False)['County'][0]
109/566: US_pop_c2[US_pop_c2['Abbreviation'] == 'RI'].sort_values(by=['Population Estimate 2018'], ascending = False)['County'].values[0]
109/567:
for i in temp:
    set_county = US_pop_c2[US_pop_c2['Abbreviation'] == i].sort_values(by=['Population Estimate 2018'], ascending = False)['County'].values[0]
    US_cases_c[US_cases_c['Abbreviation'] == i] = US_cases_c[US_cases_c['Abbreviation'] == i].replace({'county' : {'Unknown' : set_county}})
109/568: US_cases_c[US_cases_c['county'] == 'Unknown']
109/569: US_cases_c[US_cases_c['county'] == 'Snohomish']
109/570: US_cases_c['county'].unique().tolist()
109/571:
for i in US_cases_c['county'].unique().tolist():
    if i not in US_pop_c2['County'].unique().tolist():
        print(i)
109/572: US_pop_c2[US_pop_c2['County'] == 'New York City']
109/573: US_pop_c2[US_pop_c2['County'] == 'New York']
109/574: US_pop_c2[US_pop_c2['County'] == 'Anchorage']
109/575: US_pop_c2[US_pop_c2['County'].str.contains('Anchorage')]
109/576: US_pop_c2[US_pop_c2['County'].str.contains('New York City')]
109/577: US_pop_c2[US_pop_c2['County'].str.contains('St. Tammany')]
109/578: US_pop_c2[US_pop_c2['County'].str.contains('Terrebonne')]
109/579:
temp = US_cases_c['county'].unique().tolist()
for i in temp:
    if i not in US_pop_c2['County'].unique().tolist():
        print(i)
109/580:
 US_cases_c['county', 'Abbreviation'].unique().tolist()
#for i in temp:
#    if i not in US_pop_c2['County'].unique().tolist():
#        print(i)
109/581:
 US_cases_c[['county', 'Abbreviation']].unique().tolist()
#for i in temp:
#    if i not in US_pop_c2['County'].unique().tolist():
#        print(i)
109/582:
 US_cases_c[['county', 'Abbreviation']]
#for i in temp:
#    if i not in US_pop_c2['County'].unique().tolist():
#        print(i)
109/583:
 US_cases_c[['county', 'Abbreviation']][0]
#for i in temp:
#    if i not in US_pop_c2['County'].unique().tolist():
#        print(i)
109/584:
 US_cases_c[['county', 'Abbreviation']].values[0]
#for i in temp:
#    if i not in US_pop_c2['County'].unique().tolist():
#        print(i)
109/585:
 US_cases_c[['county', 'Abbreviation']].values[0][0]
#for i in temp:
#    if i not in US_pop_c2['County'].unique().tolist():
#        print(i)
109/586:
 US_cases_c[['county', 'Abbreviation']].values[0]
#for i in temp:
#    if i not in US_pop_c2['County'].unique().tolist():
#        print(i)
109/587:
 US_cases_c[['county', 'Abbreviation']].values
#for i in temp:
#    if i not in US_pop_c2['County'].unique().tolist():
#        print(i)
109/588:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp:
    if i not in US_pop_c2[['County', 'Abbreviation'].values:
        print(i)
109/589:
temp = US_cases_c[['county', 'Abbreviation']].values()
for i in temp:
    if i not in US_pop_c2[['County', 'Abbreviation'].values:
        print(i)
109/590:
temp = US_cases_c[['county', 'Abbreviation']].values()
for i in temp:
    if i not in US_pop_c2[['County', 'Abbreviation'].values():
        print(i)
109/591:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp:
    if i not in US_pop_c2[['County', 'Abbreviation']].values:
        print(i)
109/592:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp[0:1]:
    if i not in US_pop_c2[['County', 'Abbreviation']].values:
        print(i)
109/593:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp[0:1]:
    print(i)
    if i not in US_pop_c2[['County', 'Abbreviation']].values:
        print(i)
109/594:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp[0:10]:
    print(i)
    if i not in US_pop_c2[['County', 'Abbreviation']].values:
        print(i)
109/595:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp[0:10]:
    if (i[0] not in US_pop_c2[['County']].values) & (i[1] not in US_pop_c2[['Abbreviation']].values):
        print(i)
109/596:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp[0:100]:
    if (i[0] not in US_pop_c2[['County']].values) & (i[1] not in US_pop_c2[['Abbreviation']].values):
        print(i)
109/597:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp[0:1000]:
    if (i[0] not in US_pop_c2[['County']].values) & (i[1] not in US_pop_c2[['Abbreviation']].values):
        print(i)
109/598:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp[0:10000]:
    if (i[0] not in US_pop_c2[['County']].values) & (i[1] not in US_pop_c2[['Abbreviation']].values):
        print(i)
109/599:
temp = US_cases_c[['county', 'Abbreviation']].values
for i in temp[0:10000]:
    if (i[0] not in US_pop_c2[['County']].values):# & (i[1] not in US_pop_c2[['Abbreviation']].values):
        print(i)
109/600:
temp1 = US_cases_c[['county', 'Abbreviation']].values
temp2 = US_pop_c2[['County', 'Abbreviation']].values
for i in temp1[0:10000]:
    for j in temp2:
        if (i == j):# & (i[1] not in US_pop_c2[['Abbreviation']].values):
            print(i)
109/601:
temp = US_cases_c[['county', 'Abbreviation']].values
#temp2 = US_pop_c2[['County', 'Abbreviation']].values
for i in temp[0:10000]:
    if i[0] != US_pop_c2[['County']].values: # & (i[1] not in US_pop_c2[['Abbreviation']].values):
        print(i)
109/602:
temp = US_cases_c[['county', 'Abbreviation']].values
#temp2 = US_pop_c2[['County', 'Abbreviation']].values
for i in temp[0:10000]:
    if i[0] not in US_pop_c2[['County']].values: # & (i[1] not in US_pop_c2[['Abbreviation']].values):
        print(i)
109/603:
temp = US_cases_c[['county', 'Abbreviation']].values
#temp2 = US_pop_c2[['County', 'Abbreviation']].values
old_counties2 = []
for i in temp[0:10000]:
    if i[0] not in US_pop_c2[['County']].values: # & (i[1] not in US_pop_c2[['Abbreviation']].values):
        old_counties2.append(i)
        print(i)
109/604: old_counties2
109/605: set(old_counties2)
109/606:
temp = US_cases_c[['county', 'Abbreviation']].values
#temp2 = US_pop_c2[['County', 'Abbreviation']].values
old_counties2 = []
for i in temp[0:10000]:
    if i[0] not in US_pop_c2[['County']].values: # & (i[1] not in US_pop_c2[['Abbreviation']].values):
        old_counties2.append(i)
        print(i.tolist())
109/607:
temp = US_cases_c[['county', 'Abbreviation']].values
#temp2 = US_pop_c2[['County', 'Abbreviation']].values
old_counties2 = []
for i in temp[0:10000]:
    if i[0] not in US_pop_c2[['County']].values: # & (i[1] not in US_pop_c2[['Abbreviation']].values):
        old_counties2.append(i.tolist())
        print(i.tolist())
109/608: set(old_counties2)
109/609: old_counties2
109/610: old_counties2.unique()
109/611: set(old_counties2)
109/612:
temp = US_cases_c[['county', 'Abbreviation']].values
#temp2 = US_pop_c2[['County', 'Abbreviation']].values
old_counties2 = []
for i in temp[0:10000]:
    if i[0] not in US_pop_c2[['County']].values: # & (i[1] not in US_pop_c2[['Abbreviation']].values):
        old_counties2.append(tuple(i))
        print(i.tolist())
109/613: old_counties2
109/614: set(old_counties2)
109/615:
temp = US_cases_c[['county', 'Abbreviation']].values
odd_counties = []
for i in temp:
    if i[0] not in US_pop_c2[['County']].values: 
        odd_counties.append(tuple(i))
109/616: set(old_counties2)
109/617: set(odd_counties)
109/618:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i) & US_pop_c2['Abbreviation'] == 'i[1]']
    if not new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/619:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'] == 'i[1]']
    if not new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/620:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'] == 'i[1]']['County'].values
    if not new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/621: US_pop_c2[US_pop_c2['County'].str.contains('Acadia')]
109/622:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'] == 'i[1]']['County'].values
    if not new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
        print(i[0])
109/623:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'] == 'i[1]']['County'].values
    if not new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
        print(i[1])
109/624:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'] == 'i[1]']['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/625:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'] == 'i[1]']['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/626:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'] == 'i[1]']['County'].values
    print(new_county)
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/627:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[)US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == 'i[1]')]['County'].values
    print(new_county)
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/628:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == 'i[1]')]['County'].values
    print(new_county)
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/629:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == 'i[1]')]['County'].values
    print(old_county)
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/630: US_pop_c2[US_pop_c2['County'].str.contains('Acadia')]
109/631: US_pop_c2[US_pop_c2['County'].str.contains('Acadia')]['County'].values
109/632:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == 'i[1]')]['County'].values[0]
    print(new_county)
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/633:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == 'i[1]')]['County'].values
    print(new_county)
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/634:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    print(new_county)
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/635:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/636:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values[0]
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/637:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
109/638:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
    else:
        print('Not Found' ' >>>>>>> ' + old_county)
109/639: US_cases_c[US_cases_c['county'] == 'Doña Ana']
109/640: US_cases_c[US_cases_c['county'] == 'Doña Ana'] = US_cases_c[US_cases_c['county'] == 'Doña Ana'].replace({'county' : {'Doña Ana' : 'Dona Ana'}})
109/641: US_cases_c[US_cases_c['county'] == 'Doña Ana']
109/642: US_cases_c[US_cases_c['county'] == 'Dona Ana']
109/643:
temp = US_cases_c[['county', 'Abbreviation']].values
odd_counties = []
for i in temp:
    if i[0] not in US_pop_c2[['County']].values: 
        odd_counties.append(tuple(i))
109/644: set(odd_counties)
109/645:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
    else:
        print('Not Found' ' >>>>>>> ' + old_county)
109/646: US_pop_c2[US_pop_c2['County'].str.contains('Kansas')]
109/647: US_pop_c2[US_pop_c2['County'].str.contains('Kansas City')]
109/648: US_pop_c2[US_pop_c2['County'].str.contains('Kansas city')]
109/649: US_pop_c2[US_pop_c2['County'].str.contains('Kansascity')]
109/650: US_pop_c2[US_pop_c2['County'].str.contains('Kansas city')]
109/651: US_pop_c2[US_pop_c2['County'].str.contains('Kansas')]
109/652: US_pop_c2[US_pop_c2['County'].str.contains('kansas')]
109/653:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county[0] + ' >>>>>>>> ' + new_county[0])
    else:
        print('Not Found' ' >>>>>>> ' + old_county[0])
109/654:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county[0])
    else:
        print('Not Found' ' >>>>>>> ' + old_county)
109/655:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county[0])
    else:
        print('>>>>>>> Not Found ' + old_county)
109/656:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county[0])
    else:
        print('>>>>>>>>>>>>> Not Found ' + old_county)
109/657: US_pop_c2[US_pop_c2['Abbreviation'] == 'Mo')]
109/658: US_pop_c2[US_pop_c2['Abbreviation'] == 'Mo']
109/659: US_pop_c2[US_pop_c2['Abbreviation'] == 'MO']
109/660: US_pop_c2[US_pop_c2['Abbreviation'] == 'MO']['County'].tolist()
109/661: US_pop_c2[US_pop_c2['County'].str.contains('ansas')]
109/662: US_pop_c2[US_pop_c2['County'].str.contains('Wyandotte')]
109/663: US_pop_c2[US_pop_c2['County'].str.contains('Jackson')]
109/664:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county)
    else:
        print('>>>>>>>>>>>>> Not Found ' + old_county + ' ' + i[1])
109/665:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county[0])
    else:
        print('>>>>>>>>>>>>> Not Found ' + old_county + ' ' + i[1])
109/666:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county[0])
        US_cases_c[US_cases_c['county'] == old_county] = US_cases_c[US_cases_c['county'] == old_county].replace({'county' : {old_county:new_county[0]}})
    else:
        print('>>>>>>>>>>>>> Not Found ' + old_county + ' ' + i[1])
109/667:
temp = US_cases_c[['county', 'Abbreviation']].values
odd_counties = []
for i in temp:
    if i[0] not in US_pop_c2[['County']].values: 
        odd_counties.append(tuple(i))  
set(odd_counties)
109/668: (US_cases_c['county'].unique() == 'New York').any()
109/669: (US_cases_c['county'].unique() == 'New York')
109/670: US_pop_c2[US_pop_c2['County'].str.contains('New yorl')]
109/671: US_pop_c2[US_pop_c2['County'].str.contains('New york')]
109/672: US_pop_c2[US_pop_c2['County'].str.contains('New York')]
109/673:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county[0])
        US_cases_c[US_cases_c['county'] == old_county] = US_cases_c[US_cases_c['county'] == old_county].replace({'county' : {old_county:new_county[0]}})
    else:
        print('>>>>>>>>>>>>> Not Found ' + old_county + ' ' + i[1])


US_cases_c[US_cases_c['county'] == 'New York City'] = US_cases_c[US_cases_c['county'] == 'New York City'].replace({'county' : {'New York City':'New York'}})
US_cases_c[US_cases_c['county'] == 'Kansas City'] = US_cases_c[US_cases_c['county'] == 'Kansas City'].replace({'county' : {'Kansas City':'Jackson'}})
109/674:
temp = US_cases_c[['county', 'Abbreviation']].values
odd_counties = []
for i in temp:
    if i[0] not in US_pop_c2[['County']].values: 
        odd_counties.append(tuple(i))  
set(odd_counties)
109/675:
for i in set(odd_counties):
    old_county = i[0]
    new_county = US_pop_c2[(US_pop_c2['County'].str.contains(i[0])) & (US_pop_c2['Abbreviation'] == i[1])]['County'].values
    if new_county.any():
        print(old_county + ' >>>>>>>> ' + new_county[0])
        US_cases_c[US_cases_c['county'] == old_county] = US_cases_c[US_cases_c['county'] == old_county].replace({'county' : {old_county:new_county[0]}})
    else:
        print('>>>>>>>>>>>>> Not Found ' + old_county + ' ' + i[1])


US_cases_c[US_cases_c['county'] == 'New York City'] = US_cases_c[US_cases_c['county'] == 'New York City'].replace({'county' : {'New York City':'New York'}})
US_cases_c[US_cases_c['county'] == 'Kansas City'] = US_cases_c[US_cases_c['county'] == 'Kansas City'].replace({'county' : {'Kansas City':'Jackson'}})
109/676:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])] = US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])].replace({'COUNTY_NAME' : {i[0] : new_county_name}})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/677:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
109/678: US_pop[US_pop['County'].str.contains('Ana')]
109/679: US_pop.at[1802, 'County'] = 'Dona Ana'
109/680: US_pop.count()
109/681:
US_pop_c = US_pop.drop(['Id', 'Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
109/682:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
109/683: state_abb.count()
109/684: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
109/685: US_pop_c[US_pop_c.join(state_abb.set_index('State'), on = 'State')['Abbreviation'].isna()]['State'].unique()
109/686:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
109/687:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
109/688: US_hosp.count()
109/689:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c = US_hosp_c[US_hosp_c['HQ_STATE'] != 'PR']
US_hosp_c
109/690:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop.count()
109/691: US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.size
109/692:
temp = US_hosp_pop[US_hosp_pop['Population Estimate 2018'].isna()].groupby(by = ['COUNTY_NAME', 'HQ_STATE']).count().index.tolist()
temp
109/693:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])] = US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])].replace({'COUNTY_NAME' : {i[0] : new_county_name}})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(all_counties))
print(len(set(all_counties)))
109/694:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])] = US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])].replace({'COUNTY_NAME' : {i[0] : new_county_name}})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(set(old_counties)))
109/695:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any(): 
        new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
        if not new_county.any():    
            new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' County', '').capitalize()) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
            if not new_county.any():
                new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' City', ' city')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
                if not new_county.any():
                    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0].replace(' ', '')) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])] = US_hosp_pop[(US_hosp_pop['COUNTY_NAME'] == i[0]) & (US_hosp_pop['HQ_STATE'] == i[1])].replace({'COUNTY_NAME' : {i[0] : new_county_name}})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/696:
US_hosp_pop.drop(['Population Estimate 2018'], axis = 1, inplace = True)
US_hosp_pop_c = US_hosp_pop.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop_c.count()
109/697:
US_hosp_pop_c = US_hosp_pop_c[US_hosp_pop_c['Population Estimate 2018'].notna()]
US_hosp_pop_c.fillna(value = 0, inplace = True)
109/698: US_hosp_pop_c.count()
109/699:
US_cases = pd.read_csv('Data/us-counties.csv')
US_cases
109/700: US_cases.count()
109/701: US_cases[US_cases.join(state_abb.set_index('State'), on = 'state')['Abbreviation'].isna()]
109/702:
US_cases_c = US_cases.drop(['fips'], axis = 1).reset_index(drop=True)  
US_cases_c = US_cases_c.join(state_abb.set_index('State'), on = 'state').dropna()
US_cases_c.count()
109/703: US_cases_c[US_cases_c['county'] == 'Unknown']
109/704:
temp = US_cases_c[US_cases_c['county'] == 'Unknown']['Abbreviation'].unique().tolist()
temp
109/705:
for i in temp:
    set_county = US_pop_c2[US_pop_c2['Abbreviation'] == i].sort_values(by=['Population Estimate 2018'], ascending = False)['County'].values[0]
    US_cases_c[US_cases_c['Abbreviation'] == i] = US_cases_c[US_cases_c['Abbreviation'] == i].replace({'county' : {'Unknown' : set_county}})
109/706: US_cases_c[US_cases_c['county'] == 'Unknown']
109/707: US_cases_c[US_cases_c['county'] == 'Doña Ana'] = US_cases_c[US_cases_c['county'] == 'Doña Ana'].replace({'county' : {'Doña Ana' : 'Dona Ana'}})
109/708:
temp = US_cases_c[['county', 'Abbreviation']].values
odd_counties = []
for i in temp:
    if i[0] not in US_pop_c2[['County']].values: 
        odd_counties.append(tuple(i))
109/709:
temp = set(odd_counties)
temp
109/710:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/711:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any() & (i[0] == 'New York City'):
        new_county = ['New York']
    else if not new_county.any() & (i[0] == 'Kansas City'):
        new_county = ['Jackson']
        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/712:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not new_county.any() & (i[0] == 'New York City'):
        new_county = ['New York']
    elif not new_county.any() & (i[0] == 'Kansas City'):
        new_county = ['Jackson']
        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/713:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if not (new_county.any()) & (i[0] == 'New York City'):
        new_county = ['New York']
    elif not new_county.any() & (i[0] == 'Kansas City'):
        new_county = ['Jackson']
        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/714:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if (not new_county.any()) & (i[0] == 'New York City'):
        new_county = ['New York']
    elif not new_county.any() & (i[0] == 'Kansas City'):
        new_county = ['Jackson']
        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/715:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if ~new_county.any() & (i[0] == 'New York City'):
        new_county = ['New York']
    elif not new_county.any() & (i[0] == 'Kansas City'):
        new_county = ['Jackson']
        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/716:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if (i[0] == 'New York City'):
        new_county = ['New York']
    elif (i[0] == 'Kansas City'):
        new_county = ['Jackson']
        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/717:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if (i[0] == 'New York City'):
        new_county = 'New York'
    elif (i[0] == 'Kansas City'):
        new_county = 'Jackson'
        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/718: g = series(1)
109/719: g = pd.series(1)
109/720: g = pd.series(data=1)
109/721: g = pd.Series(data=1)
109/722: g = pd.Series(1)
109/723:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if (i[0] == 'New York City'):
        new_county = pd.Series('New York')
    elif (i[0] == 'Kansas City'):
        new_county = pd.Series('Jackson')
        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/724:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if (i[0] == 'New York City'):
        new_county = US_pop_c2[US_pop_c2['County'].str.contains('New York') & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    elif (i[0] == 'Kansas City'):
        new_county = US_pop_c2[US_pop_c2['County'].str.contains('Jackson') & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values

        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        #US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/725:
count_not_found = 0
new_counties = []
old_counties = []
for i in temp:
    old_counties.append(i[0])
    
    new_county = US_pop_c2[US_pop_c2['County'].str.contains(i[0]) & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    if (i[0] == 'New York City'):
        new_county = US_pop_c2[US_pop_c2['County'].str.contains('New York') & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values
    elif (i[0] == 'Kansas City'):
        new_county = US_pop_c2[US_pop_c2['County'].str.contains('Jackson') & US_pop_c2['Abbreviation'].str.contains(i[1])]['County'].values

        
            
    
    if new_county.any(): 
        new_county_name = new_county[0]
        new_counties.append(new_county_name)
        US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])] = US_cases_c[(US_cases_c['county'] == i[0]) & (US_cases_c['Abbreviation'] == i[1])].replace({'county' : {i[0] : new_county_name}})
    
    else:
        new_county_name = '>>>>>>>>>>> NOT FOUND: ' + i[0] + ' ' + i[1]
        count_not_found += 1
    print(i[0] + ' >>>>> ' + new_county_name)
print(count_not_found)
print(len(new_counties))
print(len(old_counties))
109/726:
temp = US_cases_c[['county', 'Abbreviation']].values
odd_counties = []
for i in temp:
    if i[0] not in US_pop_c2[['County']].values: 
        odd_counties.append(tuple(i))  
set(odd_counties)
109/727: US_cases_c
109/728: US_cases_c.join(US_hosp_pop_c.set_index(['COUNTY_NAME', 'HQ_STATE']), on = (['county', 'Abbreviation']))
109/729: US_cases_c
109/730: US_hosp_pop_c
109/731: US_cases_c['county'].unique()
109/732: US_cases_c.head(50)
109/733: US_cases_c.head(1000)
109/734: US_cases_c.iloc[10000:15000]
109/735:
US_cases_county = pd.DataFrame()
for i in US_cases_c['county'].unique():
109/736:
US_cases_county = pd.DataFrame()
US_cases_county
109/737:
US_cases_county = pd.DataFrame()
for i in US_cases_c['county'].unique():
    US_cases_county.append(US_cases_c[US_cases_c['county'] == i])
109/738: US_cases_county = pd.DataFrame()
109/739:
US_cases_county = pd.DataFrame()
for i in US_cases_c['county'].unique():
    US_cases_county.append(US_cases_c[US_cases_c['county'] == i])
109/740: US_cases_county
109/741:
US_cases_county = pd.DataFrame()
for i in US_cases_c['county'].unique()[0:10]:
    print(i)
    US_cases_county.append(US_cases_c[US_cases_c['county'] == i])
109/742:
US_cases_county = pd.DataFrame()
for i in US_cases_c['county'].unique()[0:10]:
    US_cases_c[US_cases_c['county'] == i]
    #US_cases_county.append(US_cases_c[US_cases_c['county'] == i])
109/743: US_cases_c
109/744: US_cases_c.sort(by='county')
109/745: US_cases_c.sort_values(by='county')
109/746: US_cases_c.sort_values(by=['county', 'Abbreviation'])
109/747: US_cases_c.sort_values(by=['county', 'Abbreviation']).reset_index(drop=True)
109/748:
US_hosp = pd.read_json('Data/gz_2010_us_050_00_500k.json', lines=True)
US_hosp.head()
109/749:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo.head()
109/750:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo.json_normalize()
109/751: json_normalize('Data/gz_2010_us_050_00_500k.json')
109/752:
file = open('Data/gz_2010_us_050_00_500k.json', 'r')
json_normalize('Data/gz_2010_us_050_00_500k.json')
109/753:
file = open('Data/gz_2010_us_050_00_500k.json', 'r')
json_normalize(file)
109/754:
data = json.load(open('Data/gz_2010_us_050_00_500k.json'))
test = json_normalize(data)
109/755:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo.head()
109/756:
data = json.load(open('Data/gz_2010_us_050_00_500k.json'))
test = json_normalize(data, record_path=[['features']])
109/757:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo
109/758:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features']
109/759:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features']['properties']
109/760:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features']['properties']
109/761:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features']
109/762:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features'][0]
109/763:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features'][0]['properties']
109/764:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features'][0]['properties']['NAME']
109/765:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features'][0]['properties']['NAME'][0]
109/766:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features'][0]['properties']['NAME']
109/767:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features'][1]['properties']['NAME']
109/768:
US_county_geo = pd.read_json('Data/gz_2010_us_050_00_500k.json')
US_county_geo['features'][0]['properties']['NAME']
109/769: len(US_county_geo['features'])
109/770:
counties = []
for i in range(len(US_county_geo['features'])):
    counties.append(US_county_geo['features'][i]['properties']['NAME'])
109/771: counties
109/772:
for i in counties:
    if i not in US_pop_c2['County']:
        print(i)
109/773: US_pop_c2['County']
109/774: US_pop_c2['County'].values
109/775:
for i in counties:
    if i not in US_pop_c2['County'].values:
        print(i)
109/776: US_pop_c2[US_pop_c2['County'].str.contains('Aleutians East')]
109/777: US_pop_c2[US_pop_c2['County'].str.contains('Aleutians West')]
109/778: US_pop_c
109/779: US_pop
109/780:
counties = []
for i in range(len(US_county_geo['features'])):
    counties.append(US_county_geo['features'][i]['properties']['NAME'])

counties_id = []
for i in range(len(US_county_geo['features'])):
    counties_id.append(US_county_geo['features'][i]['properties']['GEO_ID'])
109/781:
for i in counties:
    if i not in US_pop_c2['County'].values:
        print(i)
109/782:
for i in counties_id:
    if i not in US_pop['Id'].values:
        print(i)
109/783:
for i in range(len(US_county_geo['features'])):  
    if US_county_geo['features'][i]['properties']['GEO_ID'] == '0500000US02270':
        print(US_county_geo['features'][i]['properties']['NAME'])
109/784: US_pop_c2[US_pop_c2['County'].str.contains('Wade Hampton')]
109/785: US_pop_c2[US_pop_c2['County'].str.contains('Wade hampton')]
109/786: US_pop_c2[US_pop_c2['County'].str.contains('Wade')]
109/787:
for i in range(len(US_county_geo['features'])):  
    if US_county_geo['features'][i]['properties']['GEO_ID'] == '0500000US46113':
        print(US_county_geo['features'][i]['properties']['NAME'])
109/788:
for i in range(len(US_county_geo['features'])):  
    if US_county_geo['features'][i]['properties']['GEO_ID'] == '0500000US51515':
        print(US_county_geo['features'][i]['properties']['NAME'])
109/789: US_pop_c2[US_pop_c2['County'].str.contains('Bedford')]
109/790: US_pop[US_pop['County'].str.contains('Bedford')]
109/791:
for i in range(len(US_county_geo['features'])):  
    if US_county_geo['features'][i]['properties']['GEO_ID'] == '0500000US02270':
        print(US_county_geo['features'][i]['properties']['NAME'])
109/792: US_pop[US_pop['County'].str.contains('Wade Hampton')]
109/793: US_pop[US_pop['County'].str.contains('Wade')]
110/1:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
110/2:
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup

import json
from pandas.io.json import json_normalize 

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN 
from sklearn import preprocessing
import sklearn.utils
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

%matplotlib inline 
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib as mpl
import matplotlib.pyplot as plt

import seaborn as sns

#!conda install -c conda-forge folium=0.5.0 --yes
import folium

#!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim
110/3:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
110/4: US_pop.count()
110/5:
US_pop_c = US_pop.drop(['Id'], axis = 1).reset_index(drop=True)
US_pop_c.head()
110/6:
US_pop = pd.read_csv('Data/County_Population.csv')
US_pop.head()
110/7: US_pop.count()
110/8:
US_pop_c = US_pop.drop(['Id2'], axis = 1).reset_index(drop=True)
US_pop_c.head()
110/9:
state_abb = pd.read_csv('Data/states_abv.csv')
state_abb.head()
110/10: state_abb.count()
110/11: US_pop_c.join(state_abb.set_index('State'), on = 'State').count()
110/12:
US_pop_c2 = US_pop_c.join(state_abb.set_index('State'), on = 'State').dropna().reset_index(drop=True)
US_pop_c2
110/13:
US_hosp = pd.read_json('Data/usa-hospital-beds.geojson', lines=True)
US_hosp.head()
110/14: US_hosp.count()
110/15:
US_hosp_c = US_hosp.drop(['OBJECTID', 'HOSPITAL_TYPE' , 'HQ_ADDRESS', 'HQ_ADDRESS1', 'HQ_CITY', 'HQ_ZIP_CODE', 'STATE_NAME' , 'STATE_FIPS' , 'CNTY_FIPS', 'FIPS', 'latitude', 'longtitude'], axis = 1).reset_index(drop=True)
US_hosp_c
110/16:
US_hosp_pop = US_hosp_c.join(US_pop_c2.set_index(['County', 'Abbreviation']), on = ['COUNTY_NAME', 'HQ_STATE']).drop('State', axis = 1).reset_index(drop = True)
US_hosp_pop
   1: %history
   2: %history -g -f filename
